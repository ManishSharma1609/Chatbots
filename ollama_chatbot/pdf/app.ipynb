{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docarrayNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached docarray-0.40.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pypdf in c:\\users\\dell\\anaconda3\\envs\\ollama\\lib\\site-packages (4.2.0)\n",
      "Collecting numpy>=1.17.3 (from docarray)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting orjson>=3.8.2 (from docarray)\n",
      "  Downloading orjson-3.10.3-cp312-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     -------------------------------- ------- 41.0/50.9 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 50.9/50.9 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting pydantic>=1.10.8 (from docarray)\n",
      "  Downloading pydantic-2.7.2-py3-none-any.whl.metadata (108 kB)\n",
      "     ---------------------------------------- 0.0/108.5 kB ? eta -:--:--\n",
      "     ----------- --------------------------- 30.7/108.5 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 108.5/108.5 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting rich>=13.1.0 (from docarray)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests>=2.28.11.6 (from docarray)\n",
      "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from docarray)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain_community)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
      "  Downloading SQLAlchemy-2.0.30-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Using cached aiohttp-3.9.5-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain_community)\n",
      "  Downloading langchain_core-0.2.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
      "  Downloading langsmith-0.1.65-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting requests<3,>=2 (from langchain_community)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain_community)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10.8->docarray)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.3 (from pydantic>=1.10.8->docarray)\n",
      "  Downloading pydantic_core-2.18.3-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\anaconda3\\envs\\ollama\\lib\\site-packages (from pydantic>=1.10.8->docarray) (4.11.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain_community)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain_community)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain_community)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain_community)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.1.0->docarray)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from rich>=13.1.0->docarray) (2.17.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
      "  Using cached greenlet-3.0.3-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->docarray)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached docarray-0.40.0-py3-none-any.whl (270 kB)\n",
      "Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.1 MB 5.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.1 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.1 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.1 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.1 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
      "   ---------------------------------------- 0.0/973.5 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 307.2/973.5 kB 9.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 491.5/973.5 kB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 675.8/973.5 kB 5.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 870.4/973.5 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 973.5/973.5 kB 4.4 MB/s eta 0:00:00\n",
      "Using cached aiohttp-3.9.5-cp312-cp312-win_amd64.whl (369 kB)\n",
      "Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_core-0.2.3-py3-none-any.whl (310 kB)\n",
      "   ---------------------------------------- 0.0/310.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 310.2/310.2 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.65-py3-none-any.whl (124 kB)\n",
      "   ---------------------------------------- 0.0/124.3 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 92.2/124.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 124.3/124.3 kB 2.4 MB/s eta 0:00:00\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Downloading orjson-3.10.3-cp312-none-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 138.8/138.8 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.5 kB ? eta -:--:--\n",
      "   --------------------------------------  399.4/409.5 kB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 409.5/409.5 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.3-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.4/1.9 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.9 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/1.9 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/1.9 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.5/1.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.7/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 4.3 MB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB 3.6 MB/s eta 0:00:00\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Downloading SQLAlchemy-2.0.30-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 18.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.1 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.1/2.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.7/2.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading types_requests-2.32.0.20240523-py3-none-any.whl (15 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "Using cached greenlet-3.0.3-cp312-cp312-win_amd64.whl (293 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.3/49.3 kB ? eta 0:00:00\n",
      "Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: urllib3, tenacity, PyYAML, pydantic-core, orjson, numpy, mypy-extensions, multidict, mdurl, marshmallow, jsonpointer, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, types-requests, SQLAlchemy, requests, pydantic, markdown-it-py, jsonpatch, aiosignal, rich, langsmith, dataclasses-json, aiohttp, langchain-core, docarray, langchain-text-splitters, langchain, langchain_community\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 dataclasses-json-0.6.6 docarray-0.40.0 frozenlist-1.4.1 greenlet-3.0.3 idna-3.7 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.3 langchain-text-splitters-0.2.0 langchain_community-0.2.1 langsmith-0.1.65 markdown-it-py-3.0.0 marshmallow-3.21.2 mdurl-0.1.2 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.3 pydantic-2.7.2 pydantic-core-2.18.3 requests-2.32.3 rich-13.7.1 tenacity-8.3.0 types-requests-2.32.0.20240523 typing-inspect-0.9.0 urllib3-2.2.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "%pip install docarray langchain_community langchain pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "model.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "chain.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Here is some context\\n\\nQuestion: Here is a question\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Here is some context\", question=\"Here is a question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name is Santiago.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"context\": \"My parents named me Santiago\", \"question\": \"What's your name'?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Machine learning-driven new material discovery\\nJiazhen Cai,aXuan Chu,aKun Xu,aHongbo Liband Jing Wei *ab\\nNew materials can bring about tremendous progress in technology and applications. However, the\\ncommonly used trial-and-error method cannot meet the current need for new materials. Now, a newlyproposed idea of using machine learning to explore new materials is becoming popular. In this paper, wereview this research paradigm of applying machine learning in material discovery, including datapreprocessing, feature engineering, machine learning algorithms and cross-validation procedures.Furthermore, we propose to assist traditional DFT calculations with machine learning for materialdiscovery. Many experiments and literature reports have shown the great e ﬀects and prospects of this\\nidea. It is currently showing its potential and advantages in property prediction, material discovery,\\ninverse design, corrosion detection and many other aspects of life.\\n1. Introduction\\nMachine learning (ML)1,2is a new sub \\ue103eld of arti \\ue103cial intelli-\\ngence that focuses on optimizing computer programs to\\nimprove algorithms through data and researching experience.ML has also become an e ﬃcient and important tool for ana-\\nlysing existing materials in the \\ue103eld of new material discovery.\\n3,4\\nThe traditional trial-and-error method relies on personal expe-\\nrience. Therefore, decades o \\ue09den pass from experiment to\\nmarketing.5,6Considering the consumption of experimental\\ndevelopment, the traditional material discovery method can\\nhardly adapt to the large-scale demand for novel high-\\nperformance materials.To address this situation, the United States of America\\nlaunched the “Material Genome Initiative ”project (MGI) in\\n2011.7The MGI proposes that researchers should focus on\\n“material design ”instead of “trial-and-error ”, which requires\\nresearchers to deepen their comprehension of materials; collectenormous material data to build databases and computingplatforms; and above all, use high-throughput screening of\\nmaterials to eventually achieve the purpose of reducing research\\ncosts and speeding development. In 2016, China launched the“Material Genetic Engineering and Support Platform ”program\\nas a national strategy. Di ﬀerent from MGI, the Chinese\\ngovernment is concerned with building a high-throughputcomputing platform\\n8that can serve the majority of\\nresearchers. In this aspect, machine learning-driven newmaterial discovery is thriving.\\nWhen solving material problems by ML, datasets are needed\\nto help detect target features, properties or unknown materials.These datasets and the messages inside them are called “input ”,\\nand the targets are called “output ”. With these two de \\ue103nitions,\\nJiazhen Cai is a bachelor's\\ndegree candidate at the Schoolof Electronic Engineering atBeijing University of Posts andTelecommunications. He joinedprofessor Wei's group in the\\nsummer of 2018, and he is\\nespecially interested in machinelearning.\\nJing Wei received her PhD fromPeking University in 2017. She isnow an associate research fellowat Beijing Institute of Tech-nology. Her research interestsfocus on semiconductor mate-\\nrials and their optoelectronic\\ndevices, information functionalmaterials and devices, andcomputational materialsscience.aState Key Laboratory of Information Photonics and Optical Communications, Beijing\\nUniversity of Posts and Telecommunications, Beijing, China. E-mail: weijing@bit.edu.\\ncn\\nbExperimental Center of Advanced Materials, School of Materials Science &\\nEngineering, Beijing Institute of Technology, Beijing 100081, ChinaCite this: Nanoscale Adv. , 2020, 2, 3115\\nReceived 13th May 2020\\nAccepted 22nd June 2020\\nDOI: 10.1039/d0na00388crsc.li/nanoscale-advances\\nThis journal is © The Royal Society of Chemistry 2020 Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 | 3115Nanoscale\\nAdvances\\nREVIEW\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 0}),\n",
       " Document(page_content='Advances\\nREVIEW\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online\\nView Journal\\n | View Issue', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 0}),\n",
       " Document(page_content=\"the ML-aided method can now be de \\ue103ned as “using inputs and\\nappropriate ML algorithms to build a numerical predicting\\nmodel and detecting unknown outputs by the predicting abilityof this model ”(Fig. 1). Because outputs are \\ue103tted by inputs, it is\\nreasonable that the outputs will have similar chemical struc-tures to the inputs and can be evaluated in the same way thatthe inputs are evaluated.\\n9With this model, we can enhance the\\ncomprehension of material properties and predict unknownneeded materials. At present, this method is still confronted\\nwith many challenges: the messy datasets must be pre-\\nprocessed; the accuracy of the model is limited by its algo-rithms; the high-intensity computation places pressure oncomputing resources; etc.\\n10\\nMachine learning has been widely used in many aspects of\\nmaterial science (Fig. 2). In this review, we focus on modelconstruction, computational algorithms, model veri \\ue103cation\\nprocedures, the role ML plays in the material science \\ue103eld and\\nthe prospects of machine learning. Section 2 describes the data\\npreprocessing and feature engineering steps, which cansystemically reconstruct the datasets and aid understanding ofmaterial properties and physicochemical relationships. InSection 3, some high-performance algorithms in the materialdiscovery \\ue103eld are introduced, and some practical application\\nexamples in this \\ue103eld are surveyed. Section 4 describes several\\ncross-validation procedures for material-discovery ML models.\\nSection 5 explains how ML assists traditional density functional\\ntheory in the \\ue103eld of materials science. In Section 6, some other\\narti\\ue103cial intelligence methods that ML is involved in are dis-\\ncussed. In Section 7, we will summarize the current develop-ment condition of ML methods and brie \\ue104y explain the\\nprospects of ML in new material discovery.\\n2. Data preprocessing and feature\\nengineering\\nData is very important in ML procedures. Generally, the \\ue103nal\\nML results can be directly a ﬀected by the volume and reliability\\nof data; this is where data preprocessing and feature engi-neering come in. These two steps can reconstruct datasets so\\nthat it is more convenient for computers to understand material\\nphysicochemical relationships, detect material properties andestablish material predicting models.\\n13–15For example, Paul\\nRaccuglia once proposed that a predicting model can be trainedby data from failed experiments. He integrated experimental\\ndata using information from failed or less successful hydro-\\nthermal synthesis reactions to train a machine learning modelto predict the crystallization of template vanadium selenitecrystals. It was found that this model was obviously superiorcompared with the traditional manual analysis method. Theprediction accuracy for the formation conditions of neworganic-templated inorganic products could reach 89%.\\n5\\nIn the following parts, we will introduce the two main steps\\nof data preprocessing and how they function in material\\ndiscovery; we will also give some examples of successful appli-cations in the material \\ue103eld.\\n2.1 Data preprocessing\\nData preprocessing mainly consists of two steps: data collecting\\nand data cleaning.\\n16\\ni. Data collecting. Researchers always hope to collect\\nrepresentative data. Therefore, it is necessary for researchers toselect the appropriate data for speci \\ue103c problems. Currently,\\nnumerous open-source databases, such as the Harvard CleanEnergy Project, the Open Quantum Materials Database and theMaterials Project,\\n17have been established. These databases are\\nreliable and accessible and can be used as a bedrock of researchwork. Some of the most authoritative and reliable databases are\\nlisted below for reference (Table 1).\\nFor example, Edward O. Pyzer-Knapp's team used the data\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 1}),\n",
       " Document(page_content=\"listed below for reference (Table 1).\\nFor example, Edward O. Pyzer-Knapp's team used the data\\nfrom the Harvard Clean Energy Database for ML model trainingto solve the over-provisioning problem, in which computersmistakenly consider noise to be useful features. According tothe results, the trained ML model can return predictions on theveri\\ue103cation set within 1.5 seconds, and the over-provisioning\\nproblem is successfully solved.\\n18Kamal Choudhary's team\\nestablished a criterion to identify 2D materials based on\\ncomparison of lattice constants obtained from experiments andthe Materials Project Database. Also, to test this criterion, theycalculated the exfoliation of many layered materials. The results\\nFig. 1 The machine learning work ﬂow; the place of feature engi-\\nneering is shown in the red circle.11\\nFig. 2 An overview of the application of machine learning in materials\\nscience.12\\n3116 |Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 This journal is © The Royal Society of Chemistry 2020Nanoscale Advances Review\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 1}),\n",
       " Document(page_content=\"showed that in 88.9% of cases, the criterion was satis \\ue103ed.19T.\\nBj¨orkman's team screened the International Crystallographic\\nStructural Database (ICSD) and discovered 92 possible 2Dcompounds based on symmetry, packing ratio, structural gapsand covalent radii.\\n20In other cases, Michael Ashton and\\ncolleagues used the topology-scaling algorithm to detect layeredmaterials from ICSD and successfully found 680 stable mono-layers;\\n21Nicolas Mounet and coworkers used data mining of\\nICSD and the Crystallographic Open Database to search for\\nlayered compounds;22and Sten Haastrup's team established\\none of the largest databases of 2D materials.23All the research\\nmentioned above strongly supports the availability and superi-ority of high-quality data in practical applications. From thispoint of view, this is a necessary step of the ML method inmaterial discovery.\\nii. Data cleaning. A\\ue09der the data collection step, there are\\nstill many problems with the collected data, such as data\\nredundancy or abnormal values. In order to obtain an e ﬃcient\\nML predicting model and also to reduce the amount of calcu-lation, data cleaning is necessary. In this paper, we de \\ue103ne data\\ncleaning as a data operation consisting of four steps: datasampling, abnormal value processing, data discretization, anddata normalization.\\n24,25\\nFirst, data sampling ensures that researchers can obtain\\nhigh performance prediction models with less data without\\ncompromising predicting accuracy.26To ensure the ability and\\naccuracy of the predicting model, researchers must eliminateabnormal values to maintain the accuracy of the predictingmodels.\\n27Furthermore, data discretization can signi \\ue103cantly\\nreduce the number of possible values of a continuous feature.Also, data normalization can adjust the magnitudes of data to\\na suitable and identical level, which is crucial for many machinelearning algorithms. As an example, in the research of photo-voltaic organic-inorganic hybrid perovskites by Shuai Hua Lu'steam, all the input data were obtained from reliable databasescomposed of high throughput \\ue103rst-principles calculations. For\\ndata consistency and accuracy of the ML predictions, theycarefully constructed their own training sets and validation sets\\nwith appropriately processed data. Only orthorhombic-like\\ncrystal structures with bandgaps calculated using the Perdew –\\nBurke –Ernzerhof (PBE) functional were selected.\\n28\\nAlso, a \\ue09der the four steps above, the dataset is divided into\\ntraining sets and testing sets. As we can see, a \\ue09der data pre-\\nprocessing, the noise, data redundancy and abnormal valuesare all largely reduced. However, the dataset is still messy. Itmust be reconstructed to enable computers to better under-\\nstand the data within. This can be achieved by feature\\nengineering.\\n2.2 Feature engineering\\nFeature engineering is the process of extracting the most\\nappropriate features from data and tasks. These features arecalled descriptors. Fig. 1 shows the position of feature engi-neering in the machine learning work \\ue104ow. Fig. 4 presents\\na typical work \\ue104ow from data to \\ue103ngerprinting descriptors. Its\\npurpose is to obtain the features from the training data so that\\nthe ML algorithms can approach their best performance. In thelatest work of Ming Wang et al. ,\\n29researchers introduced\\nautomated feature engineering as a new trend in nanomaterialTable 1 An overview of some of the most authoritative databases in material science\\nTitle Website address Brief introduction\\nAFLOWLIB http://www.a \\ue104owlib.org A global database of 3 249 264 material\\ncompounds and over 588 116 784 calculatedproperties\\nASM Alloy Database https://www.asminternational.org/materials-\\nresources/online-databasesAn authoritative database focusing on alloys,\\nmechanical and alloy phases, and failed\\nexperiment data\\nCambridge Crystallographic Data Centre http://www.ccdc.cam.ac.uk It focuses on structural chemistry and contains\\nover 1 000 000 structures\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 2}),\n",
       " Document(page_content='mechanical and alloy phases, and failed\\nexperiment data\\nCambridge Crystallographic Data Centre http://www.ccdc.cam.ac.uk It focuses on structural chemistry and contains\\nover 1 000 000 structures\\nChemSpider http://www.chemspider.com A free chemical structural database providing\\nfast searching access to over 67 000 000structures\\nHarvard Clean Energy Project http://cepdb.molecularspace.org/ A massive database of organic solar cell\\nmaterials\\nHigh Performance Alloys Database https://cindasdata.com/products/hpad This high performance alloy database addresses\\nthe needs of the chemical processing, power\\ngeneration and transportation industries\\nMaterials Project https://materialsproject.org/ It o ﬀers more than 530 000 nanoporous\\nmaterials, 124 000 inorganic compounds andpower analysis tools for researchers\\nNanoHUB https://nanohub.org/resources/databases An open source database focusing on\\nnanomaterials\\nOpen Quantum Materials Database http://oqmd.org/ It contains substantial amounts of data on the\\nthermodynamic and structural properties of\\n637 644 materials\\nSpringer Materials http://materials.springer.com A comprehensive database covering multiple\\nmaterial classes, properties and applications\\nThis journal is © The Royal Society of Chemistry 2020 Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 | 3117Review Nanoscale Advances\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 2}),\n",
       " Document(page_content=\"discovery. Automated feature engineering uses deep learning\\nalgorithms to automatically develop a set of features that are\\nrelevant to predict a desired output. This signi \\ue103cantly mini-\\nmizes the amount of domain knowledge used in a trainingmodel and accelerates its application among non-expert users.\\nWe can see that this new technique has very good application\\nprospects,\\n29and it is also a very signi \\ue103cant application para-\\ndigm of deep learning. As an example, Fig. 3 shows the evolu-tion of the work \\ue104ow of machine learning in the discovery and\\ndesign nanomaterials.\\nReturning to descriptors, in particular, a descriptor refers to\\na set of meaningful parameters that describe a mechanism ora feature. For example, in the chemical periodic table, all theelements are sorted by rows (periods) and columns (families).The rows and columns can be considered as a set of descriptors.The appropriate descriptors can integrate essential informa-tion, and the quality of the predicting model is also directly\\nrelated to the quality of the descriptors. High-performance\\ndescriptors can e ﬀectively organize independent variables,\\nexpress various mechanisms and \\ue103nd hidden relationships with\\na smaller volume of data. Currently, there are two basic main-stream ideas for designing descriptors. The \\ue103rst idea is manu-\\nally creating a set of descriptors depending on relevant physicalchemical properties of the experimental candidates. The secondidea is using related mathematical and physical theories to\\nproject the features of the experimental candidates into\\nnumerical vectors as descriptors.\\n34Luca M. Ghiringhelli\\npresumed four basic standards of descriptors a \\ue09der research:30\\n1. The dimensions of the descriptor should be as low as\\npossible.\\n2. The descriptor uniquely characterizes the material as well\\nas the property-relevant elementary process.\\n3. Materials that are very di ﬀerent (similar) should be char-\\nacterized and selected by very di ﬀerent (similar) descriptor\\nvalues.4. The determination of the descriptor must not involve\\ncalculations as intensive as those needed for the evaluation ofthe property to be predicted.\\nDespite the four basic standards above, it is still challenging\\nto select the right descriptors. Daniel C. Elton believes that\\nwhen facing a small dataset, the characterization of the data ismore important than the construction of the model, and a set ofhighly e ﬃcient descriptors can ensure the accuracy of the\\nresults. When dealing with large databases, a large dataset issuﬃcient for the ML algorithm to extract complex or potential\\nfeatures from the usual traits. However, in this case, researcherssuggest selecting descriptors with superior computational e ﬃ-\\nciency and experimental performance. Also, to ensure accuracy\\nof the results, transparent and abstract descriptors arepreferred.\\n31\\nTo achieve practical application, suitable descriptors must\\nbe chosen depending on di ﬀerent situations. Anton O. Oliynyk\\nand coworkers used machine learning methods to detectpotential Heusler compounds and properties.\\n32They focused on\\nsome speci \\ue103c dimensions where material patterns are most\\nlikely to be found. In these dimensions, the utility of the\\ndescriptors can be maximized.\\nThey \\ue103nalized 22 descriptors through experiments to help\\ncomputers discover hidden relationships. A \\ue09der veri \\ue103cation, the\\npredicting model they obtained with this set of descriptorscould conduct predictions and calculations of over 400 000groups of substances within 45 minutes. Also, the results wereobtained a \\ue09der ten cross-validations, which proved the correct-\\nness of this prediction. Fleur Legrain's team attempted to\\npredict vibrational energies and entropies of compounds by theML method. In this case, they chose chemical composition-\\nFig. 4 A recipe for proceeding from data to ﬁngerprinting descriptors\\nto insights to models and discovery.35\\nFig. 3 Evolution of the work ﬂow of machine learning in nanomaterials\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 3}),\n",
       " Document(page_content='Fig. 4 A recipe for proceeding from data to ﬁngerprinting descriptors\\nto insights to models and discovery.35\\nFig. 3 Evolution of the work ﬂow of machine learning in nanomaterials\\ndiscovery and design. (a) First-generation approach. In this paradigm,\\nthere are two main steps: feature engineering from raw database todescriptors; model building from descriptors to target model. (b)Second-generation approach. The key characteristic that distinguishes\\nit from the ﬁrst-generation approach is eliminating human-expert\\nfeature engineering, which can be directly learned from rawnanomaterials.\\n29\\n3118 |Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 This journal is © The Royal Society of Chemistry 2020Nanoscale Advances Review\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 3}),\n",
       " Document(page_content=\"based descriptors to guarantee the quality and accuracy of their\\nresults in small datasets. During the experiment, they\\nconcluded that the descriptors based on the chemical compo-sition and elemental properties of atoms of materials showexcellent performance in small datasets. The predictive powerof this idea was validated by comparing experimental resultswith measured values from the National Institute of Standardsand Technology.\\n33Furthermore, Seiji Kajita and colleagues\\ndeveloped a universal 3D voxel descriptor that can represent the\\nthree-dimensionality of \\ue103eld quantities in solid-state ML. In the\\nexperimental validation, they associated the convolutionalneural network with solid-state ML by this novel descriptor andtested the experimental performance of the descriptor usingdata for 680 oxides. The results showed that this descriptoroutperforms other existing descriptors in its prediction ofHartree energies and is relevant to the long-wavelength distri-bution of valence electrons.\\n34\\nIn addition to the basic descriptors, there are some cases\\nwhen the descriptor itself must be explored deeply. YingZhang's team found that the model predicting accuracyincreases at the expense of the degrees of freedom. To solve thisproblem, they introduced a so-called “crude estimate of prop-\\nerty”descriptor in the feature space to improve accuracy\\nwithout increasing the degrees of freedom. Compared with theconventional method, the ML with new descriptors showed\\nbetter performance and smaller standard deviations in tests.\\n36\\nAnother classical case is that in which Ankit Jain and Thomas\\nBligaard developed a universal atomic-position independentdescriptor for periodic inorganic solids. Generally speaking,high throughput for periodic inorganic solids requires essentialatomic positions to encode structural and compositional detailsinto appropriate material descriptors. However, when exploringnovel materials, the atomic-position information is usually not\\navailable. Therefore, they developed this descriptor system.\\nWhen applied to predict the formation energies of bulk crys-talline solids, the descriptors achieved a prediction meanabsolute error of only 0.07 eV per atom on a test dataset of morethan 85 000 materials.\\n37\\nAs can be seen, feature engineering and descriptors can\\ngreatly reduce the workload in experiments. However, thequestion of how to choose suitable descriptors is still a serious\\none. From this aspect, further study is still needed.\\n3. Basic machine learning algorithms\\nA\\ue09der building a database, it is necessary to select appropriate\\nmachine learning algorithms. Mathematical theories such asthe Markov chain, the least squares method and the Gaussianprocess\\n38have been used to construct the foundation of ML\\nalgorithms. Currently, ML algorithms can be divided into fourtypes: supervised learning, unsupervised learning, semi-supervised learning and reinforcement learning. Reinforce-ment learning usually focuses on the interactions between\\nalgorithms and the environment but not on \\ue103nding speci \\ue103c\\npatterns from datasets, which is not appropriate for materialdiscovery. Therefore, we will not discuss it in this section. Inline with the “no free lunch theorem ”,\\n39there is no absolutelysuperior ML algorithm. Each algorithm has its own advantages\\nand disadvantages. Here, we list several commonly used\\nmachine learning algorithms for reference.\\n3.1 Regression analysis\\nRegression analysis can be divided into two categories: super-\\nvised regression analysis and unsupervised regression analysis.\\nRegression analysis algorithms can quantify the magnitude towhich the dependent variable is a ﬀected by the independent\\nvariable through analyzing a large volume of data. Then, theywill\\ue103nd matching linear or nonlinear regression equations and\\npredict the dependent variable through regression equations.Based on this feature, researchers can use regression equationsto analyse properties or discover new materials.\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 4}),\n",
       " Document(page_content=\"predict the dependent variable through regression equations.Based on this feature, researchers can use regression equationsto analyse properties or discover new materials.\\nFor example, Atsuto Seko's team individually used ordinary\\nleast squares regression (OLSR),\\n40partial least-square regres-\\nsion (PLSR),41–43support vector regression (SVR, which is also\\na type of support vector machine algorithm)44,45and Gaussian\\nprocess regression (GPR)46,47to predict the melting tempera-\\ntures of monobasic compounds and binary compounds. Theyselected four simple physical properties and ten elementproperties as features, then constructed two datasets to analyze\\nthe performance of four algorithms. The results showed that\\nsupport vector regression had the lowest root mean square errorand the best performance.\\n48Stefano Curtarolo and coworkers\\nattempted to use simple ML algorithms such as PLSR to predictformation energies and optimize high-throughput calcula-tions.\\n49In another experiment, to monitor the spatial distribu-\\ntion of CaO in cement materials, Bulent Tutmez and Ahmet Dagused the regression kriging model and geographically weighted\\nregression to evaluate spatially varying relationships in\\na cement quarry. It was found that the regression kriging modeloutperformed geographically weighted regression.\\n50\\n3.2 Na ¨ıve Bayes classi \\ue103ers\\nBayesian classi \\ue103cation is a general term for a class of algorithms\\nwhich are all established based on Bayes theory; the na ¨ıve Bayes\\nclassi \\ue103er51,52is the simplest of these algorithms. Na ¨ıve Bayes\\nclassi \\ue103ers assume that all features are independent of each\\nother. This assumption greatly simpli \\ue103es the sample space and\\nthe number of solving calculations; therefore, it is basically thesimplest of all Bayes classi \\ue103cations. It can choose the assump-\\ntion which has the highest probability of correctly representingthe data, and it is widely used because of its e ﬃcient algorithm,\\nfast classi \\ue103cation, and ability to be applied to the \\ue103eld of big\\ndata.\\n53O. Addin and colleagues attempted to detect damage of\\nlaminated composite materials using a na ¨ıve Bayes classi \\ue103er.\\nThe na ¨ıve Bayes classi \\ue103er showed high classi \\ue103cation accuracy\\nthat could reach 94.65% that of experiments.54In another\\nexperiment using a specially designed robotic \\ue103nger to recog-\\nnize the surface materials of objects, Hongbin Liu used naiveBayes classi \\ue103cation, k-nearest neighbor classi \\ue103cation and\\na radial basis function network to identify the surfaces. The\\nresults indicated that the na ¨ıve Bayes classi \\ue103cation out-\\nperformed the other two classi \\ue103cation methods, with an\\naverage success rate of 84.2%.\\n55\\nThis journal is © The Royal Society of Chemistry 2020 Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 | 3119Review Nanoscale Advances\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 4}),\n",
       " Document(page_content=\"3.3 Support vector machine (SVM)\\nA support vector machine56,57is a type of supervised learning\\nmethod. For a group of points in the Ndimension, the support\\nvector machine will \\ue103nd a hyperplane of the N/C01 dimension\\nand divide this group into two categories. SVM is built onstatistical learning theory, and the essence of SVM is to mini-mize the actual errors of ML.\\nA\\ue09der long-term development, the support vector machine\\nalgorithm has already been able to greatly simplify problems,reduce the dimensions of data, and eliminate noise, which showsgreat generalization ability in unknown samples. Xintao Qiu andcolleagues combined SVM and recursive feature elimination tomodel atmospheric corrosion of materials. This brand newmethod can extract the most in \\ue104uential factors and build a reli-\\nable analyzing model. Also, when selecting corrosion factors in\\nsmall sample sizes, it greatly outperforms other algorithms in\\nregression and prediction performance.\\n58To detect the molecular\\nfunctions and biological activities of phage virion proteins, Bala-chandran Manavalan trained a SVM predicting model with 136features called PVP-SVM. The performance of PVP-SVM wasconsistent in both the training and testing datasets, and the cross-validation showed that the accuracy of PVP-SVM is 0.870, which ishigher than that of any other control SVM predictors\\n59(Fig. 5).\\nCurrently, SVM is also making good progress in the medical\\n\\ue103eld. Manfred K. Warmuth's team used SVM to classify and\\nscreen compounds related to a target drug and successfullyidentify the most satisfactory drug in the screening. In thisprocedure, the classi \\ue103cation characteristics of SVM could\\neﬀectively judge the correlation between the compound and the\\ntarget, and it showed very good accuracy in the \\ue103nal test.\\n60\\n3.4 Decision tree and random forest\\nThe decision tree61–63is a type of supervised learning. When\\na decision tree is generated, the source dataset (which constitutesthe root node) is split into several subsets (which constitute the\\nsuccessor children) according to a series of splitting rules based\\non classi \\ue103cation features. By repeating this procedure, a decision\\ntree grows (Fig. 6a). The decision tree is also used in classi \\ue103cation\\nproblems; however, it still has some shortcomings, such asover\\ue103tting and generalizing weakness. Therefore, researchers\\ncreated the random forest algorithm, li \\ue09ding tree algorithm, and\\nmany other algorithms based on the decision tree. The randomforest algorithm\\n64,65is a classi \\ue103er composed of multiple decision\\ntrees. The random forest solves the problems of a single decision\\ntree by the voting mechanism of multiple decision trees, whichgreatly improves its generalizing ability.\\nIn an experiment by Anton O. Oliynyk and coworkers,\\n32they\\nattempted to synthesize AB 2C Heusler compounds using the\\nrandom forest algorithm.66,67By averaging the predictions of\\nthese decision trees, the random forest aggregates the di ﬀerent\\ntrends of each tree, which results in a complex and stable\\nmodel. They selected two gallium-containing compounds\\n(MRu 2Ga and RuM 2Ga (M ¼Ti–Ni)) to test its predictive ability.\\nA\\ue09der the prediction, they were considered to have more than\\n60% probability of forming a Heusler compound, and Co 2RuGa\\nhad a higher probability of around 92%.32Derek T. Ahneman\\nand colleagues attempted to predict the Buchwald –Hartwig\\namination reaction against Pd catalysis, the root mean squareerror (RMSE) of the test set of the random forest mode was\\n7.8%, which was much better than those of kernel ridge\\nregression (KRR), support vector machine, Bayes generalizedlinear model and single layer neural network algorithms.\\n68In\\nanother case, Junfei Zhang and his colleagues used a beetleantennae search algorithm-based random forest (BAS-RF)method to detect the uniaxial compressive strength (UCS) oflightweight self-compacting concrete. The results showed thatthis algorithm has high predictive accuracy, indicated by the\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 5}),\n",
       " Document(page_content='high correlation coe ﬃcient of 0.97. In Fig. 6b, it is clear that\\nBAS-RF has a lower root-mean-square error value and a highercorrelation coe ﬃcient than multiple linear regression (MLR)\\nand logistic regression (LR), indicating better performance.\\n69\\n3.5 Arti \\ue103cial neural network (ANN)\\nAn arti \\ue103cial neural network,70,71which is constructed by\\nneurons, is a type of ML algorithm that simulates biological\\ncranial nerves. The neural network dates back to 1949, whena Canadian psychologist, Donald Hebb, developed a theory oflearning known as Hebbian learning.\\n72However, it was not until\\nFig. 5 (a) Performance of SVM-based classi ﬁers in distinguishing\\nbetween PVPs and non-PVPs. The red arrow denotes the ﬁnal selected\\nmodel. (b) Classi ﬁcation accuracy of the ﬁnal selected model. (c)\\nDistribution of each feature type in the optimal feature set (136features) and original feature set (583 features).\\n59\\nFig. 6 (a) Schematic of a single decision tree.32(b) Taylor diagram of\\ndiﬀerent models for UCS prediction.69\\n3120 |Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 This journal is © The Royal Society of Chemistry 2020Nanoscale Advances Review\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 5}),\n",
       " Document(page_content='nearly two decades later that ANN was largely developed. In the\\nearly period of ANN, a single-layer neural network was \\ue103rst\\nproposed. However, due to some of its speci \\ue103c limitations,\\nresearchers generally turned to multiple-layer neural networksin later studies; these networks consist of an input layer, hiddenlayer and output layer. Fig. 7a shows a typical example ofa multiple layer neural network. Neural network algorithms cananalyze problems e ﬃciently by nonlinear complex interactions\\nbetween neurons. It has been con \\ue103rmed that ANN is very useful\\nin the following three situations:\\n35\\n1. The number of samples or experiments is very large.\\n2. There is not much a priori information about the dataset.\\n3. In the above case, the target is only predicted within the\\nmodel domain.\\nEdward O. Pyzer-Knapp used a special form of neural network\\ncalled a “multilayer perceptron ”to discover new compounds.\\nEdward and his colleagues used the multilayer perceptron to\\ncontinuously \\ue103lter datasets, eliminate data with small correla-\\ntions a \\ue09der each iteration, and then place the remaining data into\\nthe next round of screening, thereby locking the targetcompound while minimizing computational e ﬀort.\\n18Tian Zhang\\nand colleagues proposed a novel approach of using ANN torealize spectrum prediction, performance optimization andinverse design for a plasmonic waveguide-coupled with cavitiesstructure.\\n73Tarak Patra and colleagues built a neural network-\\nbiased genetic algorithm that can discover materials with\\nextremal properties in the absence of pre-existing data. It wasshown to outperform both a nonbiased genetic algorithm anda neural-network-evaluated genetic algorithm based on a pre-existing dataset in a number of test applications. Fig. 7b showsa schematic of the ANN-evaluated genetic algorithm applied inthis experiment\\n74(Fig. 7b). Tian Xie and Je ﬀrey C. Grossman\\ndeveloped a crystal graph convolutional neural network to learn\\nmaterial properties from the connection of crystal atoms and\\naccelerate the design of crystalline materials. This neural networkcould realize the design of crystal materials with very highaccuracy. In practical application, it successfully discovered 33perovskites in the dataset, and it signi \\ue103cantly reduced the search\\nspace of high throughput screening.\\n75\\nNeural network algorithms are also used for drug develop-\\nment. In the past few decades, the connection of computing\\ntechnology and drug development has become increasinglyclose.76At present, many ML algorithms such as neural\\nnetworks have already been applied in the \\ue103eld of drug design;\\nthey can help predict the structure and biological activity oftarget compounds and speci \\ue103cally study their pharmacoki-\\nnetics and toxicology.\\n77–79\\n3.6 Deep learning\\nThe idea of deep learning81,82originated from research of\\na multiple-layer ANN. In some ways, deep learning is a branchsubject of ANN. However, deep learning has already developeda series of research ideas and methods of its own. It is\\na method of characterizing learning based on data. As a new\\n\\ue103eld in machine learning, deep learning aims to build a neural\\nnetwork that mimics the human brain to analyze data. In someways, deep learning is similar to a neural network withmultiple layers. A deep learni ng algorithm can extract the\\nunderlying features in the und erlying network and combine\\nthem with the underlying features in the upper network layersto obtain more abstract high-level features. As the number of\\nneural network layers increases, the features obtained by the\\nalgorithm will be more abstract. At the top level, the \\ue103nal\\nadvanced features are combined, enabling the neural networkto correctly recognize an object. For example, for a square, thedeep learning algorithm will \\ue103rst extract features such as “four\\nline segments ”and “four right angles ”. As the neural network\\nlayer increases, the algorithm will obtain abstract featuressuch as “four line segments connected ”and “four line', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 6}),\n",
       " Document(page_content='line segments ”and “four right angles ”. As the neural network\\nlayer increases, the algorithm will obtain abstract featuressuch as “four line segments connected ”and “four line\\nsegments of equal length ”.F i n a l l y ,t h e s ef e a t u r e sw i l lb e\\ncombined and the algorithm w ill correctly recognize the\\ns q u a r e .O n et h i n gt on o t eh e r ei st h a tu n l i k em o r ep r i m i t i v emachine learning methods, deep learning does not require theresearcher to manually implement the feature engineeringprocessing of the input data; ins tead, the algorithm will self-\\nadjust and choose suitable features independently in contin-uous learning. This can be considered as a major advancement\\nin machine learning.\\nThe idea of deep learning was proposed in 2006. A \\ue09der more\\nthan a decade of research, many important algorithms have beendeveloped, such as convolutional neural networks (CNNs),\\nFig. 7 (a) An example of a feed-forward ANN with Nhidden layers and\\na single neuron in the output layer.80(b) Schematic of an ANN-eval-\\nuated genetic algorithm.74\\nFig. 8 Schematic overview of data annotation and the deep learning\\npipeline in neurodegenerative disease diagnosis.84\\nThis journal is © The Royal Society of Chemistry 2020 Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 | 3121Review Nanoscale Advances\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 6}),\n",
       " Document(page_content='Table 2 An overview of some basic machine learning algorithms\\nAlgorithm Brief introduction Advantages Disadvantages Representative applications\\nRegression analysis It can \\ue103nd regression\\nequations and predictdependent variablesDeeply developed and widely\\nused in many occasionsNeeds large amounts of data\\nand may cause over \\ue103tting in\\npractical applicationsMachine learning with\\nsystematic density-functional theory\\ncalculations: application to\\nmelting temperatures ofsingle-and binary-\\ncomponent solids\\nNa¨ıve Bayes classi \\ue103er It can classify data into\\nseveral categories followingthe highest possibilityOnly a small amount of data\\nis needed to obtain essentialparametersThe feature independence\\nhypothesis is not alwaysaccurateAn a¨ıve-Bayes classi \\ue103er for\\ndamage detection inengineering materials\\nSupport vector machine SVM can \\ue103nd a hyperplane\\nto divide a group of pointsinto two categoriesIt has great generalization\\nability and can properlyhandle high-dimensiondatasetsSVM is not very appropriate\\nfor multiple classi \\ue103cation\\nproblemsPVP-SVM: sequence-based\\nprediction of phage virionproteins using a supportvector machine\\nDecision tree and random\\nforestBy splitting source datasets\\ninto several subsets, all datawill be judged and classi \\ue103edThe calculating processes\\nare easy to comprehend.Also, it can handle large\\namounts of dataIt is diﬃcult to obtain a high-\\nperformance decision tree ora random forest. Also, the\\nover\\ue103tting problem may\\noccurHigh-throughput machine-\\nlearning-driven synthesis offull-Heusler compounds\\nArti\\ue103cial neural network By imitating neuron\\nactivities, ANN can\\nautomatically \\ue103nd\\nunderlying patterns ininputsANN has great self-\\nimproving ability, great\\nrobustness and high fault\\ntoleranceIts inner calculation\\nprogresses are very di ﬃcult\\nto understandLearning from the Harvard\\nClean Energy Project: the use\\nof neural networks to\\naccelerate materialsdiscovery\\nDeep learning Originated from ANN. It\\naims to build a neural\\nnetwork to analyze data byimitating the human brainIt has the best self-adjusting\\nand self-improving abilities\\ncompared with other MLmethodsAs a new trend in ML, deep\\nlearning has not yet been\\nwell studied. Many defectsare still unclearArti\\ue103cial intelligence in\\nneuropathology: deep\\nlearning-based assessmentof tauopathy\\n3122 |Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 This journal is © The Royal Society of Chemistry 2020Nanoscale Advances Review\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 7}),\n",
       " Document(page_content='automatic encoder, sparse coding, restricted Boltzmann machine\\n(RBM), deep belief networks (DBN), and recurrent neural\\nnetworks (RNNs). Currently, deep learning is being widely usedin many \\ue103elds, such as computer vision, image recognition and\\nnatural language recognition. For example, convolutional neuralnetworks are used to detect corrosion in many facilities;\\n83also,\\nMaxim Signaevsky and his coworkers proposed the use of deeplearning algorithms to judge the accumulation of the abnormalprotein TAU to help diagnose neurodegenerative diseases.\\n84Fig. 8\\nshows how they extracted image patches for network training and\\ntested the robustness and reliability of the network with na ¨ıve\\nimages. Izhar Wallach used deep learning to predict the biolog-ical activity of small molecules in drug discovery.\\n85Overall, as\\na new machine learning method, deep learning has excellentdevelopment prospects.\\nTable 2 summarises some basic algorithms used in material\\nscience. In addition to the algorithms mentioned above, many\\nother methods have been experimentally tested. Generally, prac-\\ntical processes are o \\ue09den based on supervised learning, in which\\nresearchers usually combine personal experience and ML algo-rithms. Narrowing down to a speci \\ue103c project, the research idea is\\nnot limited to a certain method, and algorithms are also selectedand designed individually according to the practical situation.\\n4. Cross-validation\\nThe main goal of machine learning is material prediction;therefore, it is necessary to test the quality of the predictingmodel. If the model is not \\ue104exible enough or the volume of\\ninput data is not su ﬃcient enough to \\ue103nd the appropriate\\nphysical chemical rules, the predicting results will not be reli-able. If the model is too complex, the results may be over- \\ue103tted.\\nIn order to avoid these possible risks, researchers must verify\\nthe correctness and reliability of predicting model, and the keyto veri \\ue103cation is using unknown data to test the model and\\ndetermine its accuracy. Here, we will brie \\ue104y introduce several\\nmethods of cross-validation. Additionally, it is important toknow that cross-validation is reliable only when the trainingsets and validation sets can represent the entire dataset.\\n86\\n4.1 Average cross-validation on multiple hold-out estimates\\nThe average cross-validation method87was developed on the\\nbasis of the holdout estimation method. The accuracy of theoriginal holdout validation method was usually lower than ex-pected. Therefore, a \\ue09der improvement by Geisser, it was trans-\\nformed into the average cross-validation method. The average\\ncross-validation method can avoid the random e ﬀects caused\\nby one-time division that may occur in the original method.However, if the volume of data continues to increase, it will leadto very large computational cost and una ﬀordable computa-\\ntional complexity.\\n88,89\\n4.2 Leave- p-out cross-validation and leave-one-out cross-\\nvalidation\\nIn order to reduce computational complexity, researchers\\nproposed leave- p-out cross validation (LPO).90,91In holdoutestimation, the number of subsets for validating a calculation is\\nXn¼1\\np¼1Rnp; however, in LPO, this number decreases to Rnp. In this\\nway, the computational complexity is successfully reduced;\\nhowever, the high computational costs caused by very largeamounts of data are still unacceptable.\\nLeave-one-out cross validation (LOO)\\n92is a special form of\\nleave- p-out cross validation. In LOO, the number p¼1, which\\ndecreases the number of subsets from Rnpton.A\\ue09der years of\\ndevelopment, it is now widely used due to its decreased volume\\nof computation. However, LOO still has some defects. It mayunderestimate the predicting error\\n93and may also lead to\\nover\\ue103tting.94\\n4.3 Repeated learning test cross-validation\\nThe repeated learning test (RTL) cross-validation95was\\nintroduced by Breiman and was further studied by Burmanand Zhang. It divides only a part of dataset instead of thewhole dataset.\\n95–98Compared with previous veri \\ue103cation', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 8}),\n",
       " Document(page_content='introduced by Breiman and was further studied by Burmanand Zhang. It divides only a part of dataset instead of thewhole dataset.\\n95–98Compared with previous veri \\ue103cation\\nmethods, the computational complexity of RLT is signi \\ue103-\\ncantly reduced.\\n4.4 Monte Carlo cross-validation\\nThe Monte Carlo cross-validation (MCCV)99,100is similar to\\nRLT but is easier to operate. The MCCV leaves out somesamples every time for validation, then repeats this proce-dure many times. Khaled Haddad attempted to verify theregional hydrological regression model by LOO and MCCV.Compared with LOO, MCCV can select more simpli \\ue103ed\\nmodels, and it can also better estimate the predicting ability\\nof models.\\n101\\n4.5 K-fold cross-validation\\nK-fold cross-validation102has been proposed as an alternative\\nsolution for LOO. It is now the simplest and most widely usedgeneralization error estimation method. The most obviousadvantage is that only Ktimes calculations are required, and its\\ncalculation cost is far less than the cost of LOO or LPO.However, it should be noted that when the Knumber is not\\nlarge, this method may have larger biases.\\n95\\n4.6 Bootstrap cross-validation\\nBecause K-fold cross-validation tends to have great variations in\\nthe cases of small volumes of sample data, researchersproposed bootstrap cross-validation (BCV).\\n95,103Compared with\\ntraditional validation methods, BCV has less variability andfewer biases under a small amount of samples. However, itmust be noted that the calculation amount of BCV will increasesharply under large samples; therefore, it is not recommendedto use BCV in this situation.\\n103\\nAll the analysis shows many di ﬀerent cross validation\\nmethods and their unique characteristics. As we can see, moreresearch is needed to further improve the cross-validationmethods.\\n98,104,105\\nThis journal is © The Royal Society of Chemistry 2020 Nanoscale Adv. ,2 0 2 0 , 2,3 1 1 5 –3130 | 3123Review Nanoscale Advances\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 8}),\n",
       " Document(page_content='5. Assisting DFT calculations with\\nmachine learning\\nIn this section, we will introduce a novel idea of assisting\\ntraditional DFT calculations with ML. The \\ue103rst part will thor-\\noughly state the theoretical basis of this idea and how it worksin experiments. In the second part, we will discuss several casesof applying this new idea and show its great e ﬀects and\\nprospects.\\n5.1 The theoretical basis of assisting DFT with ML\\nDensity functional theory (DFT) is a quantum mechanical\\nmethod for studying the electronic structures of multi-electron\\nsystems. It has been widely used in material science andcomputational chemistry because of its high calculating accu-racy. However, some defects of DFT are quite obvious in prac-tical application: the calculation method is overly complicated,the calculation processes occupy large amounts of computingresources, and the ability of DFT itself is limited by theexchange function that describes the non-classical interaction\\nbetween electrons. These problems are even more serious when\\nhandling complex materials. At present, the new materialdiscovery mode based on DFT may be considered to be tooexpensive with respect to computing costs and experimentalcosts. Therefore, researchers are attempting to assist or partiallyreplace DFT with machine learning in material discoveryprocedures. As an auxiliary method, machine learning can helpavoid the defects of traditional DFT calculation and improve the\\nexperimental results in practical applications. In fact, it has\\nbeen proved that when the amount of data is su ﬃcient,\\nmachine learning can reproduce the properties of DFT calcu-lations, and the deviation from the DFT values is smaller thanthe deviation of DFT calculation results from experimentalvalues.\\n80,106,107Also, when faced with small amounts of data,\\nresearchers should focus on the dataset itself and attempt toconstruct a more delicate and highly e ﬃcient dataset to elimi-\\nnate this deviation. Although Daniel C. Elton has proved that it\\nis possible to achieve high accuracy using a small amount ofdata, the amount of data is still a limitation of ML methods.There are still many unclear details in this \\ue103eld, and more\\nessential research is need.\\n31\\nFrom the theoretical point of view, the study by Rampi\\nRamprasad revealed an important phenomenon that machinelearning based on data prediction is actually consistent with the\\nnature of scienti \\ue103c processes: it starts with basic observations\\n(data analysis), followed by intuition (predictive) and \\ue103nally by\\nbuilding a quantitative theory (feedback corrections andlearning) that explains the observational phenomena. Becauseof this theoretical support, it is reasonable to assist DFTcalculations with machine learning.\\n9\\nSpeci \\ue103c to the actual research methods, \\ue103rst, researchers\\nneed to represent the various materials in the dataset digitally.\\nEach material (input) should be represented as a string of\\nnumbers, which is called the \\ue103ngerprint vector. The highly\\ndistinctive \\ue103ngerprint vectors are organized by the descriptors\\nand represent the features of the material. Secondly,researchers must establish mapping between input and target\\nfeatures, and this mapping relation is also totally digitized.\\nMany of the ML algorithms mentioned before can be appliedhere. When this mapping is established, researchers have theobjective conditions to predict new materials with similarmaterials.\\nIn addition, complete digital mapping means that\\nresearchers do not need to consider complex physicochemicalrelationships when discovering new materials. Because the\\noriginal materials (input) have all the above physicochemical\\nproperties, correspondingly, the target materials naturallyconform to those physicochemical properties. This is anessential theory that focuses on data, and it can greatly reducethe computational pressure of existing methods.\\n5.2 Practical application cases\\nMany researchers are already involved in this work. For', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 9}),\n",
       " Document(page_content=\"5.2 Practical application cases\\nMany researchers are already involved in this work. For\\nexample, I. E. Castelli and K. W. Jacobsen conducted a studyabout perovskite crystals with the ABO\\n2N cubic structure. They\\nused machine learning instead of traditional DFT calculation to\\ncalculate the tendency of various elements to form perovskites,\\nand they successfully performed simple data manipulation forbandgap evaluation.\\n108Ghanshyam Pilania and colleagues\\nattempted to use chemical structures and electron chargedensity as “\\ue103ngerprint vectors ”to\\ue103nd new material properties.\\nTheir results showed same-level accuracy and lower experi-mental consumption compared with DFT.\\n109However, this\\nmethod has a certain limitation, namely that the maps corre-\\nsponding to each type of material are totally di ﬀerent and can\\nonly be applied to speci \\ue103c types of materials. Under this\\ncircumstance, it is necessary to \\ue103nd a special map for each type\\nof material to meet research needs. It is also worth mentioningthat Logan Ward attempted to replace the dataset with a set ofuniversal material features, then use the feature set as “\\ue103nger-\\nprints ”to\\ue103nd a broad mapping property for the vast majority.\\nThis method can analyze most materials with a general model\\nand avoid calculating a mapping for each material, thus greatly\\nsaving research costs. In this research, they selected the randomforest algorithm to build the model and then tested it twice. Inthe\\ue103rst experiment (using a DFT-validated property database to\\npredict new solar cell materials), the model showed excellentperformance, with the lowest average absolute error in tencross-validations. The second experiment (exploring new metal-glass alloys using experimentally measured data on the ability\\nof crystalline compounds to form glass) showed fairly high\\naccuracy of 90.1% in ten cross-validations.\\n110\\nIn addition, in one of the most typical cases in the \\ue103eld of\\nassisting DFT with machine learning algorithms, Shuaihua Lu'steam used this method to \\ue103nd suitable mixed organic –inor-\\nganic calcium –titanium (HOIP) as a new type of photovoltaic\\nmaterial. The researchers used the machine learning method totrain the bandgap model and selected four ideal solar cell\\nproperties as the measurement indicators; then, they success-\\nfully selected six suitable materials from 5158 undetected targetcompounds. The results are listed in Fig. 9. As the iterationnumbers increased, the deviation between the training and\\n3124 |Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 This journal is © The Royal Society of Chemistry 2020Nanoscale Advances Review\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 9}),\n",
       " Document(page_content='testing sets decreased (Fig. 9a). The distribution of the post-\\npredicted bandgap was also very close to the original input\\nsets (Fig. 9b). Other hidden trends and periodicities were alsounraveled by data visualization. The results are shown inFig. 9c –f, which is divided according to the position of the X-site\\nelement (F, Cl, Br and I). Di ﬀerent from traditional methods,\\nDFT was only used here to help calculate the most probabletarget materials rather than all target substances, thus greatlyreducing the computational costs.\\n28\\nIn practical application, the experimental accuracy of ML can\\nalso be maintained. For example, Prasanna Balachandran andcolleagues constructed a new ML material discovery method byconstructing orbital radii based on data and applied thismethod from relatively simple AB compounds to more elec-tronically complex RM intermetallic compounds. The resultsshowed great agreement of the classi \\ue103cation rules extracted\\nfrom both ML and DFT calculations.\\n111In another case, Daniele\\nDragoni constructed a Gaussian approximation potential (GAP)\\nmodel and trained it with DFT data from 150k atoms. A \\ue09der\\n\\ue103nishing the construction, the researchers veri \\ue103ed the accuracyof the model with another group of DFT data. The results show\\nthat this model can reproduce DFT calculations and maintain\\nvery high accuracy. For example, the value of the bulk modulusby GAP is 198.2, while that by DFT is 199.8 /C60.1 (ref. 7). The\\ndeviation between these two methods is quite small.\\n112\\nFurthermore, many other cases have proved the high accuracyof the ML method; for example, Felix Faber used 13 di ﬀerent\\nquantum properties from over 117k atoms to test the accuracyof ML and DFT calculations, and it was shown that ML can\\nde\\ue103nitely reach the accuracy of DFT.\\n113Albert P. Bart ´ok and\\ncoworkers built a ML model that can distinguish active andinactive protein ligands with more than 99% reliability. Thisexperience-dependent method could reach the same level ofaccuracy as DFT with quite low cost.\\n114Ryosuke Jinnouchi and\\nRyoji Asahi attempted to use ML to detect the catalytic activity ofnanoparticles with DFT data on single crystals. The accuracycould also meet the expectations of practical application.\\n115\\nIn addition to all the practical examples above, researchers\\nhave attempted to optimize the DFT calculation process by ML.Solving the Kohn –Sham equation is a necessary step in the DFT\\ncalculation process, and this very step is also the most time-consuming part of the DFT calculation process. In this \\ue103eld,\\nJohn C. Snyder and Felix Brockherde have already obtainedsome signi \\ue103cant results.\\n116,117These results show that taking\\nadvantage of the \\ue104exibility and e ﬃciency of ML can highly\\nreduce the computational cost of DFT, which can decrease the\\ncalculation time and increase the calculation speed.\\nFrom these examples, we can see that the methods and ideas\\nof ML have brought great convenience to material research.Because ML is a pure data operation, the computer can quicklydetermine all physical and chemical rules using su ﬃcient data\\nand the correct algorithm whether they are hidden or discovered,which may someday back-feed theoretical chemistry.\\n118The\\nmethod of ML combined with data operation has less computa-\\ntional complexity and computational cost compared with tradi-tional DFT calculation. However, the accuracy of this new idea iscurrently below expectations. Ying Zhang and his team haveattempted to introduce additional parameters into the calcula-tion to improve the accuracy of the models; however, the bestmodels still do not have the high accuracy of traditional DFTcalculations.\\n36From this point of view, although the idea of ML\\ncombined with data operation can bring great changes to current\\nmaterial science research, there are also defects to be overcome.\\n6. Arti ﬁcial intelligence-assisted new\\nmaterial development\\nArti\\ue103cial intelligence (AI) is the subject of computer science', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 10}),\n",
       " Document(page_content='material science research, there are also defects to be overcome.\\n6. Arti ﬁcial intelligence-assisted new\\nmaterial development\\nArti\\ue103cial intelligence (AI) is the subject of computer science\\nsimulating human intelligence. Since its birth in 1950, it hasgone through many ups and downs. Currently, due to thedevelopment of big data and computer technology, the theo-retical system of AI is hugely enriched. AI now has many\\nsub\\ue103elds, such as data mining, computer vision, and machine\\nlearning. Moreover, it has shown great potential and ability inmaterial science, and it is widely used in material design,corrosion detection, material screening and many other \\ue103elds\\nFig. 9 Results and insights from the ML model. (a) The ﬁtting results of\\nthe test bandgaps EPBE\\ngand predicted bandgaps EML\\ng. (b) Scatter plots of\\ntolerance factors against the bandgaps for the prediction dataset from\\nthe trained ML model (the blue, red and dark gray plots represent the\\ntraining, testing and prediction sets, respectively). Data visualization ofpredicted bandgaps for all possible HOIPs (each color representsa class of halogen perovskites) with the (c) tolerance factor, (d) octa-\\nhedral factor, (e) ionic polarizability for the A-site ions, and (f) elec-\\ntronegativity of the B-site ions. The dotted boxes represent the mostappropriate range for each feature.\\n28\\nThis journal is © The Royal Society of Chemistry 2020 Nanoscale Adv. ,2 0 2 0 , 2,3 1 1 5 –3130 | 3125Review Nanoscale Advances\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 10}),\n",
       " Document(page_content='of material science. In this section, we will introduce some\\nsub\\ue103elds of AI and their applications in material science.119\\n6.1 Inverse design for desired compounds\\nInverse design aims to \\ue103nd materials with desired particular or\\nmaterial functionalities. Inverse design is signi \\ue103cantly di ﬀerent\\nfrom forward development. The traditional forward design is toobtain the target materials thro u g he x p e r i m e n t sa n dt h e nf u r t h e r\\njudge the functionalities of the materials. Inverse design hasa more obvious goal-oriented characteristic. It starts from thedesired properties and ends in chemical space. The di ﬀerence is\\ni l l u s t r a t e di nF i g .1 0 a .H o w e v e r ,i n v e r s ed e s i g nf a c e sav e r yo b v i o u sproblem. Because the aim of inverse design is to \\ue103nd suitable\\nmaterials based on functionalitie s, we can consider this process as\\nstarting from speci \\ue103cc o n d i t i o n st o \\ue103nd possible solutions among\\na large range of candidate materi als and material combinations.\\nAccording to current research, the optimal solution may or maynot exist, and there may be one or more solutions. However, in theprocess of searching for the optimal solution, the number ofcandidates waiting for analysis will also increase tremendously.For example, for pharmacologically relevant small molecules, the\\nnumber of structures is considered to be nearly 10\\n60.120Data of this\\nvolume cannot be veri \\ue103ed manually. Therefore, we introduce ML\\nin this process to help researchers with the analysis.\\nA basic idea of inverse design originates from high-throughput\\nvirtual screening. This is a data-driven experimental idea. Thereare many successful application examples. For example,Benjamin Sanchez-Lengeling\\n121and Al ´an Aspuru-Guzik once\\nthoroughly explored the inverse design method. From the current\\npoint of view, the research idea and implementation method of\\ninverse design are quite diversiform; however, they are still notmature. Some speci \\ue103c procedures, such as the digital represen-\\ntation of molecules, the selection of ML methods, and the designof inverse design tools, still need further study.\\n1216.2 Computer vision for material-picture analysis\\nComputer vision is de \\ue103ned as an arti \\ue103cial intelligence system\\nthat can extract information from images or multidimensionaldata. Computer vision technology originated very early; however,it did not receive attention or development until the 1970s, whenthe ability of computers su ﬃciently improved to handle large-\\nscale data such as images. For example, in the late 1980s, Y.\\nLeCun used the letters in the US postal system as an example and\\napplied computer vision technology to analyze and determine thehandwritten zip codes on letters.\\n122Today, computer vision has\\nbecome an interdisciplinary technology that is widely used invarious related \\ue103elds, and it has also been applied in many\\nsub\\ue103elds of material science. In material science, computer\\nvision can analyze unclear or unknown material properties fromenormous amounts of \\ue103gures, which can greatly help scientists\\nto understand the physical/chemical properties and inner rela-\\ntionships of similar materials. In this way, scientists can betterdesign and construct novel target materials. Examples includedetecting corrosion of concrete railway ties by analyzing railpictures\\n123and exploring the particle morphologies and surface\\ntextures of powders by analyzing microstructure pictures of thepowders for additive manufacturing processes.\\n124In the \\ue103rst case,\\nwe can see that the computer divides every tie in four pictures\\nand separately evaluates them to detect the conditions of crum-\\nbling and chipping (Fig. 10b). In the last case, researchers useda“bag of visual words ”method, and the \\ue103nal classi \\ue103cation\\naccuracy was 80 –85%. In another typical case, M. X. Bastidas-', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 11}),\n",
       " Document(page_content='bling and chipping (Fig. 10b). In the last case, researchers useda“bag of visual words ”method, and the \\ue103nal classi \\ue103cation\\naccuracy was 80 –85%. In another typical case, M. X. Bastidas-\\nRodriguez used machine learning algorithms (ANN and SVM)to classify the fracture of metal materials for failure analysis. Theresults showed that the performance of the ANN application wasslightly stronger than that of SVM, and the highest accuracy\\npercentage was 84.95%.\\n125In addition, some researchers have\\nfurther explored the theory of computer vision. For example,Brian L. DeCost and Elizabeth A. Holm attempted to developa general method to \\ue103nd useful characteristics and relationships\\nof massive microstructures in large and diverse microstructuralimage databases. In this research, a “visual dictionary ”method\\nwas created to detect the characteristics of images by extractingmeaningful details and features in pictures to construct “visual\\nwords ”and by expressing the pictures as the probability distri-\\nbution of the visual words.\\n126Moreover, computer vision plays an\\nimportant role in material classi \\ue103cation. For example, Hua Chen\\nand colleagues proposed a polarization phase-based computervision method for material classi \\ue103cation according to intrinsic\\nelectrical conductivity. This method is computationally e ﬃcient\\nand can be achieved with existing image technology.\\n127\\n6.3 High-throughput screening and big data in materialdiscovery\\nHigh-throughput screening in the \\ue103eld of novel material\\ndiscovery uses tremendous volumes of data to performcomputational tasks to detect material properties and design\\ntarget materials. Big data can be de \\ue103ned as a research method\\nthat extracts information and detects relationships fromextraordinarily large datasets. These two ideas are now o \\ue09den\\nused in novel material discovery.\\n128–130Researchers collect very\\nFig. 10 (a) Schematic of di ﬀerent approaches toward molecular\\ndesign. Inverse design starts from desired properties and ends in\\nchemical space, unlike the direct approach, which leads from chem-ical space to the desired properties.\\n121(b) Computer vision analysis of\\npictures to detect rail defects.123(c) Computational high-throughput\\nscreening for | DGH| on 256 pure metals and surface alloys. The rows\\nindicate the identities of the pure metal substrates, the columnsindicate the identities of the solutes embedded in the surface layers ofthe substrates, and the diagonal of the plot corresponds to the\\nhydrogen adsorption free energy of the pure metals.\\n131\\n3126 |Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 This journal is © The Royal Society of Chemistry 2020Nanoscale Advances Review\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 11}),\n",
       " Document(page_content=\"large volumes of data about target materials and use high-\\nthroughput screening to analyze the properties of the mate-\\nrials or the possibility of synthetizing target materials. Consid-ering the need for data when applying ML, these two methodshave literally become the foundations of the novel materialdiscovery \\ue103eld. There are many cases to prove this; for example,\\nCourtney R. Thomas's team built high-throughput screening toestimate the safety, nanotoxicology, ecotoxicology, environ-mental assessment and other properties of engineered nano-\\nmaterials.\\n131JeﬀGreeley and his colleagues used DFT-based\\nhigh-throughput screening to estimate the activity of over 700binary surface alloys to \\ue103nd an appropriate electro-catalyst for\\nthe hydrogen evolution reaction, and they successfully identi-\\ue103ed BiPt as the desired target material\\n132(Fig. 10). Kirill Slioz-\\nberg's team created an automated optical scanning droplet cellfor high-throughput screening and high-throughput prepara-tion. They applied this tool in the evaluation of thin- \\ue103lm Ti –W–\\nO and thin- \\ue103lm Fe –W–O to seek e ﬃcient semiconductors.\\n133,134\\nIn another case, Ankit Agrawal and Alok Choudhary systemi-\\ncally described the important role currently played by big datain materials informatics.\\n135\\nIn addition to ML, other AI algorithms are widely used in the\\n\\ue103eld of novel material discovery, and there are numerous cases\\nshowing the importance, e ﬀects, and development prospects of\\nAI. In summary, AI has strong e ﬀects and a bright future in\\nmaterials science; however, it still needs further development.\\n7. Prospects and conclusion\\nLed by MGI, the era of data-driven material discovery has\\narrived. Over the past decades, with the development ofcomputing technology, the progress of AI science, and abun-dant experimental results, this new material discovery methodhas generally become a research paradigm. As the researchdeepens, it is showing many advancing abilities, such as low\\nexperimental consumption, low time consumption, high\\ngeneralizing ability and high density analysis. Currently, it isused in basic chemistry, pharmaceutical science and materialsscience, such as terahertz spectral analysis and recognition,\\n136\\nprediction of the melting temperatures of binary compounds48\\nand the band gap energies of certain crystals,137,138and analysis\\nof complex reaction networks.139\\nHowever, this method also has certain defects. The accuracy\\nof the results highly depends on the quality of the data and\\nalgorithms. Defects such as computing consumption, reliabilityof algorithms and dependence of data must be overcome.Measures must be taken to improve this method, includingestablishing reliable databases, enhancing the combination ofML with other material theories, and exploring other newmaterial research methods, such as inverse design. Currently,we are making progress in this \\ue103eld and gradually improving\\nthis method. With the development of research, more impor-\\ntant eﬀects will be revealed.\\nConﬂicts of interest\\nThere are no con \\ue104icts to declare.Acknowledgements\\nDr J. Wei acknowledges that this project was funded by China\\nPostdoctoral Science Foundation (no. 2017 M620694) andNational Postdoctoral Program for Innovative Talents(BX201700040).\\nNotes and references\\n1 M. I. Jordan and T. M. Mitchell, Science , 2015, 349, 255 –260.\\n2 A. L. Blum and P. Langley, Artif. Intell. , 1997, 97, 245 –271.\\n3 E. Lopez, D. Gonzalez, J. V. Aguado, E. Abisset-Chavanne,\\nE. Cueto, C. Binetruy and F. Chinesta, Arch. Comput.\\nMethods Eng. , 2016, 25,5 9–68.\\n4 W. Lu, R. Xiao, J. Yang, H. Li and W. Zhang, J. Materiomics ,\\n2017, 3, 191 –201.\\n5 P. Raccuglia, K. C. Elbert, P. D. Adler, C. Falk, M. B. Wenny,\\nA. Mollo, M. Zeller, S. A. Friedler, J. Schrier andA. J. Norquist, Nature , 2016, 533,7 3–76.\\n6 X. Yang, J. Wang, J. Ren, J. Song, Z. Wang, Z. Zeng, X. Zhang,\\nS. Huang, P. Zhang and H. Lin, Chinese Journal of\\nComputational Physics , 2017, 34, 697 –704.\\n7 H. Lin, J. Zheng, L. Yuan and P. Feng, Energy Storage Sci.\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 12}),\n",
       " Document(page_content=\"S. Huang, P. Zhang and H. Lin, Chinese Journal of\\nComputational Physics , 2017, 34, 697 –704.\\n7 H. Lin, J. Zheng, L. Yuan and P. Feng, Energy Storage Sci.\\nTechnol. , 2017, 6, 990 –999.\\n8 X. Yang, J. Ren, J. Wang, X. Zhao, Z. Wang and S. Jianlong,\\nSci. Technol. Rev. , 2016, 34,6 2–67.\\n9 R. Ramprasad, R. Batra, G. Pilania, A. Mannodi-\\nKanakkithodi and C. Kim, npj Comput. Mater. , 2017, 3,1–13.\\n10 P. De Luna, J. Wei, Y. Bengio, A. Aspuru-Guzik and\\nE. Sargent, Nature , 2017, 552,2 3–27.\\n11 A. Zheng and A. Casari, Feature engineering for machine\\nlearning: principles and techniques for data scientists ,\\nO'Reilly Media, Inc., Sebastopol, State of California, USA,1st edn, 2018.\\n12 Y. Liu, T. Zhao, W. Ju and S. Shi, J. Materiomics , 2017, 3,\\n159–177.\\n13 L. Yang and G. Ceder, Phys. Rev. B: Condens. Matter Mater.\\nPhys. , 2013, 88, 224107.\\n14 K. T. Sch ¨utt, H. Glawe, F. Brockherde, A. Sanna, K. R. M\\n¨uller\\nand E. K. U. Gross, Phys. Rev. B: Condens. Matter Mater.\\nPhys. , 2014, 89, 205118.\\n15 H. K. D. H. Bhadeshia, R. C. Dimitriu, S. Forsik, J. H. Pak\\nand J. H. Ryu, Mater. Sci. Technol. , 2013, 25, 504 –510.\\n16 H. Yin, X. Jiang, R. Zhang, G. Liu, Q. Zheng and Q. Xuanhui,\\nMaterials China , 2017, 36, 401 –405+454.\\n17 A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards,\\nS. Dacek, S. Cholia, D. Gunter, D. Skinner and G. Ceder,APL Mater. , 2013, 1, 011002.\\n18 E. O. Pyzer-Knapp, K. Li and A. Aspuru-Guzik, Adv. Funct.\\nMater. , 2015, 25, 6495 –6502.\\n19 K. Choudhary, I. Kalish, R. Beams and F. Tavazza, Sci. Rep. ,\\n2017, 7,1–16.\\n20 S. Leb `egue, T. Bj ¨orkman, M. Klintenberg, R. M. Nieminen\\nand O. Eriksson, Phys. Rev. X , 2013, 3, 031002.\\n21 M. Ashton, J. Paul, S. B. Sinnott and R. G. Hennig, Phys. Rev.\\nLett., 2017, 118, 106101.\\nThis journal is © The Royal Society of Chemistry 2020 Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 | 3127Review Nanoscale Advances\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online\", metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 12}),\n",
       " Document(page_content='22 N. Mounet, M. Gibertini, P. Schwaller, D. Campi, A. Merkys,\\nA. Marrazzo, T. Sohier, I. E. Castelli, A. Cepellotti, G. Pizzi\\nand N. Marzari, Nat. Nanotechnol. , 2018, 13, 246 –252.\\n23 S. Haastrup, M. Strange, M. Pandey, T. Deilmann,\\nP. S. Schmidt, N. F. Hinsche, M. N. Gjerding, D. Torelli,P. M. Larsen, A. C. Riis-Jensen, J. Gath, K. W. Jacobsen,J. Jørgen Mortensen, T. Olsen and K. S. Thygesen, 2D\\nMaterials , 2018, 5, 042002.\\n24 S. Kotsiantis, D. Kanellopoulos and P. Pintelas, Int. J.\\nComput. Sci. , 2006, 1, 111 –117.\\n25 A. Holzinger, 2018 World Symposium on Digital Intelligence\\nfor Systems and Machines (DISA) , Kosice, 2018.\\n26 J. H. Friedman, Computing Science and Statistics , 1998, vol.\\n29, pp. 3 –9.\\n27 K. Lakshminarayan, S. A. Harp and T. Samad, Applied\\nIntelligence , 1999, 11, 259 –275.\\n28 S. Lu, Q. Zhou, Y. Ouyang, Y. Guo, Q. Li and J. Wang, Nat.\\nCommun. , 2018, 9,1–8.\\n29 M. Wang, T. Wang, P. Cai and X. Chen, Small Methods ,\\n2019, 3, 1900025.\\n30 L. M. Ghiringhelli, J. Vybiral, S. V. Levchenko, C. Draxl and\\nM. Sche ﬄer,Phys. Rev. Lett. , 2015, 114, 105503.\\n31 D. C. Elton, Z. Boukouvalas, M. S. Butrico, M. D. Fuge and\\nP. W. Chung, Sci. Rep. , 2018, 8, 9059.\\n32 A. O. Oliynyk, E. Antono, T. D. Sparks, L. Ghadbeigi,\\nM. W. Gaultois, B. Meredig and A. Mar, Chem. Mater. ,\\n2016, 28, 7324 –7331.\\n33 F. Legrain, J. Carrete, A. van Roekeghem, S. Curtarolo and\\nN. Mingo, Chem. Mater. , 2017, 29, 6220 –6227.\\n34 S. Kajita, N. Ohba, R. Jinnouchi and R. Asahi, Sci. Rep. ,\\n2017, 7,1–9.\\n35 P. Pankajakshan, S. Sanyal, O. E. de Noord, I. Bhattacharya,\\nA. Bhattacharyya and U. Waghmare, Chem. Mater. , 2017, 29,\\n4190 –4201.\\n36 Y. Zhang and C. Ling, npj Comput. Mater. , 2018, 4,1–8.\\n37 A. Jain and T. Bligaard, Phys. Rev. B: Condens. Matter Mater.\\nPhys. , 2018, 98, 214112.\\n38 M. Seeger, Int. J. Neural Syst. , 2004, 14,6 9–106.\\n39 D. H. Wolpert and W. G. Macready, IEEE Trans. Evol.\\nComput. , 1997, 1,6 7–82.\\n40 G. W. Haggstrom, J. Bus. Econ. Stat. , 1983, 1, 229 –238.\\n41 S. Wold, M. Sj ¨ostr¨om and L. Eriksson, Chemom. Intell. Lab.\\nSyst., 2001, 58, 109 –130.\\n42 R. Wehrens and B.-H. Mevik, J. Stat. So \\ue09dw., 2007, 18,1–23.\\n43 V. Esposito Vinzi and G. Russolillo, Wiley Interdiscip. Rev.\\nComput. Stat. , 2013, 5,1–19.\\n44 K.-R. M ¨uller, S. Mika, G. R ¨atsch, K. Tsuda and B. Sch ¨olkopf,\\nIEEE Trans. Neural Netw. , 2001, 12, 181 –201.\\n45 C.-C. Chang and C.-J. Lin, ACM Trans. Intell. Syst. Technol. ,\\n2011, 2,1–27.\\n46 C. K. Williams, in Learning in graphical models , ed. M. I.\\nJordan, Springer Science & Business Media, Dordrecht,The Netherlands, 1st edn, 1998, ch. 23, vol. 89, pp. 599 –621.\\n47 C. E. Rasmussen, Summer School on Machine Learning ,\\nT¨ubingen,Germany, 2003.\\n48 A. Seko, T. Maekawa, K. Tsuda and I. Tanaka,\\nPhys. Rev. B:\\nCondens. Matter Mater. Phys. , 2014, 89, 054303.49 S. Curtarolo, D. Morgan, K. Persson, J. Rodgers and\\nG. Ceder, Phys. Rev. Lett. , 2003, 91, 135503.\\n50 B. Tutmez and A. Dag, Comput. Concrete , 2012, 10, 457 –467.\\n51 I. Rish, IJCAI 2001 Workshop on Empirical Methods in Arti \\ue103cial\\nIntelligence , Seattle,State of Washington,USA, 2001.\\n52 D. D. Lewis, European Conference on Machine Learning ,\\nChemnitz, Germany, 1998.\\n53 Q. Wang, G. M. Garrity, J. M. Tiedje and J. R. Cole, Appl.\\nEnviron. Microbiol. , 2007, 73, 5261 –5267.\\n54 O. Addin, S. M. Sapuan, E. Mahdi and M. Othman, Mater.\\nDes., 2007, 28, 2379 –2386.\\n55 H. Liu, X. Song, J. Bimbo, L. Seneviratne and K. Althoefer,\\n2012 IEEE/RSJ International Conference on Intelligent Robotsand Systems , Algarve, Portugal, 2012.\\n56 C. J. Burges, Data Min. Knowl. Discov. , 1998, 2, 121 –167.\\n57 M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt and\\nB. Scholkopf, IEEE Intelligent Systems and Their\\nApplications , 1998, 13,1 8–28.\\n58 X. Qiu, D. Fu, Z. Fu, K. Riha and R. Burget, 2011 34th\\nInternational Conference on Telecommunications and SignalProcessing (TSP) , Budapest, Hungary, 2011.\\n59 B. Manavalan, T. H. Shin and G. Lee, Front. Microbiol. , 2018,\\n9, 476.', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 13}),\n",
       " Document(page_content='International Conference on Telecommunications and SignalProcessing (TSP) , Budapest, Hungary, 2011.\\n59 B. Manavalan, T. H. Shin and G. Lee, Front. Microbiol. , 2018,\\n9, 476.\\n60 M. K. Warmuth, J. Liao, G. Ratsch, M. Mathieson, S. Putta\\nand C. Lemmen, J. Chem. Inf. Model. , 2003, 43, 667 –673.\\n61 J. R. Quinlan, Mach. Learn. , 1986, 1,8 1–106.\\n62 A. Ehrenfeucht and D. Haussler, Inf. Comput. , 1989, 82,\\n231–246.\\n63 S. R. Safavian and D. Landgrebe, IEEE Trans. Syst. Man\\nCybern. , 1991, 21, 660\\n–674.\\n64 L. Breiman, Mach. Learn. , 2001, 45,5–32.\\n65 A. Liaw and M. Wiener, R News , 2002, 2,1 8–22.\\n66 B. Meredig, A. Agrawal, S. Kirklin, J. E. Saal, J. W. Doak,\\nA. Thompson, K. Zhang, A. Choudhary and C. Wolverton,\\nPhys. Rev. B: Condens. Matter Mater. Phys. , 2014, 89, 094104.\\n67 J. Carrete, W. Li, N. Mingo, S. Wang and S. Curtarolo, Phys.\\nRev. X , 2014, 4, 011019.\\n68 D. T. Ahneman, J. G. Estrada, S. Lin, S. D. Dreher and\\nA. G. Doyle, Science , 2018, 360, 186 –190.\\n69 J. Zhang, G. Ma, Y. Huang, J. sun, F. Aslani and B. Nener,\\nConstr. Build. Mater. , 2019, 210, 713 –719.\\n70 G. P. Zhang, IEEE Trans. Syst. Man Cybern. C Appl. Rev. ,\\n2000, 30, 451 –462.\\n71 G. B. Goh, N. O. Hodas and A. Vishnu, J. Comput. Chem. ,\\n2017, 38, 1291 –1307.\\n72 H. D. Olding, The organization of behavior: A\\nneuropsychological theory , Psychology Press, Mahwah,\\nState of New Jersey, USA, 1st edn, 2005.\\n73 T. Zhang, J. Wang, Q. Liu, J. Zhou, J. Dai, X. Han, Y. Zhou\\nand K. Xu, Photonics Res. , 2019, 7, 368 –380.\\n74 T. K. Patra, V. Meenakshisundaram, J.-H. Hung and\\nD. S. Simmons, ACS Comb. Sci. , 2017, 19,9 6–107.\\n75 T. Xie and J. C. Grossman, Phys. Rev. Lett. , 2018, 120,\\n145301.\\n76 C. Nantasenamat, C. Isarankura-Na-Ayudhya and\\nV. Prachayasittikul, Expert Opin. Drug Discovery , 2010, 5,\\n633–654.\\n3128 |Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 This journal is © The Royal Society of Chemistry 2020Nanoscale Advances Review\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 13}),\n",
       " Document(page_content='77 V. G. Maltarollo, J. C. Gertrudes, P. R. Oliveira and\\nK. M. Honorio, Expert Opin. Drug Metab. Toxicol. , 2015,\\n11, 259 –271.\\n78 T. Fox and J. M. Kriegl, Curr. Top. Med. Chem. , 2006, 6,\\n1579 –1591.\\n79 A. N. Lima, E. A. Philot, G. H. Trossini, L. P. Scott,\\nV. G. Maltarollo and K. M. Honorio, Expert Opin. Drug\\nDiscovery , 2016, 11, 225 –239.\\n80 G. R. Schleder, A. C. M. Padilha, C. M. Acosta, M. Costa and\\nA. Fazzio, Journal of Physics: Materials , 2019, 2, 032001.\\n81 Li Deng and D. Yu, Signal Process. , 2014, 7, 197 –387.\\n82 Y. LeCun, Y. Bengio and G. Hinton, Nature , 2015, 521, 436 –\\n444.\\n83 W. Nash, T. Drummond and N. Birbilis, npj Mater. Degrad. ,\\n2018, 2,1–12.\\n84 M. Signaevsky, M. Prastawa, K. Farrell, N. Tabish,\\nE. Baldwin, N. Han, M. A. Iida, J. Koll, C. Bryce,\\nD. Purohit, V. Haroutunian, A. C. McKee, T. D. Stein,\\nC. L. White 3rd, J. Walker, T. E. Richardson, R. Hanson,M. J. Donovan, C. Cordon-Cardo, J. Zeineh, G. Fernandezand J. F. Crary, Lab. Invest. , 2019, 99, 1019.\\n85 I. Wallach, M. Dzamba and A. Heifets, Abstr. Pap. Am. Chem.\\nSoc., 2016, 251,1 .\\n86 K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev and\\nA. Walsh, Nature , 2018, 559, 547 –555.\\n87 S. Geisser, J. Am. Stat. Assoc. , 1975, 70, 320 –328.\\n88 A. Luntz, Technicheskaya Kibernetica , 1969, vol. 3.\\n89 L. Bo, L. Wang and L. Jiao, Neural Comput. , 2006, 18, 961 –\\n978.\\n90 C. R. Rao and Y. Wu, J. Stat. Plan. Inference , 2005, 128, 231 –\\n240.\\n91 A. Celisse and S. Robin, Comput. Stat. Data Anal.\\n, 2008, 52,\\n2350 –2368.\\n92 M. Kearns and D. Ron, Neural Comput. , 1999, 11, 1427 –\\n1453.\\n93 B. Efron, J. Am. Stat. Assoc. , 1986, 81, 461 –470.\\n94 A. K. Smilde, J. Qual. Technol. , 2018, 34, 464 –465.\\n95 P. Burman, Biometrika , 1989, 76, 503 –514.\\n96 C. Nadeau and Y. Bengio, Advances in Neural Information\\nProcessing Systems , Denver, Colorado, USA, 2000.\\n97 P. Zhang, Ann. Stat. , 1993, 21, 299 –313.\\n98 S. Arlot and A. Celisse, Stat. Surv. , 2010, 4,4 0–79.\\n99 R. R. Picard and R. D. Cook, J. Am. Stat. Assoc. , 1984, 79,\\n575–583.\\n100 Q.-S. Xu and Y.-Z. Liang, Chemom. Intell. Lab. Syst. , 2001, 56,\\n1–11.\\n101 K. Haddad, A. Rahman, M. A. Zaman and S. Shrestha, J.\\nHydrol. , 2013, 482, 119 –128.\\n102 J. D. Rodriguez, A. Perez and J. A. Lozano, IEEE Trans.\\nPattern Anal. Mach. Intell. , 2010, 32, 569 –575.\\n103 W. J. Fu, R. J. Carroll and S. Wang, Bioinformatics , 2005, 21,\\n1979 –1986.\\n104 W. Y. Yang Liu, Application Research of Computers , 2015, 32,\\n1287 –1290, 1297.\\n105 S. Borra and A. Di Ciaccio, Comput. Stat. Data Anal. , 2010,\\n54, 2976 –2989.106 L. Ward, R. Liu, A. Krishna, V. I. Hegde, A. Agrawal,\\nA. Choudhary and C. Wolverton, Phys. Rev. B: Condens.\\nMatter Mater. Phys. , 2017, 96, 024104.\\n107 F. A. Faber, L. Hutchison, B. Huang, J. Gilmer,\\nS. S. Schoenholz, G. E. Dahl, O. Vinyals, S. Kearnes,P. F. Riley and O. A. von Lilienfeld, J. Chem. Theory\\nComput. , 2017, 13, 5255 –5264.\\n108 I. E. Castelli and K. W. Jacobsen, Modell. Simul. Mater. Sci.\\nEng., 2014, 22, 055007.\\n109 G. Pilania, C. Wang, X. Jiang, S. Rajasekaran and\\nR. Ramprasad, Sci. Rep. , 2013, 3, 2810.\\n110 L. Ward, A. Agrawal, A. Choudhary and C. Wolverton, npj\\nComput. Mater. , 2016, 2, 16028.\\n111 P. V. Balachandran, J. Theiler, J. M. Rondinelli and\\nT. Lookman, Sci. Rep. , 2015, 5, 13285.\\n112 D. Dragoni, T. D. Da ﬀ,G .C s ´anyi and N. Marzari, Phys. Rev.\\nMater. , 2018, 2, 013808.\\n113 F. A. Faber, L. Hutchison, B. Huang, J. Gilmer,\\nS. S. Schoenholz, G. E. Dahl, O. Vinyals, S. Kearnes,P. F. Riley and O. A. von Lilienfeld, 2017, arXiv:05532.\\n114 A. P. Bart ´ok, S. De, C. Poelking, N. Bernstein, J. R. Kermode,\\nG. Cs ´anyi and M. Ceriotti, Sci. Adv. , 2017, 3, e1701816.\\n115 R. Jinnouchi and R. Asahi, J. Phys. Chem. Lett. , 2017, 8,\\n4279 –4283.\\n116 J. C. Snyder, M. Rupp, K. Hansen, K. R. Muller and\\nK. Burke, Phys. Rev. Lett. , 2012, 108, 253002.\\n117 F. Brockherde, L. Vogt, L. Li, M. E. Tuckerman, K. Burke\\nand K. R. Muller, Nat. Commun. , 2017, 8, 872.\\n118 S. V. Kalinin, B. G. Sumpter and R. K. Archibald, Nat.', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 14}),\n",
       " Document(page_content='117 F. Brockherde, L. Vogt, L. Li, M. E. Tuckerman, K. Burke\\nand K. R. Muller, Nat. Commun. , 2017, 8, 872.\\n118 S. V. Kalinin, B. G. Sumpter and R. K. Archibald, Nat.\\nMater. , 2015, 14, 973 –980.\\n119 M. Haenlein and A. Kaplan, Calif. Manage. Rev. , 2019, 61,5\\n–\\n14.\\n120 A. M. Virshup, J. Contreras-Garc ´ıa, P. Wipf, W. Yang and\\nD. N. Beratan, J. Am. Chem. Soc. , 2013, 135, 7296 –7303.\\n121 B. Sanchez-Lengeling and A. Aspuru-Guzik, Science , 2018,\\n361, 360 –365.\\n122 Y. LeCun, B. Boser, J. S. Denker, D. Henderson,\\nR. E. Howard, W. Hubbard and L. D. Jackel, Neural\\nComput. , 1989, 1, 541 –551.\\n123 X. Gibert, V. M. Patel and R. Chellappa, IEEE Trans. Intell.\\nTransp. Syst. , 2017, 18, 153 –164.\\n124 B. L. DeCost and E. A. Holm, Comput. Mater. Sci. , 2017, 126,\\n438–445.\\n125 M. X. Bastidas-Rodriguez, F. A. Prieto-Ortiz and E. Espejo,\\nEng. Failure Anal. , 2016, 59, 237 –252.\\n126 B. L. DeCost and E. A. Holm, Comput. Mater. Sci. , 2015, 110,\\n126–133.\\n127 H. Chen and L. B. Wol ﬀ,Int. J. Comput. Vis. , 1998, 28,7 3–83.\\n128 C. L. Philip Chen and C.-Y. Zhang, Inf. Sci. , 2014, 275, 314 –\\n347.\\n129 C. Xue-Wen and L. Xiaotong, IEEE Access , 2014, 2, 514 –525.\\n130 L. Zhou, S. Pan, J. Wang and A. V. Vasilakos,\\nNeurocomputing , 2017, 237, 350 –361.\\n131 C. R. Thomas, S. George, A. M. Horst, Z. Ji, R. J. Miller,\\nJ. R. Peralta-Videa, T. Xia, S. Pokhrel, L. M ¨adler and\\nJ. L. Gardea-Torresdey, ACS Nano , 2011, 5,1 3–20.\\nThis journal is © The Royal Society of Chemistry 2020 Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 | 3129Review Nanoscale Advances\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 14}),\n",
       " Document(page_content='132 J. Greeley, T. F. Jaramillo, J. Bonde, I. Chorkendor ﬀand\\nJ. K. Nørskov, Nat. Mater. , 2006, 5, 909 –913.\\n133 K. Sliozberg, D. Sch ¨afer, T. Erichsen, R. Meyer, C. Khare,\\nA. Ludwig and W. Schuhmann, ChemSusChem , 2015, 8,\\n1270 –1278.\\n134 R. Meyer, K. Sliozberg, C. Khare, W. Schuhmann and\\nA. Ludwig, ChemSusChem , 2015, 8, 1279 –1285.\\n135 A. Agrawal and A. Choudhary, APL Mater. , 2016, 4, 053208.\\n136 Z. Yue, S. Ji, Y. Sigang, C. Hongwei and X. Kun, Radio Eng. ,\\n2019, 49, 1031 –1036.137 P. Dey, J. Bible, S. Datta, S. Broderick, J. Jasinski,\\nM. Sunkara, M. Menon and K. Rajan, Comput. Mater. Sci. ,\\n2014, 83, 185 –195.\\n138 G. Pilania, A. Mannodi-Kanakkithodi, B. P. Uberuaga,\\nR. Ramprasad, J. E. Gubernatis and T. Lookman, Sci.\\nRep., 2016, 6, 19375.\\n139 Z. W. Ulissi, A. J. Medford, T. Bligaard and J. K. Norskov,\\nNat. Commun. , 2017, 8,1–7.\\n3130 |Nanoscale Adv. ,2 0 2 0 , 2, 3115 –3130 This journal is © The Royal Society of Chemistry 2020Nanoscale Advances Review\\nOpen Access Article. Published on 22 June 2020. Downloaded on 4/24/2024 8:35:09 AM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 15}),\n",
       " Document(page_content='EfﬁcientViT: Memory Efﬁcient Vision Transformer with\\nCascaded Group Attention\\nXinyu Liu1,∗, Houwen Peng2, Ningxin Zheng2, Yuqing Yang2, Han Hu2, Yixuan Yuan1\\n1The Chinese University of Hong Kong,2Microsoft Research\\nAbstract\\nVision transformers have shown great success due to\\ntheir high model capabilities. However, their remarkable\\nperformance is accompanied by heavy computation costs,\\nwhich makes them unsuitable for real-time applications. In\\nthis paper, we propose a family of high-speed vision trans-\\nformers named EfﬁcientViT. We ﬁnd that the speed of ex-\\nisting transformer models is commonly bounded by mem-\\nory inefﬁcient operations, especially the tensor reshaping\\nand element-wise functions in MHSA. Therefore, we design\\na new building block with a sandwich layout, i.e., using a\\nsingle memory-bound MHSA between efﬁcient FFN layers,\\nwhich improves memory efﬁciency while enhancing channel\\ncommunication. Moreover, we discover that the attention\\nmaps share high similarities across heads, leading to com-\\nputational redundancy. To address this, we present a cas-\\ncaded group attention module feeding attention heads with\\ndifferent splits of the full feature, which not only saves com-\\nputation cost but also improves attention diversity. Compre-\\nhensive experiments demonstrate EfﬁcientViT outperforms\\nexisting efﬁcient models, striking a good trade-off between\\nspeed and accuracy. For instance, our EfﬁcientViT-M5 sur-\\npasses MobileNetV3-Large by 1.9% in accuracy, while get-\\nting 40.4% and 45.2% higher throughput on Nvidia V100\\nGPU and Intel Xeon CPU, respectively. Compared to\\nthe recent efﬁcient model MobileViT-XXS, EfﬁcientViT-M2\\nachieves 1.8% superior accuracy, while running 5.8×/3.7×\\nfaster on the GPU/CPU, and 7.4× faster when converted to\\nONNX format. Code and models are available at here.\\n1. Introduction\\nVision Transformers (ViTs) have taken computer vision\\ndomain by storm due to their high model capabilities and\\nsuperior performance [18, 44, 69]. However, the constantly\\nimproved accuracy comes at the cost of increasing model\\nsizes and computation overhead. For example, SwinV2 [43]\\nuses 3.0B parameters, while V-MoE [62] taking 14.7B pa-\\nrameters, to achieve state-of-the-art performance on Ima-\\n∗Work done when Xinyu was an intern of Microsoft Research.\\nFigure 1. Speed and accuracy comparisons between EfﬁcientViT\\n(Ours) and other efﬁcient CNN and ViT models tested on an\\nNvidia V100 GPU with ImageNet-1K dataset [17].\\ngeNet [17]. Such large model sizes and the accompanying\\nheavy computational costs make these models unsuitable\\nfor applications with real-time requirements [40, 78, 86].\\nThere are several recent works designing light and efﬁ-\\ncient vision transformer models [9,19,29,49,50,56,79,81].\\nUnfortunately, most of these methods aim to reduce model\\nparameters or Flops, which are indirect metrics for speed\\nand do not reﬂect the actual inference throughput of models.\\nFor example, MobileViT-XS [50] using 700M Flops runs\\nmuch slower than DeiT-T [69] with 1,220M Flops on an\\nNvidia V100 GPU. Although these methods have achieved\\ngood performance with fewer Flops or parameters, many\\nof them do not show signiﬁcant wall-clock speedup against\\nstandard isomorphic or hierarchical transformers, e.g., DeiT\\n[69] and Swin [44], and have not gained wide adoption.\\nTo address this issue, in this paper, we explore how to\\ngo faster with vision transformers, seeking to ﬁnd princi-\\nples for designing efﬁcient transformer architectures. Based\\non the prevailing vision transformers DeiT [69] and Swin\\n[44], we systematically analyze three main factors that af-\\nfect model inference speed, including memory access, com-\\nputation redundancy, and parameter usage. In particular,\\nwe ﬁnd that the speed of transformer models is commonly\\nmemory-bound. In other words, memory accessing de-\\nlay prohibits the full utilization of the computing power\\nin GPU/CPUs [21, 32, 72], leading to a critically negative\\nimpact on the runtime speed of transformers [15, 31]. The', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 0}),\n",
       " Document(page_content='lay prohibits the full utilization of the computing power\\nin GPU/CPUs [21, 32, 72], leading to a critically negative\\nimpact on the runtime speed of transformers [15, 31]. The\\n1arXiv:2305.07027v1  [cs.CV]  11 May 2023', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 0}),\n",
       " Document(page_content='most memory-inefﬁcient operations are the frequent tensor\\nreshaping and element-wise functions in multi-head self-\\nattention (MHSA). We observe that through an appropri-\\nate adjustment of the ratio between MHSA and FFN (feed-\\nforward network) layers, the memory access time can be re-\\nduced signiﬁcantly without compromising the performance.\\nMoreover, we ﬁnd that some attention heads tend to learn\\nsimilar linear projections, resulting in redundancy in atten-\\ntion maps. The analysis shows that explicitly decomposing\\nthe computation of each head by feeding them with diverse\\nfeatures can mitigate this issue while improving computa-\\ntion efﬁciency. In addition, the parameter allocation in dif-\\nferent modules is often overlooked by existing lightweight\\nmodels, as they mainly follow the conﬁgurations in stan-\\ndard transformer models [44,69]. To improve parameter ef-\\nﬁciency, we use structured pruning [45] to identify the most\\nimportant network components, and summarize empirical\\nguidance of parameter reallocation for model acceleration.\\nBased upon the analysis and ﬁndings, we propose a new\\nfamily of memory efﬁcient transformer models named Efﬁ-\\ncientViT. Speciﬁcally, we design a new block with a sand-\\nwich layout to build up the model. The sandwich layout\\nblock applies a single memory-bound MHSA layer between\\nFFN layers. It reduces the time cost caused by memory-\\nbound operations in MHSA, and applies more FFN layers\\nto allow communication between different channels, which\\nis more memory efﬁcient. Then, we propose a new cascaded\\ngroup attention (CGA) module to improve computation ef-\\nﬁciency. The core idea is to enhance the diversity of the fea-\\ntures fed into the attention heads. In contrast to prior self-\\nattention using the same feature for all heads, CGA feeds\\neach head with different input splits and cascades the out-\\nput features across heads. This module not only reduces the\\ncomputation redundancy in multi-head attention, but also\\nelevates model capacity by increasing network depth. Last\\nbut not least, we redistribute parameters through expanding\\nthe channel width of critical network components such as\\nvalue projections, while shrinking the ones with lower im-\\nportance like hidden dimensions in FFNs. This reallocation\\nﬁnally promotes model parameter efﬁciency.\\nExperiments demonstrate that our models achieve clear\\nimprovements over existing efﬁcient CNN and ViT models\\nin terms of both speed and accuracy, as shown in Fig. 1.\\nFor instance, our EfﬁcientViT-M5 gets 77.1% top-1 accu-\\nracy on ImageNet with throughput of 10,621 images/s on an\\nNvidia V100 GPU and 56.8 images/s on an Intel Xeon E5-\\n2690 v4 CPU @ 2.60GHz, outperforming MobileNetV3-\\nLarge [26] by 1.9% in accuracy, 40.4% in GPU inference\\nspeed, and 45.2% in CPU speed. Moreover, EfﬁcientViT-\\nM2 gets 70.8% accuracy, surpassing MobileViT-XXS [50]\\nby 1.8%, while running 5.8 ×/3.7×faster on the GPU/CPU,\\nand 7.4×faster when converted to ONNX [3] format. When\\ndeployed on the mobile chipset, i.e., Apple A13 Bionic chip\\nFigure 2. Runtime proﬁling on two standard vision transformers\\nSwin-T and DeiT-T. Red text denotes memory-bound operations,\\ni.e., the time taken by the operation is mainly determined by mem-\\nory accesses, while time spent in computation is much smaller.\\nin iPhone 11, EfﬁcientViT-M2 model runs 2.3 ×faster than\\nMobileViT-XXS [50] using the CoreML [1].\\nIn summary, the contributions of this work are two-fold:\\n• We present a systematic analysis on the factors that\\naffect the inference speed of vision transformers, de-\\nriving a set of guidelines for efﬁcient model design.\\n• We design a new family of vision transformer models,\\nwhich strike a good trade-off between efﬁciency and\\naccuracy. The models also demonstrate good transfer\\nability on a variety of downstream tasks.\\n2. Going Faster with Vision Transformers\\nIn this section, we explore how to improve the efﬁciency\\nof vision transformers from three perspectives: memory ac-', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 1}),\n",
       " Document(page_content='ability on a variety of downstream tasks.\\n2. Going Faster with Vision Transformers\\nIn this section, we explore how to improve the efﬁciency\\nof vision transformers from three perspectives: memory ac-\\ncess, computation redundancy, and parameter usage. We\\nseek to identify the underlying speed bottlenecks through\\nempirical studies, and summarize useful design guidelines.\\n2.1. Memory Efﬁciency\\nMemory access overhead is a critical factor affecting\\nmodel speed [15,28,31,65]. Many operators in transformer\\n[71], such as frequent reshaping, element-wise addition,\\nand normalization are memory inefﬁcient, requiring time-\\nconsuming access across different memory units, as shown\\nin Fig. 2. Although there are some methods proposed to ad-\\ndress this issue by simplifying the computation of standard\\nsoftmax self-attention, e.g., sparse attention [34, 57, 61, 75]\\nand low-rank approximation [11,51,74], they often come at\\nthe cost of accuracy degradation and limited acceleration.\\nIn this work, we turn to save memory access cost by\\nreducing memory-inefﬁcient layers. Recent studies reveal\\nthat memory-inefﬁcient operations are mainly located in\\nMHSA rather than FFN layers [31, 33]. However, most ex-\\nisting ViTs [18, 44, 69] use an equivalent number of these\\ntwo layers, which may not achieve the optimal efﬁciency.\\nWe thereby explore the optimal allocation of MHSA and\\nFFN layers in small models with fast inference. Speciﬁ-\\ncally, we scale down Swin-T [44] and DeiT-T [69] to several\\nsmall subnetworks with 1.25 ×and 1.5×higher inference\\nthroughput, and compare the performance of subnetworks\\nwith different proportions of MHSA layers. As shown in\\nFig. 3, subnetworks with 20%-40% MHSA layers tend to\\nget better accuracy. Such ratios are much smaller than the\\n2', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 3. The accuracy of downscaled baseline models with dif-\\nferent MHSA layer proportions, where the dots on each line rep-\\nresent subnetworks with similar throughput. Left: Swin-T as the\\nbaseline. Right: DeiT-T as the baseline. The 1.25 ×/1.5×denote\\naccelerating the baseline models by 1.25/1.5 times, respectively.\\ntypical ViTs that adopt 50% MHSA layers. Furthermore,\\nwe measure the time consumption on memory-bound op-\\nerations to compare memory access efﬁciency, including\\nreshaping, element-wise addition, copying, and normaliza-\\ntion. Memory-bound operations is reduced to 44.26% of\\nthe total runtime in Swin-T-1.25 ×that has 20% MHSA lay-\\ners. The observation also generalizes to DeiT and smaller\\nmodels with 1.5×speed-up. It is demonstrated that reduc-\\ning MHSA layer utilization ratio appropriately can enhance\\nmemory efﬁciency while improving model performance.\\n2.2. Computation Efﬁciency\\nMHSA embeds the input sequence into multiple sub-\\nspaces (heads) and computes attention maps separately,\\nwhich has been proven effective in improving performance\\n[18, 69, 71]. However, attention maps are computationally\\nexpensive, and studies have shown that a number of them\\nare not of vital importance [52, 73]. To save computation\\ncost, we explore how to reduce redundant attention in small\\nViT models. We train width downscaled Swin-T [44] and\\nDeiT-T [69] models with 1.25 ×inference speed-up, and\\nmeasure the maximum cosine similarity of each head and\\nthe remaining heads within each block. From Fig. 4, we ob-\\nserve there exists high similarities between attention heads,\\nespecially in the last blocks. The phenomenon suggests that\\nmany heads learn similar projections of the same full fea-\\nture and incur computation redundancy. To explicitly en-\\ncourage the heads to learn different patterns, we apply an\\nintuitive solution by feeding each head with only a split of\\nthe full feature, which is similar to the idea of group con-\\nvolution in [10, 87]. We train the variants of downscaled\\nmodels with the modiﬁed MHSA, and also compute the at-\\ntention similarities in Fig. 4. It is shown that using different\\nchannel-wise splits of the feature in different heads, instead\\nof using the same full feature for all heads as MHSA, could\\neffectively mitigate attention computation redundancy.\\n2.3. Parameter Efﬁciency\\nTypical ViTs mainly inherit the design strategies from\\nNLP transformer [71], e.g., using an equivalent width for\\nQ,K,Vprojections, increasing heads over stages, and set-\\nting the expansion ratio to 4 in FFN. For lightweight mod-\\nFigure 4. The average maximum cosine similarity of each head in\\ndifferent blocks. Left: downscaled Swin-T models. Right: down-\\nscaled DeiT-T models. Blue lines denote Swin-T-1.25 ×/DeiT-T-\\n1.25×model, while darkblue lines denote the variants that feed\\neach head with only a split of the full feature.\\nFigure 5. The ratio of the channels to the input embeddings before\\nand after pruning Swin-T. Baseline accuracy: 79.1%; pruned ac-\\ncuracy: 76.5%. Results for DeiT-T are given in the supplementary.\\nels, the conﬁgurations of these components need to be care-\\nfully re-designed [7, 8, 39]. Inspired by [45, 82], we adopt\\nTaylor structured pruning [53] to automatically ﬁnd the im-\\nportant components in Swin-T and DeiT-T, and explore the\\nunderlying principles of parameter allocation. The pruning\\nmethod removes unimportant channels under a certain re-\\nsource constraint and keeps the most critical ones to best\\npreserve the accuracy. It uses the multiplication of gradient\\nand weight as channel importance, which approximates the\\nloss ﬂuctuation when removing channels [38].\\nThe ratio between the remaining output channels to the\\ninput channels is plotted in Fig. 5, and the original ratios\\nin the unpruned model are also given for reference. It is\\nobserved that: 1) The ﬁrst two stages preserve more dimen-\\nsions, while the last stage keeps much less; 2) The Q,Kand\\nFFN dimensions are largely trimmed, whereas the dimen-', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 2}),\n",
       " Document(page_content='observed that: 1) The ﬁrst two stages preserve more dimen-\\nsions, while the last stage keeps much less; 2) The Q,Kand\\nFFN dimensions are largely trimmed, whereas the dimen-\\nsion of Vis almost preserved and diminishes only at the\\nlast few blocks. These phenomena show that 1 ) the typical\\nchannel conﬁguration, that doubles the channel after each\\nstage [44] or use equivalent channels for all blocks [69],\\nmay produce substantial redundancy in last few blocks; 2)\\nThe redundancy in Q,Kis much larger than Vwhen they\\nhave the same dimensions. Vprefers a relative large chan-\\nnels, being close to the input embedding dimension.\\n3. Efﬁcient Vision Transformer\\nBased upon the above analysis, in this section, we pro-\\npose a new hierarchical model with fast inference named\\nEfﬁcientViT. The architecture overview is shown in Fig. 6.\\n3', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 2}),\n",
       " Document(page_content='Stage1\\n×L1EfficientViT\\nBlockStage2OverlapPatchEmbedH×W×3H\\n16×W\\n16×C1\\nEfficientViT SubsampleH\\n32×W\\n32×C2\\nStage3H\\n64×W\\n64×C3\\nAvgPool+ClassifierInput\\n×\\nToken\\nInteractionSplit\\nQKV QKV QKV+ +\\nConcat&ProjectionHead1\\nHead2\\nHeadhInputOutput\\nToken\\nInteractionToken\\nInteraction(a)\\n(b)(c)\\nSelf -attention Self -attention Self -attention\\n……\\n+ +\\nTokenInteractionFeedForward Network\\nEfficientViT Subsample\\n×L2EfficientViT\\nBlock\\n×L3EfficientViT\\nBlock\\nCascaded\\nGroupAttention\\n+\\n×\\n+ +\\nTokenInteractionFeedForward NetworkFigure 6. Overview of EfﬁcientViT. (a) Architecture of EfﬁcientViT; (b) Sandwich Layout block; (c) Cascaded Group Attention.\\n3.1. EfﬁcientViT Building Blocks\\nWe propose a new efﬁcient building block for vision\\ntransformer, as shown in Fig. 6 (b). It is composed of a\\nmemory-efﬁcient sandwich layout, a cascaded group atten-\\ntion module, and a parameter reallocation strategy, which\\nfocus on improving model efﬁciency in terms of memory,\\ncomputation, and parameter, respectively.\\nSandwich Layout. To build up a memory-efﬁcient block,\\nwe propose a sandwich layout that employs less memory-\\nbound self-attention layers and more memory-efﬁcient FFN\\nlayers for channel communication. Speciﬁcally, it applies a\\nsingle self-attention layer ΦA\\nifor spatial mixing, which is\\nsandwiched between FFN layers ΦF\\ni. The computation can\\nbe formulated as:\\nXi+1=N∏\\nΦF\\ni(ΦA\\ni(N∏\\nΦF\\ni(Xi))),(1)\\nwhere Xiis the full input feature for the i-th block. The\\nblock transforms XiintoXi+1withNFFNs before and af-\\nter the single self-attention layer. This design reduces the\\nmemory time consumption caused by self-attention layers\\nin the model, and applies more FFN layers to allow com-\\nmunication between different feature channels efﬁciently.\\nWe also apply an extra token interaction layer before each\\nFFN using a depthwise convolution (DWConv) [27]. It in-\\ntroduces inductive bias of the local structural information to\\nenhance model capability [14].\\nCascaded Group Attention. Attention head redundancy\\nis a severe issue in MHSA, which causes computation inef-\\nﬁciency. Inspired by group convolutions in efﬁcient CNNs[10, 37, 64, 87], we propose a new attention module named\\ncascaded group attention (CGA) for vision transformers. It\\nfeeds each head with different splits of the full features, thus\\nexplicitly decomposing the attention computation across\\nheads. Formally, this attention can be formulated as:\\n˜Xij=Attn(XijWQ\\nij, XijWK\\nij, XijWV\\nij),\\n˜Xi+1=Concat [˜Xij]j=1:hWP\\ni,(2)\\nwhere the j-th head computes the self-attention over Xij,\\nwhich is the j-th split of the input feature Xi,i.e.,Xi=\\n[Xi1, Xi2, . . . , X ih]and1≤j≤h.his the total number\\nof heads, WQ\\nij,WK\\nij, andWV\\nijare projection layers mapping\\nthe input feature split into different subspaces, and WP\\niis\\na linear layer that projects the concatenated output features\\nback to the dimension consistent with the input.\\nAlthough using feature splits instead of the full features\\nfor each head is more efﬁcient and saves computation over-\\nhead, we continue to improve its capacity, by encouraging\\ntheQ,K,Vlayers to learn projections on features with\\nricher information. We compute the attention map of each\\nhead in a cascaded manner, as illustrated in Fig. 6 (c), which\\nadds the output of each head to the subsequent head to reﬁne\\nthe feature representations progressively:\\nX′\\nij=Xij+˜Xi(j−1),1< j≤h, (3)\\nwhere X′\\nijis the addition of the j-th input split Xijand the\\n(j−1)-th head output ˜Xi(j−1)calculated by Eq. (2). It re-\\nplaces Xijto serve as the new input feature for the j-th head\\nwhen calculating the self-attention. Besides, another token\\n4', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 3}),\n",
       " Document(page_content='Table 1. Architecture details of EfﬁcientViT model variants.\\nModel {C1,C2,C3} {L1,L2,L3}{H1,H2,H3}\\nEfﬁcientViT-M0 {64, 128, 192} { 1, 2, 3} { 4, 4, 4}\\nEfﬁcientViT-M1{128, 144, 192} { 1, 2, 3} { 2, 3, 3}\\nEfﬁcientViT-M2{128, 192, 224} { 1, 2, 3} { 4, 3, 2}\\nEfﬁcientViT-M3{128, 240, 320} { 1, 2, 3} { 4, 3, 4}\\nEfﬁcientViT-M4{128, 256, 384} { 1, 2, 3} { 4, 4, 4}\\nEfﬁcientViT-M5{192, 288, 384} { 1, 3, 4} { 3, 3, 4}\\ninteraction layer is applied after the Qprojection, which\\nenables the self-attention to jointly capture local and global\\nrelations and further enhances the feature representation.\\nSuch a cascaded design enjoys two advantages. First,\\nfeeding each head with different feature splits could im-\\nprove the diversity of attention maps, as validated in Sec.\\n2.2. Similar to group convolutions [10, 87], the cascaded\\ngroup attention could save the Flops and parameters by h×,\\nsince the input and output channels in the QKV layers are\\nreduced by h×. Second, cascading the attention heads al-\\nlows for an increase of network depth, thus further elevat-\\ning the model capacity without introducing any extra pa-\\nrameters. It only incurs minor latency overhead since the\\nattention map computation in each head uses smaller QK\\nchannel dimensions.\\nParameter Reallocation. To improve parameter efﬁ-\\nciency, we reallocate the parameters in the network by ex-\\npanding the channel width of critical modules while shrink-\\ning the unimportant ones. Speciﬁcally, based on the Tay-\\nlor importance analysis in Sec. 2.3, we set small channel\\ndimensions for QandKprojections in each head for all\\nstages. For the Vprojection, we allow it to have the same\\ndimension as the input embedding. The expansion ratio in\\nFFN is also reduced from 4 to 2 due to its parameter redun-\\ndancy. With the proposed reallocation strategy, the impor-\\ntant modules have larger number of channels to learn rep-\\nresentations in a high dimensional space, which prevent the\\nloss of feature information. Meanwhile, the redundant pa-\\nrameters in unimportant modules are removed to speed up\\ninference and enhance the model efﬁciency.\\n3.2. EfﬁcientViT Network Architectures\\nThe overall architecture of our EfﬁcientViT is presented\\nin Fig. 6 (a). Concretely, we introduce overlapping patch\\nembedding [20, 80] to embed 16 ×16 patches into tokens\\nwithC1dimension, which enhances the model capacity in\\nlow-level visual representation learning. The architecture\\ncontains three stages. Each stage stacks the proposed Ef-\\nﬁcientViT building blocks and the number of tokens is re-\\nduced by 4×at each subsampling layer (2 ×subsampling\\nof the resolution). To achieve efﬁcient subsampling, we\\npropose an EfﬁcientViT subsample block which also has\\nthe sandwich layout, except that the self-attention layer is\\nreplaced by an inverted residual block to reduce the infor-\\nmation loss during subsampling [26, 63]. It is worth not-\\ning that we adopt BatchNorm (BN) [30] instead of Layer-Norm (LN) [2] throughout the model, as BN can be folded\\ninto the preceding convolution or linear layers, which is a\\nruntime advantage over LN. We also use ReLU [54] as the\\nactivation function, as the commonly used GELU [25] or\\nHardSwish [26] are much slower, and sometimes not well-\\nsupported by certain inference deployment platforms [1, 3].\\nWe build our model family with six different width and\\ndepth scales, and set different number of heads for each\\nstage. We use fewer blocks in early stages than late stages\\nsimilar to MobileNetV3 [26] and LeViT [20], since that the\\nprocessing on early stages with larger resolutions is more\\ntime consuming. We increase the width over stages with a\\nsmall factor (≤2) to alleviate redundancy in later stages, as\\nanalyzed in Sec. 2.3. The architecture details of our model\\nfamily are presented in Tab. 1. Ci,Li, and Hirefer to the\\nwidth, depth, and number of heads in the i-th stage.\\n4. Experiments\\n4.1. Implementation Details\\nWe conduct image classiﬁcation experiments on\\nImageNet-1K [17]. The models are built with PyTorch', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 4}),\n",
       " Document(page_content='width, depth, and number of heads in the i-th stage.\\n4. Experiments\\n4.1. Implementation Details\\nWe conduct image classiﬁcation experiments on\\nImageNet-1K [17]. The models are built with PyTorch\\n1.11.0 [59] and Timm 0.5.4 [77], and trained from scratch\\nfor 300 epochs on 8 Nvidia V100 GPUs using AdamW\\n[46] optimizer and cosine learning rate scheduler. We set\\nthe total batchsize as 2,048. The input images are resized\\nand randomly cropped into 224 ×224. The initial learning\\nrate is 1×10−3with weight decay of 2.5 ×10−2. We use\\nthe same data augmentation as [69], including Mixup [85],\\nauto-augmentation [13], and random erasing [88]. In ad-\\ndition, we provide throughput evaluation on different hard-\\nware. For GPU, we measure the throughput on an Nvidia\\nV100, with the maximum power-of-two batchsize that ﬁts\\nin memory following [20, 69]. For CPU and ONNX, we\\nmeasure the runtime on an Intel Xeon E5-2690 v4 @ 2.60\\nGHz processor, with batchsize 16 and run the model in a\\nsingle thread following [20]. We also test the transferabil-\\nity of EfﬁcientViT on downstream tasks. For the experi-\\nments on downstream image classiﬁcation, we ﬁnetune the\\nmodels for 300 epochs following [86], using AdamW [46]\\nwith batchsize 256, learning rate 1 ×10−3and weight-decay\\n1×10−8. We use RetinaNet [41] for object detection on\\nCOCO [42], and train the models for 12 epochs (1 ×sched-\\nule) with the same settings as [44] on mmdetection [6]. For\\ninstance segmentation, please refer to the supplementary.\\n4.2. Results on ImageNet\\nWe compare EfﬁcientViT with prevailing efﬁcient CNN\\nand ViT models on ImageNet [17], and report the results in\\nTab. 2 and Fig. 1. The results show that, in most cases, our\\nEfﬁcientViT achieves the best accuracy and speed trade-off\\nacross different evaluation settings.\\nComparisons with efﬁcient CNNs. We ﬁrst compare Ef-\\nﬁcientViT with vanilla CNN models, such as MobileNets\\n5', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 4}),\n",
       " Document(page_content='Table 2. EfﬁcientViT image classiﬁcation performance on ImageNet-1K [17] with comparisons to state-of-the-art efﬁcient CNN and ViT\\nmodels trained without extra data. Throughput is tested on Nvidia V100 for GPU and Intel Xeon E5-2690 v4 @ 2.60 GHz processor for\\nCPU and ONNX, where larger throughput means faster inference speed. ↑: ﬁnetune with higher resolution.\\nModelTop-1 Top-5 Throughput (images/s) Flops Params\\nInput Epochs\\n(%) (%) GPU CPU ONNX (M) (M)\\nEfﬁcientViT-M0 63.2 85.4 27644 228.4 340.1 79 2.3 224 300\\nMobileNetV3-Small [26] 67.4 - 19738 156.5 231.7 57 2.5 224 600\\nEfﬁcientViT-M1 68.4 88.7 20093 126.9 215.9 167 3.0 224 300\\nMobile-Former-52M [9] 68.7 - 3141 32.8 21.5 52 3.5 224 450\\nMobileViT-XXS [50] 69.0 - 4456 29.4 41.7 410 1.3 256 300\\nShufﬂeNetV2 1.0×[48] 69.4 88.9 13301 106.7 177.0 146 2.3 224 300\\nMobileViTV2-0.5 [51] 70.2 - 5142 34.4 44.9 466 1.4 256 300\\nEfﬁcientViT-M2 70.8 90.2 18218 121.2 158.7 201 4.2 224 300\\nMobileOne-S0 [70] 71.4 - 11320 67.4 128.6 274 2.1 224 300\\nMobileNetV2 1.0×[63] 72.0 91.0 6534 32.5 80.4 300 3.4 224 300\\nEfﬁcientViT-M3 73.4 91.4 16644 96.4 120.8 263 6.9 224 300\\nGhostNet 1.0×[23] 73.9 91.4 7382 57.3 77.0 141 5.2 224 300\\nNASNet-A-Mobile [89] 74.1 - 2623 19.8 25.5 564 5.3 224 300\\nEfﬁcientViT-M4 74.3 91.8 15914 88.5 108.6 299 8.8 224 300\\nEdgeViT-XXS [56] 74.4 - 3638 28.2 29.6 556 4.1 224 300\\nMobileViT-XS [50] 74.7 - 3344 11.1 20.5 986 2.3 256 300\\nShufﬂeNetV2 2.0×[48] 74.9 92.4 6962 37.9 52.3 591 7.4 224 300\\nMobileNetV3-Large [26] 75.2 - 7560 39.1 70.5 217 5.4 224 600\\nMobileViTV2-0.75 [51] 75.6 - 3350 16.0 22.7 1030 2.9 256 300\\nMobileOne-S1 [70] 75.9 - 6663 30.7 51.1 825 4.8 224 300\\nGLiT-Tiny [5] 76.4 - 3516 17.5 15.7 1333 7.3 224 300\\nEfﬁcientNet-B0 [67] 77.1 93.3 4532 30.2 29.5 390 5.3 224 350\\nEfﬁcientViT-M5 77.1 93.4 10621 56.8 62.5 522 12.4 224 300\\nEfﬁcientViT-M4↑384 79.8 95.0 3986 15.8 22.6 1486 12.4 384 330\\nEfﬁcientViT-M5↑512 80.8 95.5 2313 8.3 10.5 2670 12.4 512 360\\n[26, 63] and EfﬁcientNet [67]. Speciﬁcally, compared to\\nMobileNetV2 1.0×[63], EfﬁcientViT-M3 obtains 1.4%\\nbetter top-1 accuracy, while running at 2.5 ×and 3.0×\\nfaster speed on V100 GPU and Intel CPU, respectively.\\nCompared to the state-of-the-art MobileNetV3-Large [26],\\nEfﬁcientViT-M5 achieves 1.9% higher accuracy yet runs\\nmuch faster, e.g., 40.5% faster on the V100 GPU and 45.2%\\nfaster on the Intel CPU but is 11.5% slower as ONNX mod-\\nels. This may because reshaping is slower in ONNX imple-\\nmentation, which is inevitable in computing self-attention.\\nMoreover, EfﬁcientViT-M5 achieves comparable accuracy\\nwith the searched model EfﬁcientNet-B0 [67], while runs\\n2.3×/1.9×faster on the V100 GPU/Intel CPU, and 2.1 ×\\nfaster as ONNX models. Although our model uses more pa-\\nrameters, it reduces memory-inefﬁcient operations that af-\\nfect the inference speed and achieves higher throughput.\\nComparisons with efﬁcient ViTs. We also compare our\\nmodels with recent efﬁcient vision transformers [5, 9, 50,\\n51, 56] in Tab. 2. In particular, when getting similar per-\\nformance on ImageNet-1K [17], our EfﬁcientViT-M4 runs\\n4.4×and 3.0×faster than the recent EdgeViT-XXS [56] on\\nthe tested CPU and GPU devices. Even converted to ONNXruntime format, our model still gets 3.7 ×higher speed.\\nCompared to the state-of-the-art MobileViTV2-0.5 [51],\\nour EfﬁcientViT-M2 achieves slightly better performance\\nwith higher throughput, e.g., 3.4×and 3.5×higher through-\\nput tested on the GPU and CPU devices, respectively. Fur-\\nthermore, we compare with tiny variants of state-of-the-art\\nlarge ViTs in Tab. 3. PoolFormer-12S [83] has comparable\\naccuracy with EfﬁcientViT-M5 yet runs 3.0 ×slower on the\\nV100 GPU. Compared to Swin-T [44], EfﬁcientViT-M5 is\\n4.1% inferior in accuracy yet is 12.3 ×faster on the Intel\\nCPU, demonstrating the efﬁciency of the proposed design.\\nIn addition, we present the speed evaluation and comparison\\non mobile chipsets in the supplementary material.\\nFinetune with higher resolutions. Recent works on ViTs', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 5}),\n",
       " Document(page_content='In addition, we present the speed evaluation and comparison\\non mobile chipsets in the supplementary material.\\nFinetune with higher resolutions. Recent works on ViTs\\nhave demonstrated that ﬁnetuning with higher resolutions\\ncan further improve the capacity of the models. We also\\nﬁnetune our largest model EfﬁcientViT-M5 to higher res-\\nolutions. EfﬁcientViT-M5 ↑384 reaches 79.8% top-1 accu-\\nracy with throughput of 3,986 images/s on the V100 GPU,\\nand EfﬁcientViT-M5 ↑512 further improves the top-1 accu-\\nracy to 80.8%, demonstrating the efﬁciency on processing\\nimages with larger resolutions and the good model capacity.\\n6', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 5}),\n",
       " Document(page_content='Table 3. Comparison with the tiny variants of state-of-the-art\\nlarge-scale ViTs on ImageNet-1K [17].\\nModelTop-1 Throughput (imgs/s) Flops Params\\n(%) GPU CPU ONNX (G) (M)\\nPVTV2-B0 [76] 70.5 3507 12.7 18.5 0.6 1.4\\nT2T-ViT-7 [84] 71.7 1156 22.5 16.1 1.1 4.3\\nDeiT-T [69] 72.2 4631 26.0 25.1 1.3 5.9\\nPoolFormer-12S [83] 77.2 3534 10.4 14.6 1.9 12.0\\nEffFormer-L1 [40] 79.2 4465 12.9 21.2 1.3 12.3\\nSwin-T [44] 81.2 1393 4.6 6.4 4.5 29.0\\nEfﬁcientViT-M5 77.1 10621 56.8 62.5 0.5 12.4\\nTable 4. Results of EfﬁcientViT and other efﬁcient models on\\ndownstream image classiﬁcation datasets.\\nModel\\nThroughput\\nImageNet\\nCIFAR10\\nCIFAR100\\nCars\\nFlowers\\nPets\\nMobileNetV1 [27] 8543 70.6 96.1 82.3 91.4 96.7 89.9\\nMobileNetV2 [63] 6534 72.9 95.7 80.8 91.0 96.6 90.5\\nMobileNetV3 [26] 7560 75.2 97.6 85.5 91.2 97.0 90.1\\nNASNet-A-M [89] 2623 74.1 96.8 83.9 88.5 96.8 89.4\\nViT-S/16 [18] 2135 81.4 97.6 85.7 - 86.4 90.4\\nEfﬁcientViT-M5 10621 77.1 98.0 86.4 89.7 97.1 92.0\\n4.3. Transfer Learning Results\\nTo further evaluate the transfer ability, we apply Efﬁ-\\ncientViT on various downstream tasks.\\nDownstream Image Classiﬁcation. We transfer Efﬁ-\\ncientViT to downstream image classiﬁcation datasets to\\ntest its generalization ability: 1) CIFAR-10 and CIFAR-\\n100 [36]; 2) ﬁne-grained classiﬁcation: Flowers [55], Stan-\\nford Cars [35], and Oxford-IIIT Pets [58]. We report the\\nresults in Tab. 4. Compared to existing efﬁcient mod-\\nels [18, 26, 27, 63, 89], our EfﬁcientViT-M5 achieves com-\\nparable or slightly better accuracy across all datasets with\\nmuch higher throughput. An exception lies in Cars, where\\nour model is slightly inferior in accuracy. This may because\\nthe subtle differences between classes lie more in local de-\\ntails thus is more feasible to be captured with convolution.\\nObject Detection. We compare EfﬁcientViT-M4 with ef-\\nﬁcient models [12, 22, 26, 63, 66, 68] on the COCO [42] ob-\\nject detection task, and present the results in Tab. 5. Specif-\\nically, EfﬁcientViT-M4 surpasses MobileNetV2 [63] by\\n4.4% AP with comparable Flops. Compared to the searched\\nmethod SPOS [22], our EfﬁcientViT-M4 uses 18.1% fewer\\nFlops while achieving 2.0% higher AP, demonstrating its\\ncapacity and generalization ability in different vision tasks.\\n4.4. Ablation Study\\nIn this section, we ablate important design elements in\\nthe proposed EfﬁcientViT on ImageNet-1K [17]. All mod-\\nels are trained for 100 epochs to magnify the differences\\nand reduce training time [20]. Tab. 6 reports the results.\\nImpact of the sandwich layout block. We ﬁrst present an\\nablation study to verify the efﬁciency of the proposed sand-\\nwich layout design, by replacing the sandwich layout block\\nwith the original Swin block [44]. The depth is adjusted toTable 5. EfﬁcientViT object detection performance on COCO\\nval2017 [42] with comparisons to other efﬁcient models.\\nModelRetinaNet 1× Flops Params\\nAP AP 50AP75APsAPmAPl(M) (M)\\nMobileNetV2 [63] 28.3 46.7 29.3 14.8 30.7 38.1 300 3.4\\nMobileNetV3 [26] 29.9 49.3 30.8 14.9 33.3 41.1 217 5.4\\nSPOS [22] 30.7 49.8 32.2 15.4 33.9 41.6 365 4.3\\nMNASNet-A2 [66] 30.5 50.2 32.0 16.6 34.1 41.1 340 4.8\\nFairNAS-C [12] 31.2 50.8 32.7 16.3 34.4 42.3 325 5.6\\nMixNet-M [68] 31.3 51.7 32.4 17.0 35.0 41.9 360 5.0\\nEfﬁcientViT-M4 32.7 52.2 34.1 17.6 35.3 46.0 299 8.8\\nTable 6. Ablation for EfﬁcientViT-M4 on ImageNet-1K [17]\\ndataset. Top-1 accuracy, GPU and ONNX throughput are reported.\\n# Ablation Top-1 (%)Throughput (imgs/s)\\nGPU ONNX\\n1 EfﬁcientViT-M4 71.3 15914 108.6\\n2 Sandwich →Swin [44] 68.3 15804 114.5\\n3N= 1→2 70.2 14977 112.3\\n4N= 1→3 65.7 15856 139.7\\n5 CGA →MHSA [18] 70.2 16243 102.2\\n6 Cascade →None 69.8 16411 111.0\\n7 QKV allocation →None 69.9 15132 103.1\\n8 FFN ratio 2 →4 69.8 15310 112.4\\n9 DWConv →None 69.9 16325 110.4\\n10 BN →LN [2] 70.4 15463 103.6\\n11 ReLU →HSwish [26] 72.2 15887 87.5\\n{2, 2, 3}to guarantee similar throughput with EfﬁcientViT-\\nM4 for a fair comparison. The top-1 accuracy degrades\\nby 3.0% at a similar speed, verifying that applying more\\nFFNs instead of memory-bound MHSA is more effective', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 6}),\n",
       " Document(page_content='M4 for a fair comparison. The top-1 accuracy degrades\\nby 3.0% at a similar speed, verifying that applying more\\nFFNs instead of memory-bound MHSA is more effective\\nfor small models. Furthermore, to analyze the impact of\\nthe number of FFNs Nbefore and after self-attention , we\\nchange the number from 1 to 2 and 3. The number of blocks\\nis reduced accordingly to maintain similar throughput. As\\npresented in Tab. 6 (#3 and #4), further increasing the num-\\nber of FFNs is not effective due to the lack of long-range\\nspatial relation and N=1 achieves the best efﬁciency.\\nImpact of the cascaded group attention. We have\\nproposed CGA to improve the computation efﬁciency of\\nMHSA. As shown in Tab. 6 (#5 and #6), replacing CGA\\nwith MHSA decreases the accuracy by 1.1% and ONNX\\nspeed by 5.9%, suggesting that addressing head redun-\\ndancy improves the model efﬁciency. For the model without\\nthe cascade operation, its performance is comparable with\\nMHSA but worse than CGA, demonstrating the efﬁcacy of\\nenhancing the feature representations of each head.\\nImpact of the parameter reallocation. Our EfﬁcientViT-\\nM4 yields 1.4%/1.5% higher top-1 accuracy, 4.9%/3.8%\\nhigher GPU throughput than the models without QKV\\nchannel dimension reallocation or FFN ratio reduction, re-\\nspectively, indicating the effectiveness of parameter reallo-\\ncation (#1 vs.#7, #8). Moreover, we study the choices of\\nQK dimension in each head and the ratio of Vdimension to\\nthe input embedding in Fig. 7. It is shown that the perfor-\\nmance is improved gradually as QK dimension increases\\n7', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 6}),\n",
       " Document(page_content='Table 7. Performance comparison on ImageNet-1K [17] and\\nImageNet-ReaL [4]. Results with †are trained with 1,000 epochs\\nand knowledge distillation following LeViT [20].\\nModelImageNet (%) Throughput (imgs/s) Flops Params\\nTop-1 Top-1†ReaL†GPU CPU ONNX (M) (M)\\nLeViT-128S [20] 73.6 76.6 82.6 14457 82.3 80.9 305 7.8\\nEfﬁcientViT-M4 74.3 77.1 83.6 15914 88.5 108.6 299 8.8\\nfrom 4 to 16, while further increasing it gives inferior per-\\nformance. Besides, the performance improves from 70.3%\\nto 71.3% when increasing the ratio between Vdimension\\nand input embedding from 0.4 to 1.0. When further en-\\nlarging the ratio to 1.2, it only gets 0.1% improvements.\\nTherefore, setting the channels of Vclose to the input em-\\nbedding achieves the best parameter efﬁciency, which meets\\nour analysis in Sec. 2.3 and design strategy.\\nImpact of other components. We ablate the impact of us-\\ning DWConv for token interaction, the normalization layer,\\nand the activation function, as presented in Tab. 6 (#9, #10,\\nand #11). With DWConv, the accuracy improves by 1.4%\\nwith a minor latency overhead, demonstrating the effective-\\nness of introducing local structural information. Replacing\\nBN with LN decreases accuracy by 0.9% and GPU speed\\nby 2.9%. Using HardSwish instead of ReLU improves ac-\\ncuracy by 0.9% but leads to a large drop of 20.0% ONNX\\nspeed. The activation functions are element-wise operations\\nthat occupy a considerable amount of processing time on\\nGPU/CPU [15,48,72], thus utilizing ReLU instead of more\\ncomplicated activation functions is of better efﬁciency.\\nResults of 1,000 training epochs and distillation. Tab. 7\\nshows the results with 1,000 training epochs and knowledge\\ndistillation using RegNetY-16GF [60] as the teacher model\\nfollowing [20] on ImageNet-1K [17] and ImageNet-ReaL\\n[4]. Compared to LeViT-128S [20], EfﬁcientViT-M4 sur-\\npasses it by 0.5% on ImageNet-1K and 1.0% on ImageNet-\\nReaL, respectively. For the inference speed, our model has\\n34.2% higher throughput on ONNX and also shows supe-\\nriority on other settings. The results demonstrate that the\\nstrong capability and generalization ability of EfﬁcientViT\\ncan be further explored with longer training schedules.\\n5. Related Work\\nEfﬁcient CNNs. With the demand of deploying CNNs on\\nresource-constrained scenarios, efﬁcient CNNs have been\\nintensively studied in literature [23, 24, 26, 48, 63, 67, 87].\\nXception [10] proposes an architecture built with depth-\\nwise separable convolutions. MobileNetV2 [63] builds an\\ninverted residual structure which expands the input to a\\nhigher dimension. MobileNetV3 [26] and EfﬁcientNet [67]\\nresort to neural architecture search techniques to design\\ncompact models. To boost the actual speed on hardware,\\nShufﬂeNetV2 [48] introduces channel split and shufﬂe op-\\nerations to improve the information communication among\\nchannel groups. However, the spatial locality of convolu-\\ntional kernels hampers CNN models from capturing long-\\nFigure 7. Ablation on the QK dimension of each head and the\\nratio of Vdimension to the input embedding.\\nrange dependencies, thus limiting their model capacity.\\nEfﬁcient ViTs. ViT and its variants [18, 44, 69, 76] have\\nachieved success on various vision tasks. Despite the supe-\\nrior performance, most of them are inferior to typical CNNs\\nin inference speed. Some efﬁcient transformers have been\\nproposed recently and they fall into two camps: 1) efﬁcient\\nself-attention; and 2) efﬁcient architecture design. Efﬁcient\\nself-attention methods reduce the cost of softmax attention\\nvia sparse attention [34, 57, 61, 75] or low-rank approxima-\\ntion [11, 51, 74]. However, they suffer from performance\\ndegradation with negligible or moderate inference accelera-\\ntion over softmax attention [71]. Another line of work com-\\nbines ViTs with lightweight CNNs to build efﬁcient archi-\\ntectures [9, 47, 49, 50, 81]. LVT [81] proposes enhanced at-\\ntention mechanisms with dilated convolution to improve the\\nmodel performance and efﬁciency. Mobile-Former [9] de-', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 7}),\n",
       " Document(page_content='tectures [9, 47, 49, 50, 81]. LVT [81] proposes enhanced at-\\ntention mechanisms with dilated convolution to improve the\\nmodel performance and efﬁciency. Mobile-Former [9] de-\\nsigns a parallel CNN-transformer block to encode both local\\nfeatures and global interaction. However, most of them tar-\\nget at minimizing Flops and parameters [16], which could\\nhave low correlations with actual inference latency [70] and\\nstill inferior to efﬁcient CNNs in speed. Different from\\nthem, we explore models with fast inference by directly op-\\ntimizing their throughput on different hardware and deploy-\\nment settings, and design a family of hierarchical models\\nwith a good trade-off between speed and accuracy.\\n6. Conclusion\\nIn this paper, we have presented a systematic analysis on\\nthe factors that affect the inference speed of vision trans-\\nformers, and proposed a new family of fast vision trans-\\nformers with memory-efﬁcient operations and cascaded\\ngroup attention, named EfﬁcientViT. Extensive experiments\\nhave demonstrated the efﬁcacy and high speed of Efﬁ-\\ncientViT, and also show its superiority on various down-\\nstream benchmarks.\\nLimitations . One limitation of EfﬁcientViT is that, de-\\nspite its high inference speed, the model size is slightly\\nlarger compared to state-of-the-art efﬁcient CNN [26] due\\nto the extra FFNs in the introduced sandwich layout. Be-\\nsides, our models are designed manually based on the de-\\nrived guidelines on building efﬁcient vision transformers.\\nIn future work, we are interested in reducing the model size\\nand incorporating automatic search techniques to further en-\\nhance the model capacity and efﬁciency.\\nAcknowledgement. Prof. Yuan was partially supported\\nby Hong Kong Research Grants Council (RGC) General\\nResearch Fund 11211221, and Innovation and Technology\\nCommission-Innovation and Technology Fund ITS/100/20.\\n8', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 7}),\n",
       " Document(page_content='References\\n[1] Apple. Coremltools: Use coremltools to convert machine\\nlearning models from third-party libraries to the core ml for-\\nmat, 2021. 2, 5\\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\\nLayer normalization. arXiv , 2016. 5, 7\\n[3] Junjie Bai, Fang Lu, Ke Zhang, et al. Onnx: Open neural net-\\nwork exchange. https://github.com/onnx/onnx ,\\n2019. 2, 5\\n[4] Lucas Beyer, Olivier J H ´enaff, Alexander Kolesnikov, Xi-\\naohua Zhai, and A ¨aron van den Oord. Are we done with\\nimagenet? arXiv preprint arXiv:2006.07159 , 2020. 8\\n[5] Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen\\nLin, Ming Sun, Junjie Yan, and Wanli Ouyang. Glit: Neural\\narchitecture search for global and local image transformer.\\nInICCV , 2021. 6\\n[6] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\\nbox and benchmark. arXiv preprint arXiv:1906.07155 , 2019.\\n5\\n[7] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin\\nLing. Autoformer: Searching transformers for visual recog-\\nnition. In ICCV , 2021. 3\\n[8] Wuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song,\\nZhangyang Wang, and Denny Zhou. Auto-scaling vision\\ntransformers without training. In ICLR , 2021. 3\\n[9] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen\\nLiu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-\\nformer: Bridging mobilenet and transformer. In CVPR ,\\n2022. 1, 6, 8\\n[10] Franc ¸ois Chollet. Xception: Deep learning with depthwise\\nseparable convolutions. In CVPR , 2017. 3, 4, 5, 8\\n[11] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\\nhan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\\net al. Rethinking attention with performers. In ICLR , 2021.\\n2, 8\\n[12] Xiangxiang Chu, Bo Zhang, and Ruijun Xu. Fairnas: Re-\\nthinking evaluation fairness of weight sharing neural archi-\\ntecture search. In ICCV , pages 12239–12248, 2021. 7\\n[13] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\\nvan, and Quoc V Le. Autoaugment: Learning augmentation\\nstrategies from data. In CVPR , pages 113–123, 2019. 5\\n[14] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.\\nCoatnet: Marrying convolution and attention for all data\\nsizes. NeurIPS , 34:3965–3977, 2021. 4\\n[15] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra,\\nand Christopher R ´e. Flashattention: Fast and memory-\\nefﬁcient exact attention with io-awareness. arXiv preprint\\narXiv:2205.14135 , 2022. 1, 2, 8\\n[16] Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish\\nVaswani, and Yi Tay. The efﬁciency misnomer. In ICLR ,\\n2022. 8\\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\ndatabase. In CVPR , 2009. 1, 5, 6, 7, 8[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. ICLR , 2021. 1, 2, 3,\\n7, 8\\n[19] Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen,\\nZhicheng Yan, Yuandong Tian, qiang liu, and Vikas Chan-\\ndra. NASVit: Neural architecture search for efﬁcient vision\\ntransformers with gradient conﬂict aware supernet training.\\nInICLR , 2022. 1\\n[20] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron,\\nPierre Stock, Armand Joulin, Herv ´e J´egou, and Matthijs\\nDouze. Levit: a vision transformer in convnet’s clothing for\\nfaster inference. In ICCV , 2021. 5, 7, 8\\n[21] Jiaqi Gu, Hanqing Zhu, Chenghao Feng, Mingjie Liu, Zix-\\nuan Jiang, Ray T Chen, and David Z Pan. Towards memory-\\nefﬁcient neural networks via multi-level in situ generation.\\nInICCV , pages 5229–5238, 2021. 1\\n[22] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,\\nZechun Liu, Yichen Wei, and Jian Sun. Single path one-shot\\nneural architecture search with uniform sampling. In ECCV ,\\npages 544–560. Springer, 2020. 7', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 8}),\n",
       " Document(page_content='Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot\\nneural architecture search with uniform sampling. In ECCV ,\\npages 544–560. Springer, 2020. 7\\n[23] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu,\\nand Chang Xu. Ghostnet: More features from cheap opera-\\ntions. In CVPR , pages 1580–1589, 2020. 6, 8\\n[24] Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chun-\\njing Xu, Enhua Wu, and Qi Tian. Ghostnets on heteroge-\\nneous devices via cheap operations. Int. J. Comput. Vis. ,\\n130(4):1050–1069, 2022. 8\\n[25] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv , 2016. 5\\n[26] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\\nbilenetv3. In ICCV , 2019. 2, 5, 6, 7, 8\\n[27] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\\ntional neural networks for mobile vision applications. arXiv\\npreprint arXiv:1704.04861 , 2017. 4, 7\\n[28] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Trans-\\nformer quality in linear time. In ICML , pages 9099–9117.\\nPMLR, 2022. 2\\n[29] Tao Huang, Lang Huang, Shan You, Fei Wang, Chen Qian,\\nand Chang Xu. Lightvit: Towards light-weight convolution-\\nfree vision transformers. arXiv preprint arXiv:2207.05557 ,\\n2022. 1\\n[30] Sergey Ioffe and Christian Szegedy. Batch normalization:\\nAccelerating deep network training by reducing internal co-\\nvariate shift. In ICML , 2015. 5\\n[31] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and\\nTorsten Hoeﬂer. Data movement is all you need: A case\\nstudy on optimizing transformers. MLSys , 3:711–732, 2021.\\n1, 2\\n[32] Congfeng Jiang, Yitao Qiu, Weisong Shi, Zhefeng Ge, Jiwei\\nWang, Shenglei Chen, Christophe Cerin, Zujie Ren, Guoyao\\n9', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 8}),\n",
       " Document(page_content='Xu, and Jiangbin Lin. Characterizing co-located workloads\\nin alibaba cloud datacenters. IEEE Trans. on Cloud Comput. ,\\n2020. 1\\n[33] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal,\\nand Tushar Krishna. An optimized dataﬂow for miti-\\ngating attention performance bottlenecks. arXiv preprint\\narXiv:2107.06419 , 2021. 2\\n[34] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Re-\\nformer: The efﬁcient transformer. In ICLR , 2020. 2, 8\\n[35] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\\n3d object representations for ﬁne-grained categorization. In\\nICCVW , pages 554–561, 2013. 7\\n[36] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\\nlayers of features from tiny images. 2009. 7\\n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\nImagenet classiﬁcation with deep convolutional neural net-\\nworks. Commun. ACM , 60(6):84–90, 2017. 4\\n[38] Yann LeCun, John Denker, and Sara Solla. Optimal brain\\ndamage. NeurIPS , 2, 1989. 3\\n[39] Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang,\\nZhihui Li, and Xiaojun Chang. Ds-net++: Dynamic weight\\nslicing for efﬁcient inference in cnns and vision transform-\\ners.IEEE Trans. Pattern Anal. Mach. Intell. , 2022. 3\\n[40] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evan-\\ngelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Ef-\\nﬁcientformer: Vision transformers at mobilenet speed. In\\nNeurIPS , 2022. 1, 7\\n[41] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\\nPiotr Doll ´ar. Focal loss for dense object detection. In ICCV ,\\npages 2980–2988, 2017. 5\\n[42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nECCV , 2014. 5, 7\\n[43] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\\nSwin transformer v2: Scaling up capacity and resolution. In\\nCVPR , 2022. 1\\n[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nICCV , 2021. 1, 2, 3, 5, 6, 7, 8\\n[45] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and\\nTrevor Darrell. Rethinking the value of network pruning. In\\nICLR , 2018. 2, 3\\n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. In ICLR , 2018. 5\\n[47] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan\\nCao, Yongjian Wu, Feiyue Huang, and Rongrong Ji. To-\\nwards lightweight transformer via group-wise transforma-\\ntion for vision-and-language tasks. IEEE Trans. Image Pro-\\ncess. , 31:3386–3398, 2022. 8\\n[48] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\\nShufﬂenet v2: Practical guidelines for efﬁcient cnn architec-\\nture design. In ECCV , pages 116–131, 2018. 6, 8\\n[49] Muhammad Maaz, Abdelrahman Shaker, Hisham\\nCholakkal, Salman Khan, Syed Waqas Zamir, Rao Muham-\\nmad Anwer, and Fahad Shahbaz Khan. Edgenext: efﬁcientlyamalgamated cnn-transformer architecture for mobile vision\\napplications. In ECCVW , 2022. 1, 8\\n[50] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-\\nweight, general-purpose, and mobile-friendly vision trans-\\nformer. In ICLR , 2021. 1, 2, 6, 8\\n[51] Sachin Mehta and Mohammad Rastegari. Separable self-\\nattention for mobile vision transformers. arXiv preprint\\narXiv:2206.02680 , 2022. 2, 6, 8\\n[52] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen\\nheads really better than one? NeurIPS , 32, 2019. 3\\n[53] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio,\\nand Jan Kautz. Importance estimation for neural network\\npruning. In CVPR , pages 11264–11272, 2019. 3\\n[54] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units\\nimprove restricted boltzmann machines. In ICML , 2010. 5\\n[55] M-E Nilsback and Andrew Zisserman. A visual vocabulary\\nfor ﬂower classiﬁcation. In CVPR , 2006. 7\\n[56] Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz\\nDudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 9}),\n",
       " Document(page_content='for ﬂower classiﬁcation. In CVPR , 2006. 7\\n[56] Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz\\nDudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais\\nMartinez. Edgevits: Competing light-weight cnns on mobile\\ndevices with vision transformers. In ECCV , 2022. 1, 6\\n[57] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision\\ntransformers with hilo attention. NeurIPS , 2022. 2, 8\\n[58] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\\nCV Jawahar. Cats and dogs. In CVPR , pages 3498–3505,\\n2012. 7\\n[59] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\\ning Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:\\nAn imperative style, high-performance deep learning library.\\nNeurIPS , 2019. 5\\n[60] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll ´ar. Designing network design\\nspaces. In CVPR , pages 10428–10436, 2020. 8\\n[61] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure\\nLeskovec, Dale Schuurmans, and Bo Dai. Combiner: Full at-\\ntention transformer with sparse computation cost. NeurIPS ,\\n34:22470–22482, 2021. 2, 8\\n[62] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim\\nNeumann, Rodolphe Jenatton, Andr ´e Susano Pinto, Daniel\\nKeysers, and Neil Houlsby. Scaling vision with sparse mix-\\nture of experts. NeurIPS , 34:8583–8595, 2021. 1\\n[63] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\\nresiduals and linear bottlenecks. In CVPR , pages 4510–4520,\\n2018. 5, 6, 7, 8\\n[64] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\\nchitecture for computer vision. In CVPR , pages 2818–2826,\\n2016. 4\\n[65] Hamid Tabani, Ajay Balasubramaniam, Shabbir Marzban,\\nElahe Arani, and Bahram Zonooz. Improving the efﬁciency\\nof transformers for resource-constrained devices. In DSD ,\\npages 449–456, 2021. 2\\n[66] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\\nMark Sandler, Andrew Howard, and Quoc V Le. Mnas-\\nnet: Platform-aware neural architecture search for mobile.\\nInCVPR , pages 2820–2828, 2019. 7\\n10', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 9}),\n",
       " Document(page_content='[67] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\\nscaling for convolutional neural networks. In ICML , 2019.\\n6, 8\\n[68] Mingxing Tan and Quoc V Le. Mixconv: Mixed depthwise\\nconvolutional kernels. In BMVC , 2019. 7\\n[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\\ndata-efﬁcient image transformers & distillation through at-\\ntention. In ICML . PMLR, 2021. 1, 2, 3, 5, 7, 8\\n[70] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, On-\\ncel Tuzel, and Anurag Ranjan. An improved one millisecond\\nmobile backbone. arXiv preprint arXiv:2206.04040 , 2022. 6,\\n8\\n[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 2,\\n3, 8\\n[72] Anand Venkat, Tharindu Rusira, Raj Barik, Mary Hall, and\\nLeonard Truong. Swirl: High-performance many-core cpu\\ncode generation for deep neural networks. Int. J. High Per-\\nform. Comput. Appl. , 33(6):1275–1289, 2019. 1, 8\\n[73] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich,\\nand Ivan Titov. Analyzing multi-head self-attention: Spe-\\ncialized heads do the heavy lifting, the rest can be pruned. In\\nACL, pages 5797–5808, 2019. 3\\n[74] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and\\nHao Ma. Linformer: Self-attention with linear complexity.\\narXiv preprint arXiv:2006.04768 , 2020. 2, 8\\n[75] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\\nmid vision transformer: A versatile backbone for dense pre-\\ndiction without convolutions. In ICCV , 2021. 2, 8\\n[76] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\\nPvtv2: Improved baselines with pyramid vision transformer.\\nComput. Vis. Media , 2022. 7, 8\\n[77] Ross Wightman. Pytorch image models, 2019. 5\\n[78] Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin\\nXiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast pretraining\\ndistillation for small vision transformers. In ECCV , 2022. 1\\n[79] Sitong Wu, Tianyi Wu, Haoru Tan, and Guodong Guo. Pale\\ntransformer: A general vision transformer backbone with\\npale-shaped attention. In AAAI , 2022. 1\\n[80] Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor\\nDarrell, and Ross Girshick. Early convolutions help trans-\\nformers see better. NeurIPS , 2021. 5\\n[81] Chenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zi-\\njun Wei, Zhe Lin, and Alan Yuille. Lite vision transformer\\nwith enhanced self-attention. In CVPR , pages 11998–12008,\\n2022. 1, 8\\n[82] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, and\\nJan Kautz. Nvit: Vision transformer compression and param-\\neter redistribution. arXiv preprint arXiv:2110.04869 , 2021.\\n3\\n[83] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou,\\nXinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer\\nis actually what you need for vision. In CVPR , pages 10819–\\n10829, 2022. 6, 7[84] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. ICCV , 2021. 7\\n[85] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\\ntion. In ICLR , 2018. 5\\n[86] Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin\\nXiao, Jianlong Fu, and Lu Yuan. Minivit: Compressing vi-\\nsion transformers with weight multiplexing. In CVPR , 2022.\\n1, 5\\n[87] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\\nShufﬂenet: An extremely efﬁcient convolutional neural net-\\nwork for mobile devices. In CVPR , 2018. 3, 4, 5, 8\\n[88] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\\nYi Yang. Random erasing data augmentation. In AAAI , 2020.\\n5\\n[89] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\\nLe. Learning transferable architectures for scalable image\\nrecognition. In CVPR , pages 8697–8710, 2018. 6, 7\\n11', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 10}),\n",
       " Document(page_content='CREST: A Joint Framework for Rationalization and\\nCounterfactual Text Generation\\nMarcos Treviso1,2∗, Alexis Ross3, Nuno M. Guerreiro1,2, André F. T. Martins1,2,4\\n1Instituto de Telecomunicações, Lisbon, Portugal\\n2Instituto Superior Técnico & LUMLIS (Lisbon ELLIS Unit), Lisbon, Portugal\\n3Massachusetts Institute of Technology\\n4Unbabel, Lisbon, Portugal\\nAbstract\\nSelective rationales and counterfactual exam-\\nples have emerged as two effective, comple-\\nmentary classes of interpretability methods for\\nanalyzing and training NLP models. However,\\nprior work has not explored how these meth-\\nods can be integrated to combine their comple-\\nmentary advantages. We overcome this limita-\\ntion by introducing CREST (ContRastive Edits\\nwith Sparse raTionalization), a joint framework\\nfor selective rationalization and counterfactual\\ntext generation, and show that this framework\\nleads to improvements in counterfactual quality,\\nmodel robustness, and interpretability. First,\\nCREST generates valid counterfactuals that\\nare more natural than those produced by pre-\\nvious methods, and subsequently can be used\\nfor data augmentation at scale, reducing the\\nneed for human-generated examples. Second,\\nwe introduce a new loss function that lever-\\nages CREST counterfactuals to regularize se-\\nlective rationales and show that this regulariza-\\ntion improves both model robustness and ratio-\\nnale quality, compared to methods that do not\\nleverage CREST counterfactuals. Our results\\ndemonstrate that CREST successfully bridges\\nthe gap between selective rationales and coun-\\nterfactual examples, addressing the limitations\\nof existing methods and providing a more com-\\nprehensive view of a model’s predictions.\\n1 Introduction\\nAs NLP models have become larger and less trans-\\nparent, there has been a growing interest in devel-\\noping methods for finer-grained interpretation and\\ncontrol of their predictions. One class of meth-\\nods leverages selective rationalization (Lei et al.,\\n2016; Bastings et al., 2019), which trains models to\\nfirst select rationales , or subsets of relevant input to-\\nkens, and then make predictions based only on the\\nselected rationales. These methods offer increased\\ninterpretability, as well as learning benefits, such\\n∗Correspondence to: marcos.treviso@tecnico.pt\\nFigure 1: Our generation procedure consists of two\\nstages: (i) a mask stage that highlights relevant tokens\\nin the input through a learnable masker; and (ii) an\\nedit stage, which receives a masked input and uses a\\nmasked language model to infill spans conditioned on a\\nprepended label.\\nas improved robustness to input perturbations (Jain\\net al., 2020; Chen et al., 2022). Another class of\\nmethods generates counterfactual examples , or\\nmodifications to input examples that change their\\nlabels. By providing localized views of decision\\nboundaries, counterfactual examples can be used as\\nexplanations of model predictions, contrast datasets\\nfor fine-grained evaluation, or new training data-\\npoints for learning more robust models (Ross et al.,\\n2021; Gardner et al., 2020; Kaushik et al., 2020).\\nThis paper is motivated by the observation that\\nselective rationales and counterfactual examples\\nallow for interpreting and controlling model be-\\nhavior through different means: selective rational-\\nization improves model transparency by weaving\\ninterpretability into a model’s internal decision-\\nmaking process, while counterfactual examples\\nprovide external signal more closely aligned with\\nhuman causal reasoning (Wu et al., 2021).\\nWe propose to combine both methods to lever-\\nage their complementary advantages. We introduce\\nCREST ( ContRastive Edits with Sparse raTional-\\nization ), a joint framework for rationalization andarXiv:2305.17075v1  [cs.CL]  26 May 2023', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 2: Overview of CREST-Rationalization. We start by passing an input xthrough CREST-Generation, which\\nyields a counterfactual edit ˜xalong side two masks: z⋆for the original input, and ˜z⋆for the counterfactual. Next,\\nwe train a new rationalizer (masker) decomposed into two flows: a factual flow that takes in xand produces a\\nrationale z, and a counterfactual flow that receives ˜xand produces a rationale ˜z. Lastly, we employ a regularization\\ntermΩ(z,˜z)to encourage agreement between rationales for original and counterfactual examples.\\ncounterfactual text generation. CREST first gener-\\nates high-quality counterfactuals (Figure 1), then\\nleverages those counterfactuals to encourage con-\\nsistency across “flows” for factual and counterfac-\\ntual inputs (Figure 2). In doing so, CREST unifies\\ntwo key important dimensions of interpretability\\nintroduced by Doshi-Velez and Kim (2017, §3.2),\\nforward simulation and counterfactual simulation.\\nOur main contributions are:1\\n•We present CREST-Generation (Figure 1), a\\nnovel approach to generating counterfactual\\nexamples by combining sparse rationalization\\nwith span-level masked language modeling (§3),\\nwhich produces valid, fluent, and diverse coun-\\nterfactuals (§4, Table 1).\\n•We introduce CREST-Rationalization (Fig-\\nure 2), a novel approach to regularizing ratio-\\nnalizers. CREST-Rationalization decomposes a\\nrationalizer into factual and counterfactual flows\\nand encourages agreement between the rationales\\nfor both (§5).\\n•We show that CREST-generated counterfactuals\\ncan be effectively used to increase model robust-\\nness, leading to larger improvements on contrast\\nand out-of-domain datasets than using manual\\ncounterfactuals (§6.2, Tables 2 and 3).\\n•We find that rationales trained with CREST-\\nRationalization not only are more plausible, but\\nalso achieve higher forward and counterfactual\\nsimulabilities (§6.3, Table 4).\\n1Code at https://github.com/deep-spin/crest/ .Overall, our experiments show that CREST suc-\\ncessfully combines the benefits of counterfactual\\nexamples and selective rationales to improve the\\nquality of each, resulting in a more interpretable\\nand robust learned model.\\n2 Background\\n2.1 Rationalizers\\nThe traditional framework of rationalization in-\\nvolves training two components cooperatively: the\\ngenerator —which consists of an encoder and an\\nexplainer—and the predictor . The generator en-\\ncodes the input and produces a “rationale” (e.g.,\\nword highlights), while the predictor classifies the\\ntext given only the rationale as input (Lei et al.,\\n2016).\\nAssume a document xwithntokens as input.\\nThe encoder module ( enc) converts the input\\ntokens into d-dimensional hidden state vectors\\nH∈Rn×d, which are passed to the explainer\\n(expl) to generate a latent mask z∈ {0,1}n. The\\nlatent mask serves as the rationale since it is used\\nto select a subset of the input x⊙z, which is then\\npassed to the predictor module ( pred) to produce\\na final prediction ˆy∈ Y, where Y={1, ..., k}\\nfork-class classification. The full process can be\\nsummarized as follows:\\nz=expl(enc(x;ϕ);γ), (1)\\nˆy=pred(x⊙z;θ), (2)\\nwhere ϕ, γ, θ are trainable parameters. To ensure\\nthat the explainer does not select all tokens (i.e.,\\nzi= 1,∀i), sparsity is usually encouraged in the', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 1}),\n",
       " Document(page_content='rationale extraction. Moreover, explainers can also\\nbe encouraged to select contiguous words, as there\\nis some evidence that it improves readibility (Jain\\net al., 2020). These desired properties may\\nbe encouraged via regularization terms during\\ntraining (Lei et al., 2016; Bastings et al., 2019), or\\nvia application of sparse mappings (Treviso and\\nMartins, 2020; Guerreiro and Martins, 2021).\\nIn this work, we will focus specifically on\\nthe SPECTRA rationalizer (Guerreiro and Mar-\\ntins, 2021): this model leverages an explainer\\nthat extracts a deterministic structured mask z\\nby solving a constrained inference problem with\\nSparseMAP (Niculae et al., 2018). SPECTRA\\nhas been shown to achieve comparable perfor-\\nmance with other rationalization approaches, in\\nterms of end-task performance, plausibility with\\nhuman explanations, and robustness to input per-\\nturbation (Chen et al., 2022). Moreover, it is easier\\nto train than other stochastic alternatives (Lei et al.,\\n2016; Bastings et al., 2019), and, importantly, it\\nallows for simple control over the properties of the\\nrationales, such as sparsity via its constrained in-\\nference formulation: by setting a budget Bon the\\nrationale extraction, SPECTRA ensures that the\\nrationale size will not exceed ⌈Bn⌉tokens.\\n2.2 Counterfactuals\\nIn NLP, counterfactuals refer to alternative texts\\nthat describe a different outcome than what is en-\\ncoded in a given factual text. Prior works (Verma\\net al., 2020) have focused on developing methods\\nfor generating counterfactuals that adhere to several\\nkey properties, including:\\n•Validity : the generated counterfactuals should\\nencode a different label from the original text.\\n•Closeness : the changes made to the text should\\nbe small, not involving large-scale rewriting of\\nthe input.\\n•Fluency : the generated counterfactuals should\\nbe coherent and grammatically correct.\\n•Diversity : the method should generate a wide\\nrange of counterfactuals with diverse character-\\nistics, rather than only a limited set of variations.\\nWhile many methods for automatic counterfac-\\ntual generation exist (Wu et al., 2021; Robeer et al.,\\n2021; Dixit et al., 2022), our work is mostly re-\\nlated to MiCE (Ross et al., 2021), which generates\\ncounterfactuals in a two stage process that involvesmasking the top- ktokens with the highest ℓ1gra-\\ndient attribution of a pre-trained classifier, and in-\\nfilling tokens for masked position with a T5-based\\nmodel (Raffel et al., 2020). MiCE further refines\\nthe resultant counterfactual with a binary search\\nprocedure to seek strictly minimal edits. However,\\nthis process is computationally expensive and, as\\nwe show in §4.2, directly optimizing for closeness\\ncan lead to counterfactuals that are less valid, fluent,\\nand diverse. Next, we present an alternative method\\nthat overcomes these limitations while still produc-\\ning counterfactuals that are close to original inputs.\\n3 CREST-Generation\\nWe now introduce CREST (ContRastive Edits with\\nSparse raTionalization), a framework that com-\\nbines selective rationalization and counterfactual\\ntext generation. CREST has two key components:\\n(i)CREST-Generation offers a controlled ap-\\nproach to generating counterfactuals, which we\\nshow are valid, fluent, and diverse (§4.2); and\\n(ii)CREST-Rationalization leverages these coun-\\nterfactuals through a novel regularization technique\\nencouraging agreement between rationales for orig-\\ninal and counterfactual examples. We demonstrate\\nthat combining these two components leads to mod-\\nels that are more robust (§6.2) and interpretable\\n(§6.3). We describe CREST-Generation below and\\nCREST-Rationalization in §5.\\nFormally, let x=⟨x1, ..., x n⟩represent a factual\\ninput text with a label yf. We define a counterfac-\\ntual as an input ˜x=⟨x1, ..., x m⟩labeled with yc\\nsuch that yf̸=yc. A counterfactual generator is\\na mapping that transforms the original text xto a\\ncounterfactual ˜x. Like MiCE, our approach for gen-\\nerating counterfactuals consists of two stages, as', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 2}),\n",
       " Document(page_content='a mapping that transforms the original text xto a\\ncounterfactual ˜x. Like MiCE, our approach for gen-\\nerating counterfactuals consists of two stages, as\\ndepicted in Figure 1: the mask and the edit stages.\\nMask stage. We aim to find a mask vector z∈\\n{0,1}nsuch that tokens xiassociated with zi=\\n1are relevant for the factual prediction ˆyfof a\\nparticular classifier C. To this end, we employ a\\nSPECTRA rationalizer as the masker . Concretely,\\nwe pre-train a SPECTRA rationalizer on the task\\nat hand with a budget constraint B, and define the\\nmask as the rationale vector z∈ {0,1}n(see §2.1).\\nEdit stage. Here, we create edits by infilling the\\nmasked positions using an editor module G, such\\nas a masked language model: ˜x∼GLM(x⊙z).\\nIn order to infill spans rather than single tokens, we\\nfollow MiCE and use a T5-based model to infill', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 2}),\n",
       " Document(page_content='IMDB SNLI\\nMethod val. ↑fl.↓div.↓clo.↓#tks val. ↑fl.↓div.↓clo.↓#tks\\nChance baseline 50.20 - - - - 52.70 - - - -\\nReferences 97.95 66.51 - - 184.4 96.75 63.52 - - 7.5\\nManual edits 93.44 72.89 81.67 0.14 183.7 93.88 65.25 35.82 0.42 7.7\\nPWWS 28.07 101.91 74.56 0.16 179.0 17.97 160.11 31.81 0.36 6.8\\nCFGAN - - - - - 34.46 155.84 68.94 0.23 7.0\\nPolyJuice 36.69 68.59 56.41 0.45 94.6 41.80 62.62 39.01 0.40 11.6\\nMiCE (bin. search) 72.13 76.72 73.76 0.20 171.3 76.17 63.94 42.18 0.35 7.9\\nMiCE (30% mask) 76.80 79.35 49.64 0.39 161.3 77.26 59.71 34.08 0.40 8.3\\nMiCE (50% mask) 83.20 89.92 20.71 0.65 115.7 84.48 68.32 24.27 0.52 7.6\\nCREST (30% mask) 75.82 67.29 57.58 0.33 180.9 75.45 62.00 41.36 0.29 7.4\\nCREST (50% mask) 93.24 50.69 23.08 0.66 193.9 81.23 62.60 30.53 0.41 7.3\\nTable 1: Intrinsic evaluation of counterfactuals generated by various methods. Validity is computed as the accuracy\\nof an off-the-shelf RoBERTa-base classifier in relation to the gold counterfactual label (not available for PWWS\\nand PolyJuice); fluency is determined by the perplexity score given by GPT-2 large; diversity is computed with\\nself-BLEU; and closeness is reported by the (normalized) edit distance to the factual input. In addition, we report\\nthe average number of tokens in the input.\\nspans for masked positions. During training, we\\nfine-tune the editor to infill original spans of text by\\nprepending gold target labels yfto original inputs.\\nIn order to generate counterfactual edits at test time,\\nwe prepend a counterfactual label ycinstead, and\\nsample counterfactuals using beam search.\\nOverall, our procedure differs from that of MiCE\\nin the mask stage: instead of extracting a mask via\\ngradient-based attributions and subsequent binary\\nsearch, we leverage SPECTRA to find an optimal\\nmask. Interestingly, by doing so, we not only\\navoid the computationally expensive binary search\\nprocedure, but we also open up new opportunities:\\nas our masking process is differentiable, we can\\noptimize our masker to enhance the quality of\\nboth the counterfactuals (§4.2) and the selected\\nrationales (§6.3). We will demonstrate the\\nlatter with our proposed CREST-Rationalization\\nsetup (§5). All implementation details for the\\nmasker and the editor can be found in §B.\\n4 Evaluating CREST Counterfactuals\\nThis section presents an extensive comparison of\\ncounterfactuals generated by different methods.\\n4.1 Experimental Setting\\nData and evaluation. We experiment with\\nour counterfactual generation framework on two\\ndifferent tasks: sentiment classification using\\nIMDB (Maas et al., 2011) and natural language\\ninference (NLI) using SNLI (Bowman et al., 2015).\\nIn sentiment classification, we only have a single\\ninput to consider, while NLI inputs consist of apremise and a hypothesis, which we concatenate\\nto form a single input. To assess the quality of\\nour automatic counterfactuals, we compare them\\nto manually crafted counterfactuals in the revised\\nIMDB and SNLI datasets created by Kaushik et al.\\n(2020). More dataset details can be found in §A.\\nTraining. We employ a SPECTRA rationalizer\\nwith a T5-small architecture as the masker, and\\ntrain it for 10 epochs on the full IMDB and SNLI\\ndatasets. We also use a T5-small architecture for\\nthe editor, and train it for 20 epochs with early stop-\\nping, following the same training recipe as MiCE.\\nFull training details can be found in §B.3.\\nGeneration. As illustrated in Figure 1, at test\\ntime we generate counterfactuals by prepending\\na contrastive label to the input and passing it to\\nthe editor. For sentiment classification, this means\\nswitching between positive and negative labels. For\\nNLI, in alignment with Dixit et al. (2022), we adopt\\na refined approach by restricting the generation of\\ncounterfactuals to entailments and contradictions\\nonly, therefore ignoring neutral examples, which\\nhave a subtle semantic meaning. In contrast, our\\npredictors were trained using neutral examples, and\\nin cases where they predict the neutral class, we', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 3}),\n",
       " Document(page_content='only, therefore ignoring neutral examples, which\\nhave a subtle semantic meaning. In contrast, our\\npredictors were trained using neutral examples, and\\nin cases where they predict the neutral class, we\\ndefault to the second-most probable class.\\nBaselines. We compare our approach with four\\nopen-source baselines that generate counterfactu-\\nals: PWWS (Ren et al., 2019), PolyJuice (Wu et al.,\\n2021), CounterfactualGAN (Robeer et al., 2021),2\\n2Despite many attempts, CounterfactualGAN did not con-\\nverge on IMDB, possibly due to the long length of the inputs.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 3}),\n",
       " Document(page_content='0.1 0.2 0.3 0.4 0.54054688296\\nBudget percentageValidity (%)\\n0.1 0.2 0.3 0.4 0.55058667482\\nBudget percentageFluency (ppl.)\\n0.1 0.2 0.3 0.4 0.51025405570\\nBudget percentageCloseness (%)original\\naugmented\\nfinetuned\\nFigure 3: Sparsity analysis of CREST-Generation on IMDB with different budget percentages. The original curves\\nshow the performance of CREST without any changes, while the augmented andfinetuned curves show the per-\\nformance of CREST when using manually crafted counterfactuals for data augmentation or finetuning, respectively.\\nand MiCE (Ross et al., 2021). In particular, to en-\\nsure a fair comparison with MiCE, we apply three\\nmodifications to the original formulation: (i) we\\nreplace its RoBERTa classifier with a T5-based\\nclassifier (as used in SPECTRA); (ii) we disable its\\nvalidity filtering;3(iii) we report results with and\\nwithout the binary search procedure by fixing the\\npercentage of masked tokens.\\nMetrics. To determine the general validity of\\ncounterfactuals, we report the accuracy of an\\noff-the-shelf RoBERTa-base classifier available\\nin the HuggingFace Hub.4Moreover, we mea-\\nsure fluency using perplexity scores from GPT-2\\nlarge (Radford et al., 2019) and diversity with self-\\nBLEU (Zhu et al., 2018). Finally, we quantify the\\nnotion of closeness by computing the normalized\\nedit distance to the factual input and the average\\nnumber of tokens in the document.\\n4.2 Results\\nResults are presented in Table 1. As expected, man-\\nually crafted counterfactuals achieve high validity,\\nsignificantly surpassing the chance baseline and\\nestablishing a reliable reference point. For IMDB,\\nwe find that CREST outperforms other methods\\nby a wide margin in terms of validity and fluency.\\nAt the same time, CREST’s validity is comparable\\nto the manually crafted counterfactuals, while sur-\\nprisingly deemed more fluent by GPT-2. Moreover,\\nwe note that our modification of disabling MiCE’s\\nminimality search leads to counterfactuals that are\\nmore valid and diverse but less fluent and less close\\nto the original inputs.\\nFor SNLI, this modification allows MiCE to\\nachieve the best overall scores, closely followed\\n3MiCE with binary search uses implicit validity filtering\\nthroughout the search process to set the masking percentage.\\n4mtreviso/roberta-base-imdb ,\\nmtreviso/roberta-base-snli .by CREST. However, when controlling for close-\\nness, we observe that CREST outperforms MiCE:\\nat closeness of ∼0.30, CREST (30% mask) outper-\\nforms MiCE with binary search in terms of fluency\\nand diversity. Similarly, at a closeness of ∼0.40,\\nCREST (50% mask) surpasses MiCE (30% mask)\\nacross the board. As detailed in §C, CREST’s coun-\\nterfactuals are more valid than MiCE’s for all close-\\nness bins lower than 38%. We provide examples of\\ncounterfactuals produced by CREST and MiCE in\\nAppendix G. Finally, we note that CREST is highly\\naffected by the masking budget, which we explore\\nfurther next.\\nSparsity analysis. We investigate how the num-\\nber of edits affects counterfactual quality by train-\\ning maskers with increasing budget constraints (as\\ndescribed in §2.1). The results in Figure 3 show\\nthat with increasing masking percentage, gener-\\nated counterfactuals become less textually similar\\nto original inputs (i.e., less close) but more valid\\nand fluent. This inverse relationship demonstrates\\nthat strict minimality, optimized for in methods like\\nMiCE, comes with tradeoffs in counterfactual qual-\\nity, and that the sparsity budget in CREST can be\\nused to modulate the trade-off between validity and\\ncloseness. In Figure 3 we also examine the benefit\\nof manually crafted counterfactuals in two ways:\\n(i) using these examples as additional training data;\\nand (ii) upon having a trained editor, further fine-\\ntuning it with these manual counterfactuals. The\\nresults suggest that at lower budget percentages,\\nexploiting a few manually crafted counterfactuals\\nto fine-tune CREST can improve the validity of\\ncounterfactuals without harming fluency.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 4}),\n",
       " Document(page_content='results suggest that at lower budget percentages,\\nexploiting a few manually crafted counterfactuals\\nto fine-tune CREST can improve the validity of\\ncounterfactuals without harming fluency.\\nValidity filtering. As previously demonstrated\\nby Wu et al. (2021) and Ross et al. (2022), it\\nis possible to filter out potentially disfluent or\\ninvalid counterfactuals by passing all examples to', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 4}),\n",
       " Document(page_content='Validity Naturalness12345RatingManual CREST MiCEFigure 4: Human study results for counterfactuals pro-\\nduced manually and automatically (CREST and MiCE).\\na classifier and discarding the subset with incorrect\\npredictions. In our case, we use the predictor\\nassociated with the masker as the classifier. We\\nfound find that applying this filtering increases\\nthe validity of IMDB counterfactuals from 75.82\\nto 86.36 with B= 0.3, and from 93.24 to 97.36\\nwith B= 0.5. For SNLI, validity jumps from\\n75.45 to 96.39 with B= 0.3, and from 81.23 to\\n96.67 with B= 0.5. These results indicate that\\nCREST can rely on its predictor to filter out invalid\\ncounterfactuals, a useful characteristic for doing\\ndata augmentation, as we will see in §6.2.\\n4.3 Human Study\\nWe conduct a small-scale human study to evaluate\\nthe quality of counterfactuals produced by MiCE\\nand CREST with 50% masking percentage. An-\\nnotators were tasked with rating counterfactuals’\\nvalidity andnaturalness (e.g., based on style, tone,\\nand grammar), each using a 5-point Likert scale.\\nTwo fluent English annotators rated 50 examples\\nfrom the IMDB dataset, and two others rated 50\\nexamples from SNLI. We also evaluate manually\\ncreated counterfactuals to establish a reliable base-\\nline. More annotation details can be found in §D.\\nThe study results, depicted in Figure 4, show that\\nhumans find manual counterfactuals to be more\\nvalid and natural compared to automatically gener-\\nated ones. Furthermore, CREST’s counterfactuals\\nreceive higher ratings for validity and naturalness\\ncompared to MiCE, aligning with the results ob-\\ntained from automatic metrics.\\n5 CREST-Rationalization\\nNow that we have a method that generates high-\\nquality counterfactual examples, a natural step is\\nto use these examples for data augmentation. How-\\never, vanilla data augmentation does not take advan-\\ntage of the paired structure of original/contrastive\\nexamples and instead just treats them as individual\\ndatapoints. In this section, we present CREST’ssecond component, CREST-Rationalization (illus-\\ntrated in Figure 2), which leverages the relation-\\nships between factual and counterfactual inputs\\nthrough a SPECTRA rationalizer with an agree-\\nment regularization strategy, described next.\\n5.1 Linking Counterfactuals and Rationales\\nWe propose to incorporate counterfactuals into\\na model’s functionality by taking advantage of\\nthe fully differentiable rationalization setup. Con-\\ncretely, we decompose a rationalizer into two flows,\\nas depicted in Figure 2: a factual flow that receives\\nfactual inputs xand outputs a factual prediction\\nˆy, and a counterfactual flow that receives coun-\\nterfactual inputs ˜xand should output a counterfac-\\ntual prediction ˜y̸= ˆy. As a by-product of using\\na rationalizer, we also obtain a factual rationale\\nz∈ {0,1}nforxand a counterfactual rationale\\n˜z∈ {0,1}mfor˜x, where n=|x|andm=|˜x|.\\nTraining. LetΘ ={ϕ, γ, θ}represent the train-\\nable parameters of a rationalizer (defined in §2.1).\\nWe propose the following loss function:\\nL(Θ) = Lf(yf,ˆy(Θ)) + αLc(yc,˜y(Θ)) (3)\\n+λΩ(z(Θ),˜z(Θ)),\\nwhere Lf(·)andLc(·)represent cross-entropy\\nlosses for the factual and counterfactual flows, re-\\nspectively, and Ω(·)is a novel penalty term to en-\\ncourage factual and counterfactual rationales to fo-\\ncus on the same positions, as defined next. α∈R\\nandλ∈Rare hyperparameters.\\nAgreement regularization. To produce paired\\nrationales for both the factual and counterfactual\\nflows, we incorporate regularization terms into the\\ntraining of a rationalizer to encourage the factual\\nexplainer to produce rationales similar to those orig-\\ninally generated by the masker z⋆, and the counter-\\nfactual explainer to produce rationales that focus\\non the tokens modified by the editor ˜z⋆. We de-\\nrive the ground truth counterfactual rationale ˜z⋆\\nby aligning xto˜xand marking tokens that were\\ninserted or substituted as 1, and others as 0. The\\nregularization terms are defined as:\\nΩ(z,˜z) =∥z(Θ)−z⋆∥2\\n2+∥˜z(Θ)−˜z⋆∥2\\n2.(4)', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 5}),\n",
       " Document(page_content='by aligning xto˜xand marking tokens that were\\ninserted or substituted as 1, and others as 0. The\\nregularization terms are defined as:\\nΩ(z,˜z) =∥z(Θ)−z⋆∥2\\n2+∥˜z(Θ)−˜z⋆∥2\\n2.(4)\\nTo allow the counterfactual rationale ˜zto focus\\non all important positions in the input, we adjust\\nthe budget for the counterfactual flow based on the\\nlength of the synthetic example produced by the\\ncounterfactual generator. Specifically, we multiply\\nthe budget by a factor of∥˜z⋆∥0\\n∥z⋆∥0.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 5}),\n",
       " Document(page_content='Setup IMDB rIMDB cIMDB RotTom SST-2 Amazon Yelp\\nF 91.1±0.3 91.4±0.8 88.5±0.9 76.5±1.6 79.8±1.6 86.0±0.7 88.5±0.7\\nWith data augmentation:\\nF+CH 90.9±0.5 92.9±0.9 90.4±1.6 76.6±1.5 80.7±1.3 86.3±1.0 89.1±1.2\\nF+CS,V 91.0±0.2 91.2±1.0 89.3±0.8 76.8±0.9 79.3±0.3 85.2±0.9 88.0±1.0\\nF+CS 90.8±0.2 91.6±1.3 89.2±0.4 76.7±1.0 80.6±0.6 86.4±0.6 89.1±0.5\\nWith agreement regularization:\\nF&CS,V 90.7±0.5 92.2±0.7 88.9±1.0 76.3±1.4 80.2±1.3 86.3±0.7 88.9±0.7\\nF&CS 91.2±0.5 92.9±0.5 89.7±1.1 77.3±2.3 81.1±2.4 86.8±0.8 89.3±0.7\\nTable 2: Accuracy of SPECTRA trained on IMDB and evaluated on in-domain ,contrast , and out-of-domain datasets.\\nWe present mean and std. values across five random seeds. Values in bold : top results; underlined : second-best.\\n6Exploiting Counterfactuals for Training\\nIn this section, we evaluate the effects of incorporat-\\ning CREST-generated counterfactuals into training\\nby comparing a vanilla data augmentation approach\\nwith our CREST-Rationalization approach. We\\ncompare how each affects model robustness (§6.2)\\nand interpretability (§6.3).\\n6.1 Experimental Setting\\nWe use the IMDB and SNLI datasets to train\\nSPECTRA rationalizers with and without coun-\\nterfactual examples, and further evaluate on\\nin-domain ,contrast and out-of-domain (OOD)\\ndatasets. For IMDB, we evaluate on the\\nrevised IMDB ,contrast IMDB ,RottenTomatoes ,\\nSST-2 ,Amazon Polarity , and Yelp. For SNLI, we\\nevaluate on the Hard SNLI ,revised SNLI ,break ,\\nMultiNLI , and Adversarial NLI . Dataset details\\ncan be found in §A. To produce CREST counter-\\nfactuals, which we refer to as “synthetic”, we use\\na 30% masking budget as it provides a good bal-\\nance between validity, fluency, and closeness ( cf.\\nFigure 3). We tune the counterfactual loss ( α) and\\nagreement regularization ( λ) weights on the dev set.\\nWe report results with α= 0.01andλ= 0.001for\\nIMDB, and α= 0.01andλ= 0.1for SNLI.\\n6.2 Robustness Results\\nTables 2 and 3 show results for counterfactual\\ndata augmentation and agreement regularization\\nfor IMDB and SNLI, respectively. We compare a\\nstandard SPECTRA trained on factual examples\\n(F) with other SPECTRA models trained on aug-\\nmentated data from human-crafted counterfactuals\\n(F+CH) and synthetic counterfactuals generated\\nby CREST ( F+CS), which we additionally post-\\nprocess to drop invalid examples ( F+CS,V).\\nDiscussion. As shown in Table 2, CREST-\\nRationalization ( F&CS) consistently outperformsvanilla counterfactual augmentation ( F+CS) on\\nall sentiment classification datasets. It achieves\\nthe top results on the full IMDB and on all OOD\\ndatasets, while also leading to strong results on\\ncontrastive datasets—competitive with manual\\ncounterfactuals ( F+CH). When analyzing the\\nperformance of CREST-Rationalization trained\\non a subset of valid examples ( F&CS,V) versus\\nthe entire dataset ( F&CS), the models trained\\non the entire dataset maintain a higher level of\\nperformance across all datasets. However, when\\nusing counterfactuals for data augmentation, this\\ntrend is less pronounced, especially for in-domain\\nand contrastive datasets. In §E, we explore the\\nimpact of the number of augmented examples\\non results and find that, consistent with previous\\nresearch (Huang et al., 2020; Joshi and He, 2022),\\naugmenting the training set with a small portion\\nof valid and diverse synthetic counterfactuals leads\\nto more robust models, and can even outweigh the\\nbenefits of manual counterfactuals.\\nExamining the results for NLI in Table 3, we\\nobserve that both counterfactual augmentation and\\nagreement regularization interchangeably yield top\\nresults across datasets. Remarkably, in contrast to\\nsentiment classification, we achieve more substan-\\ntial improvements with agreement regularization\\nmodels when these are trained on valid counterfac-\\ntuals, as opposed to the full set.\\nOverall, these observations imply that CREST-\\nRationalization is a viable alternative to data aug-\\nmentation for improving model robustness, espe-\\ncially for learning contrastive behavior for senti-', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 6}),\n",
       " Document(page_content='Overall, these observations imply that CREST-\\nRationalization is a viable alternative to data aug-\\nmentation for improving model robustness, espe-\\ncially for learning contrastive behavior for senti-\\nment classification. In the next section, we explore\\nthe advantages of CREST-Rationalization for im-\\nproving model interpretability.\\n6.3 Interpretability Analysis\\nIn our final experiments, we assess the benefits of\\nour proposed regularization method on model inter-', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 6}),\n",
       " Document(page_content='Setup SNLI SNLI-h rSNLI break MNLI-m MNLI-mm ANLI\\nF 86.6±0.2 73.7±0.2 71.1±0.8 69.5±1.5 64.6±1.1 65.9±0.9 32.6±0.7\\nWith data augmentation:\\nF+CH 86.6±0.3 74.9±1.1 72.4±0.3 70.1±1.9 64.2±0.9 65.8±0.9 31.8±0.4\\nF+CS,V 86.5±0.3 75.8±1.2 71.8±1.0 69.1±2.0 64.4±0.3 65.9±0.4 32.2±0.2\\nF+CS 86.6±0.3 74.7±1.1 71.6±0.8 71.2±1.4 64.5±0.4 66.4±0.6 32.2±1.0\\nWith agreement regularization:\\nF&CS,V 86.8±0.1 75.3±0.8 66.8±0.7 68.2±2.1 64.6±0.7 66.1±0.6 32.8±0.6\\nF&CS 86.6±0.1 75.5±1.3 67.0±1.3 69.9±1.7 64.2±1.1 66.0±0.7 32.5±0.5\\nTable 3: Accuracy of SPECTRA trained on SNLI and evaluated on in-domain ,contrast , and out-of-domain datasets.\\nWe present mean and std. values across five random seeds. Values in bold : top results; underlined : second-best.\\npretability. We evaluate effects on rationale quality\\nalong three dimensions: plausibility, forward simu-\\nlability, and counterfactual simulability.\\nPlausibility. We use the MovieReviews (DeY-\\noung et al., 2020) and the e-SNLI (Camburu et al.,\\n2018) datasets to study the human-likeness of ratio-\\nnales by matching them with human-labeled expla-\\nnations and measuring their AUC, which automati-\\ncally accounts for multiple binarization levels.5\\nForward simulability. Simulability measures\\nhow often a human agrees with a given classifier\\nwhen presented with explanations, and many works\\npropose different variants to compute simulability\\nscores in an automatic way (Doshi-Velez and Kim,\\n2017; Treviso and Martins, 2020; Hase et al., 2020;\\nPruthi et al., 2022). Here, we adopt the framework\\nproposed by Treviso and Martins (2020), which\\nviews explanations as a message between a clas-\\nsifier and a linear student model, and determines\\nsimulability as the fraction of examples for which\\nthe communication is successful. In our case, we\\ncast a SPECTRA rationalizer as the classifier, use\\nits rationales as explanations, and train a linear stu-\\ndent on factual examples of the IMDB and SNLI\\ndatasets. High simulability scores indicate more\\nunderstandable and informative explanations.\\nCounterfactual simulability. Building on the\\nmanual simulability setup proposed by Doshi-Velez\\nand Kim (2017), we introduce a new approach to\\nautomatically evaluate explanations that interact\\nwith counterfactuals. Formally, let Cbe a classifier\\nthat when given an input xproduces a prediction ˆy\\nand a rationale z. Moreover, let Gbe a pre-trained\\ncounterfactual editor, which receives xandzand\\nproduces a counterfactual ˜xby infilling spans on\\npositions masked according to z(e.g., via masking).\\n5We determine the explanation score for a single word by\\ncalculating the average of the scores of its word pieces.We define counterfactual simulability as follows:\\n1\\nNNX\\nn=1[[C(xn)̸=C(G(xn⊙zn))]],(5)\\nwhere [[·]]is the Iverson bracket notation. Intu-\\nitively, counterfactual simulability measures the\\nability of a rationale to change the label predicted\\nby the classifier when it receives a contrastive edit\\nwith infilled tokens by a counterfactual generator as\\ninput. Therefore, a high counterfactual simulability\\nindicates that the rationale zfocuses on the highly\\ncontrastive parts of the input.\\nResults. The results of our analysis are shown\\nin Table 4. We observe that plausibility can sub-\\nstantially benefit from synthetic CREST-generated\\ncounterfactual examples, especially for a ratio-\\nnalizer trained with our agreement regularization,\\nwhich outperforms other approaches by a large\\nmargin. Additionally, leveraging synthetic counter-\\nfactuals, either via data augmentation or agreement\\nregularization, leads to a high forward simulabil-\\nity score, though by a smaller margin—within the\\nstandard deviation of other approaches. Finally,\\nwhen looking at counterfactual simulability, we\\nnote that models that leverage CREST counterfac-\\ntuals consistently lead to better rationales. In par-\\nticular, agreement regularization leads to strong re-\\nsults on both tasks while also producing more plau-\\nsible rationales, showing the efficacy of CREST-', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 7}),\n",
       " Document(page_content='ticular, agreement regularization leads to strong re-\\nsults on both tasks while also producing more plau-\\nsible rationales, showing the efficacy of CREST-\\nRationalization in learning contrastive behavior.\\n7 Related Works\\nGenerating counterfactuals. Existing ap-\\nproaches to generating counterfactuals for NLP\\nuse heuristics (Ren et al., 2019; Ribeiro et al.,\\n2020), leverage plug-and-play approaches to\\ncontrolled generation (Madaan et al., 2021), or,\\nmost relatedly, fine-tune language models to', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 7}),\n",
       " Document(page_content='Sentiment Classification Natural Language Inference\\nSetup Plausibility F. sim. C. sim. Plausibility F. sim. C. sim.\\nF 0.6733 ±0.02 91.70±0.92 81.18±2.79 0.7735 ±0.00 59.26±0.41 70.01±0.44\\nWith data augmentation:\\nF+CH 0.6718 ±0.04 91.44±1.46 80.53±4.17 0.7736 ±0.01 59.51±0.86 69.90±0.57\\nF+CS 0.6758 ±0.01 91.68±0.59 84.54±1.09 0.7779 ±0.00 59.54±0.08 70.76±0.54\\nWith agreement regularization:\\nF&CS 0.6904 ±0.02 91.93±0.83 86.43±1.56 0.7808 ±0.00 59.31±0.20 70.69±0.29\\nTable 4: Interpretability analysis of rationalizers trained with CREST-generated counterfactuals, either with data\\naugmentation or agreement regularization. Plausibility represents matching with human rationales, whereas F. sim.\\nand C. sim. represent forward and counterfactual simulability. Bold : top results; underlined : second-best.\\ngenerate counterfactuals (Wu et al., 2021; Ross\\net al., 2021, 2022; Robeer et al., 2021). For\\ninstance, PolyJuice (Wu et al., 2021) finetunes a\\nGPT-2 model on human-crafted counterfactuals\\nto generate counterfactuals following pre-defined\\ncontrol codes, while CounterfactualGAN (Robeer\\net al., 2021) adopts a GAN-like setup. We show\\nthat CREST-Generation outperforms both methods\\nin terms of counterfactual quality. Most closely\\nrelated is MiCE (Ross et al., 2021), which also uses\\na two-stage approach based on a masker and an\\neditor to generate counterfactuals. Unlike MiCE,\\nwe propose to relax the minimality constraint and\\ngenerate masks using selective rationales rather\\nthan gradients, resulting not only in higher-quality\\ncounterfactuals, but also in a fully-differentiable\\nset-up that allows for further optimization of the\\nmasker. Other recent work includes Tailor (Ross\\net al., 2022), a semantically-controlled generation\\nsystem that requires a human-in-the-loop to\\ngenerate counterfactuals, as well as retrieval-based\\nand prompting approaches such as RGF (Paranjape\\net al., 2022) and CORE (Dixit et al., 2022).\\nTraining with counterfactuals. Existing ap-\\nproaches to training with counterfactuals predom-\\ninantly leverage data augmentation. Priors works\\nhave explored how augmenting with both man-\\nual (Kaushik et al., 2020; Khashabi et al., 2020;\\nHuang et al., 2020; Joshi and He, 2022) and\\nautomatically-generated (Wu et al., 2021; Ross\\net al., 2022; Dixit et al., 2022) counterfactuals\\naffects model robustness. Unlike these works,\\nCREST-Rationalization introduces a new strategy\\nfor training with counterfactuals that leverages\\nthe paired structure of original and counterfactual\\nexamples, improving model robustness and inter-\\npretability compared to data augmentation. Also\\nrelated is the training objective proposed by Guptaet al. (2021) to promote consistency across pairs of\\nexamples with shared substructures for neural mod-\\nule networks, and the loss term proposed by Teney\\net al. (2020) to model the factual-counterfactual\\npaired structured via gradient supervision. In con-\\ntrast, CREST can be used to generate paired ex-\\namples, can be applied to non-modular tasks, and\\ndoes not require second-order derivatives.\\nRationalization. There have been many modifi-\\ncations to the rationalization setup to improve task\\naccuracy and rationale quality. Some examples\\ninclude conditioning the rationalization on\\npre-specified labels (Yu et al., 2019), using an\\ninformation-bottleneck formulation to ensure infor-\\nmative rationales (Paranjape et al., 2020), training\\nwith human-created rationales (Lehman et al.,\\n2019), and replacing stochastic variables with deter-\\nministic mappings (Guerreiro and Martins, 2021).\\nWe find that CREST-Rationalization, which is fully\\nunsupervised, outperforms standard rationalizers in\\nterms of model robustness and quality of rationales.\\n8 Conclusions\\nWe proposed CREST, a joint framework for selec-\\ntive rationalization and counterfactual text genera-\\ntion that is capable of producing valid, fluent, and\\ndiverse counterfactuals, while being flexible for\\ncontrolling the amount of perturbations. We have', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 8}),\n",
       " Document(page_content='tion that is capable of producing valid, fluent, and\\ndiverse counterfactuals, while being flexible for\\ncontrolling the amount of perturbations. We have\\nshown that counterfactuals can be successfully in-\\ncorporated into a rationalizer, either via counterfac-\\ntual data augmentation or agreement regularization,\\nto improve model robustness and rationale quality.\\nOur results demonstrate that CREST successfully\\nbridges the gap between selective rationales and\\ncounterfactual examples, addressing the limitations\\nof existing methods and providing a more compre-\\nhensive view of a model’s predictions.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 8}),\n",
       " Document(page_content='Limitations\\nOur work shows that CREST is a suitable frame-\\nwork for generating high-quality counterfactuals\\nand producing plausible rationales, and we hope\\nthat CREST motivates new research to develop\\nmore robust and interpretable models. We note,\\nhowever, two main limitations in our framework.\\nFirst, our counterfactuals are the result of a large\\nlanguage model (T5), and as such, they may carry\\nall the limitations within these models. Therefore,\\ncaution should be exercised when making state-\\nments about the quality of counterfactuals beyond\\nthe metrics reported in this paper, especially if\\nthese statements might have societal impacts. Sec-\\nond, CREST relies on a rationalizer to produce\\nhighlights-based explanations, and therefore it is\\nlimited in its ability to answer interpretability ques-\\ntions that go beyond the tokens of the factual or\\ncounterfactual input.\\nAcknowledgments\\nThis work was supported by the European Research\\nCouncil (ERC StG DeepSPIN 758969), by EU’s\\nHorizon Europe Research and Innovation Actions\\n(UTTER, contract 101070631), by P2020 project\\nMAIA (LISBOA-01-0247- FEDER045909), by\\nthe Portuguese Recovery and Resilience Plan\\nthrough project C645008882-00000055 (NextGe-\\nnAI, Center for Responsible AI), and by contract\\nUIDB/50008/2020. We are grateful to Duarte\\nAlves, Haau-Sing Lee, Taisiya Glushkova, and\\nHenrico Brum for the participation in human eval-\\nuation experiments.\\nReferences\\nJasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019.\\nInterpretable neural predictions with differentiable\\nbinary variables. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Lin-\\nguistics , pages 2963–2977, Florence, Italy. Associa-\\ntion for Computational Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nInProceedings of the 2015 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n632–642, Lisbon, Portugal. Association for Compu-\\ntational Linguistics.\\nOana-Maria Camburu, Tim Rocktäschel, Thomas\\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\\nral language inference with natural language explana-\\ntions. In Advances in Neural Information ProcessingSystems 31 , pages 9539–9549. Curran Associates,\\nInc.\\nHoward Chen, Jacqueline He, Karthik Narasimhan, and\\nDanqi Chen. 2022. Can rationalization improve ro-\\nbustness? In Proceedings of the 2022 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies , pages 3792–3805, Seattle, United States.\\nAssociation for Computational Linguistics.\\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\\nEric Lehman, Caiming Xiong, Richard Socher, and\\nByron C. Wallace. 2020. ERASER: A benchmark to\\nevaluate rationalized NLP models. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics , pages 4443–4458, Online.\\nAssociation for Computational Linguistics.\\nTanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi,\\nand Luke Zettlemoyer. 2022. CORE: A retrieve-then-\\nedit framework for counterfactual data generation.\\nInFindings of the Association for Computational\\nLinguistics: EMNLP 2022 , pages 2964–2984, Abu\\nDhabi, United Arab Emirates. Association for Com-\\nputational Linguistics.\\nFinale Doshi-Velez and Been Kim. 2017. Towards a\\nrigorous science of interpretable machine learning.\\narXiv preprint arXiv:1702.08608v2 .\\nMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan\\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\\nNitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,\\nDaniel Khashabi, Kevin Lin, Jiangming Liu, Nel-\\nson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer\\nSingh, Noah A. Smith, Sanjay Subramanian, Reut\\nTsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.\\n2020. Evaluating models’ local decision boundaries\\nvia contrast sets. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2020 , pages', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 9}),\n",
       " Document(page_content='2020. Evaluating models’ local decision boundaries\\nvia contrast sets. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2020 , pages\\n1307–1323, Online. Association for Computational\\nLinguistics.\\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\\n2018. Breaking NLI systems with sentences that\\nrequire simple lexical inferences. In Proceedings\\nof the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 2: Short Papers) ,\\npages 650–655, Melbourne, Australia. Association\\nfor Computational Linguistics.\\nNuno M. Guerreiro and André F. T. Martins. 2021.\\nSPECTRA: Sparse structured text rationalization.\\nInProceedings of the 2021 Conference on Empir-\\nical Methods in Natural Language Processing , pages\\n6534–6550, Online and Punta Cana, Dominican Re-\\npublic. Association for Computational Linguistics.\\nNitish Gupta, Sameer Singh, Matt Gardner, and Dan\\nRoth. 2021. Paired examples as indirect supervision\\nin latent decision models. In Proceedings of the\\n2021 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 5774–5785, Online and', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 9}),\n",
       " Document(page_content='Punta Cana, Dominican Republic. Association for\\nComputational Linguistics.\\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\\n2018. Annotation artifacts in natural language infer-\\nence data. In Proceedings of the 2018 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 2 (Short Papers) , pages 107–112,\\nNew Orleans, Louisiana. Association for Computa-\\ntional Linguistics.\\nPeter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal.\\n2020. Leakage-adjusted simulatability: Can models\\ngenerate non-trivial explanations of their behavior\\nin natural language? In Findings of the Association\\nfor Computational Linguistics: EMNLP 2020 , pages\\n4351–4367, Online. Association for Computational\\nLinguistics.\\nWilliam Huang, Haokun Liu, and Samuel R. Bowman.\\n2020. Counterfactually-augmented SNLI training\\ndata does not yield better generalization than unaug-\\nmented data. In Proceedings of the First Workshop on\\nInsights from Negative Results in NLP , pages 82–87,\\nOnline. Association for Computational Linguistics.\\nSarthak Jain, Sarah Wiegreffe, Yuval Pinter, and By-\\nron C. Wallace. 2020. Learning to faithfully ratio-\\nnalize by construction. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational\\nLinguistics , pages 4459–4473, Online. Association\\nfor Computational Linguistics.\\nNitish Joshi and He He. 2022. An investigation of the\\n(in)effectiveness of counterfactually augmented data.\\nInProceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers) , pages 3668–3681, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton.\\n2020. Learning the difference that makes a differ-\\nence with counterfactually-augmented data. In Inter-\\nnational Conference on Learning Representations .\\nDaniel Khashabi, Tushar Khot, and Ashish Sabharwal.\\n2020. More bang for your buck: Natural perturba-\\ntion for robust question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Natu-\\nral Language Processing (EMNLP) , pages 163–170,\\nOnline. Association for Computational Linguistics.\\nEric Lehman, Jay DeYoung, Regina Barzilay, and By-\\nron C. Wallace. 2019. Inferring which medical treat-\\nments work from reports of clinical trials. In Pro-\\nceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume\\n1 (Long and Short Papers) , pages 3705–3717, Min-\\nneapolis, Minnesota. Association for Computational\\nLinguistics.Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.\\nRationalizing neural predictions. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Nat-\\nural Language Processing , pages 107–117, Austin,\\nTexas. Association for Computational Linguistics.\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\\nweight decay regularization. In International Confer-\\nence on Learning Representations .\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\\nDan Huang, Andrew Y . Ng, and Christopher Potts.\\n2011. Learning word vectors for sentiment analysis.\\nInProceedings of the 49th Annual Meeting of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies , pages 142–150, Portland,\\nOregon, USA. Association for Computational Lin-\\nguistics.\\nNishtha Madaan, Inkit Padhi, Naveen Panwar, and Dip-\\ntikalyan Saha. 2021. Generate your counterfactuals:\\nTowards controlled counterfactual generation for text.\\nInProceedings of the AAAI Conference on Artificial\\nIntelligence , pages 13516–13524.\\nVlad Niculae, Andre Martins, Mathieu Blondel, and\\nClaire Cardie. 2018. Sparsemap: Differentiable\\nsparse structured inference. In International Con-\\nference on Machine Learning , pages 3799–3808.\\nPMLR.\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\\nJason Weston, and Douwe Kiela. 2020. Adversarial\\nNLI: A new benchmark for natural language under-', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 10}),\n",
       " Document(page_content='PMLR.\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\\nJason Weston, and Douwe Kiela. 2020. Adversarial\\nNLI: A new benchmark for natural language under-\\nstanding. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics ,\\npages 4885–4901, Online. Association for Computa-\\ntional Linguistics.\\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\\ning class relationships for sentiment categorization\\nwith respect to rating scales. In Proceedings of the\\n43rd Annual Meeting of the Association for Compu-\\ntational Linguistics (ACL’05) , pages 115–124, Ann\\nArbor, Michigan. Association for Computational Lin-\\nguistics.\\nBhargavi Paranjape, Mandar Joshi, John Thickstun,\\nHannaneh Hajishirzi, and Luke Zettlemoyer. 2020.\\nAn information bottleneck approach for controlling\\nconciseness in rationale extraction. In Proceedings\\nof the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP) , pages 1938–\\n1952, Online. Association for Computational Linguis-\\ntics.\\nBhargavi Paranjape, Matthew Lamm, and Ian Tenney.\\n2022. Retrieval-guided counterfactual generation\\nfor QA. In Proceedings of the 60th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1670–1686, Dublin,\\nIreland. Association for Computational Linguistics.\\nDanish Pruthi, Rachit Bansal, Bhuwan Dhingra,\\nLivio Baldini Soares, Michael Collins, Zachary C.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 10}),\n",
       " Document(page_content='Lipton, Graham Neubig, and William W. Cohen.\\n2022. Evaluating explanations: How much do ex-\\nplanations from the teacher aid students? Transac-\\ntions of the Association for Computational Linguis-\\ntics, 10:359–375.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, Ilya Sutskever, et al. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nblog, 1(8):9.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research ,\\n21(140):1–67.\\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\\n2019. Generating natural language adversarial exam-\\nples through probability weighted word saliency. In\\nProceedings of the 57th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 1085–\\n1097, Florence, Italy. Association for Computational\\nLinguistics.\\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\\nand Sameer Singh. 2020. Beyond accuracy: Be-\\nhavioral testing of NLP models with CheckList. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 4902–\\n4912, Online. Association for Computational Lin-\\nguistics.\\nMarcel Robeer, Floris Bex, and Ad Feelders. 2021. Gen-\\nerating realistic natural language counterfactuals. In\\nFindings of the Association for Computational Lin-\\nguistics: EMNLP 2021 , pages 3611–3625, Punta\\nCana, Dominican Republic. Association for Compu-\\ntational Linguistics.\\nAlexis Ross, Ana Marasovi ´c, and Matthew Peters. 2021.\\nExplaining NLP models via minimal contrastive edit-\\ning (MiCE). In Findings of the Association for Com-\\nputational Linguistics: ACL-IJCNLP 2021 , pages\\n3840–3852, Online. Association for Computational\\nLinguistics.\\nAlexis Ross, Tongshuang Wu, Hao Peng, Matthew Pe-\\nters, and Matt Gardner. 2022. Tailor: Generating and\\nperturbing text with semantic controls. In Proceed-\\nings of the 60th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Pa-\\npers) , pages 3194–3213, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D. Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models for\\nsemantic compositionality over a sentiment treebank.\\nInProceedings of the 2013 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n1631–1642, Seattle, Washington, USA. Association\\nfor Computational Linguistics.Damien Teney, Ehsan Abbasnedjad, and Anton van den\\nHengel. 2020. Learning what makes a difference\\nfrom counterfactual examples and gradient supervi-\\nsion. In Computer Vision–ECCV 2020: 16th Euro-\\npean Conference, Glasgow, UK, August 23–28, 2020,\\nProceedings, Part X 16 , pages 580–599. Springer.\\nMarcos Treviso and André F. T. Martins. 2020. The\\nexplanation game: Towards prediction explainability\\nthrough sparse communication. In Proceedings of the\\nThird BlackboxNLP Workshop on Analyzing and In-\\nterpreting Neural Networks for NLP , pages 107–118,\\nOnline. Association for Computational Linguistics.\\nSahil Verma, John Dickerson, and Keegan Hines. 2020.\\nCounterfactual explanations for machine learning: A\\nreview. arXiv preprint arXiv:2010.10596v3 .\\nAdina Williams, Nikita Nangia, and Samuel Bowman.\\n2018. A broad-coverage challenge corpus for sen-\\ntence understanding through inference. In Proceed-\\nings of the 2018 Conference of the North American\\nChapter of the Association for Computational Lin-\\nguistics: Human Language Technologies, Volume\\n1 (Long Papers) , pages 1112–1122, New Orleans,\\nLouisiana. Association for Computational Linguis-\\ntics.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\\nTeven Le Scao, Sylvain Gugger, Mariama Drame,', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 11}),\n",
       " Document(page_content='ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\\nformers: State-of-the-art natural language processing.\\nInProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations , pages 38–45, Online. Association\\nfor Computational Linguistics.\\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and\\nDaniel Weld. 2021. Polyjuice: Generating counter-\\nfactuals for explaining, evaluating, and improving\\nmodels. In Proceedings of the 59th Annual Meet-\\ning of the Association for Computational Linguistics\\nand the 11th International Joint Conference on Natu-\\nral Language Processing (Volume 1: Long Papers) ,\\npages 6707–6723, Online. Association for Computa-\\ntional Linguistics.\\nMo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola.\\n2019. Rethinking cooperative rationalization: In-\\ntrospective extraction and complement control. In\\nProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 4094–\\n4103, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\\nCharacter-level convolutional networks for text classi-\\nfication. Advances in neural information processing\\nsystems , 28.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 11}),\n",
       " Document(page_content='Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\\nbenchmarking platform for text generation models.\\nInThe 41st International ACM SIGIR Conference on\\nResearch & Development in Information Retrieval ,\\nSIGIR ’18, page 1097–1100, New York, NY , USA.\\nAssociation for Computing Machinery.\\nA Datasets\\nThe revised IMDB and SNLI datasets, which we\\nrefer to as rIMDB and rSNLI respectively, were cre-\\nated by Kaushik et al. (2020). They contain coun-\\nterfactuals consisting of revised versions made by\\nhumans on the Amazon’s Mechanical Turk crowd-\\nsourcing platform. For both datasets, the authors\\nensure that (a) the counterfactuals are valid; (b) the\\nedited reviews are coherent; and (c) the counter-\\nfactuals do not contain unnecessary modifications.\\nFor SNLI, counterfactuals were created either by\\nrevising the premise or the hypothesis. We refer to\\n(Kaushik et al., 2020) for more details on the data\\ngeneration process. Table 5 presents statistics for\\nthe datasets used for training models in this work.\\nTrain Val. Test\\nDataset docs tks docs tks docs tks\\nIMDB 22.5K 6M 2.5K 679K 25K 6M\\nrIMDB 3414 629K 490 92K 976 180K\\nSNLI 549K 12M 10K 232K 10K 231K\\nrSNLI 4165 188K 500 24K 1000 48K\\nTable 5: Datasets statistics.\\nAdditionally, we incorporate various contrastive\\nand out-of-domain datasets to evaluate our mod-\\nels. For IMDB, we use the contrast IMDB (Gardner\\net al., 2020), RottenTomatoes (Pang and Lee, 2005),\\nSST-2 (Socher et al., 2013), Amazon Polarity and\\nYelp (Zhang et al., 2015). For SNLI, we evalu-\\nate on the Hard SNLI (Gururangan et al., 2018),\\nbreak (Glockner et al., 2018), MultiNLI (Williams\\net al., 2018), and Adversarial NLI (Nie et al., 2020).\\nWe refer to the original works for more details.\\nB CREST Details\\nB.1 Masker\\nFor all datasets, the masker consists of a SPEC-\\nTRA rationalizer that uses a T5-small encoder as\\nthe backbone for the encoder and predictor (see\\n§2.1). Our implementation is derived directly from\\nits original source (Guerreiro and Martins, 2021).\\nWe set the maximum sequence length to 512, trun-\\ncating inputs when necessary. We employ a con-tiguity penalty of 10−4for IMDB and 10−2for\\nSNLI. We train all models for a minimum of 3\\nepochs and maximum of 15 epochs along with\\nearly stopping with a patience of 5 epochs. We\\nuse AdamW (Loshchilov and Hutter, 2019) with a\\nlearning rate of 10−4and weight decay of 10−6.\\nB.2 Editor\\nFor all datasets, CREST and MiCE editors con-\\nsist of a full T5-small model (Raffel et al., 2020),\\nwhich includes both the encoder and the decoder\\nmodules. We use the T5 implementation available\\nin the transformers library (Wolf et al., 2020) for\\nour editor. We train all models for a minimum of\\n3 epochs and maximum of 20 epochs along with\\nearly stopping with a patience of 5 epochs. We\\nuse AdamW (Loshchilov and Hutter, 2019) with\\na learning rate of 10−4and weight decay of 10−6.\\nFor both CREST and MiCE, we generate counter-\\nfactuals with beam search with a beam of size 15\\nand disabling bigram repetitions. We post-process\\nthe output of the editor to trim spaces and repeti-\\ntions of special symbols (e.g., </s> ).\\nB.3 SPECTRA rationalizers\\nAll of our SPECTRA rationalizers share the same\\nsetup and training hyperparameters as the one used\\nby the masker in §4, but were trained with distinct\\nrandom seeds. We tuned the counterfactual loss\\nweight αwithin {1.0,0.1,0.01,0.001,0.0001},\\nandλwithin {1.0,0.1,0.01,0.001}for models\\ntrained with agreement rationalization. More\\nspecifically, we performed hyperparameter tuning\\non the validation set, with the goal of maximiz-\\ning in-domain accuracy. As a result, we obtained\\nα= 0.01andλ= 0.001for IMDB, and α= 0.01\\nandλ= 0.1for SNLI.\\nC Validity vs. Closeness\\nTo better assess the performance of CREST and\\nMiCE by varying closeness, we plot in Figure 5\\nbinned-validity scores of CREST and MiCE with\\n30% masking on the revised SNLI dataset. Al-\\nthough CREST is deemed less valid than MiCE\\noverall ( cf.Table 1), we note that CREST gener-', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 12}),\n",
       " Document(page_content='binned-validity scores of CREST and MiCE with\\n30% masking on the revised SNLI dataset. Al-\\nthough CREST is deemed less valid than MiCE\\noverall ( cf.Table 1), we note that CREST gener-\\nates more valid counterfactuals in lower minimality\\nranges. This provides further evidence that CREST\\nremains superior to MiCE on closeness intervals of\\nparticular interest for generating counterfactuals in\\nan automatic way.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 12}),\n",
       " Document(page_content='[0.0, 0.125] (0.125, 0.25] (0.25, 0.375] (0.375, 0.5] (0.5, 0.625]\\nCloseness0.00.20.40.60.81.0ValidityCREST\\nMiCEFigure 5: Validity by binned closeness ranges for MiCE\\n(30% masking) and CREST (30% masking). At lower\\ncloseness ranges, CREST produces more valid counter-\\nfactuals than does MiCE.\\nD Human Annotation\\nThe annotation task was conducted by four distinct\\nindividuals, all of whom are English-fluent PhD stu-\\ndents. Two annotators were employed for IMDB\\nand two for SNLI. The annotators were not given\\nany information regarding the methods used to cre-\\nate each counterfactual, and the documents were\\npresented in a random order to maintain source\\nanonymity. The annotators were presented with\\nthe reference text and its corresponding gold label.\\nSubsequently, for each method, they were asked\\nto assess both the validity and the naturalness of\\nthe resulting counterfactuals using a 5-point Lik-\\nert scale. We provided a guide page to calibrate\\nthe annotators’ understating of validity and natu-\\nralness prior the annotation process. We presented\\nhypothetical examples with different levels of va-\\nlidity and naturalness and provided the following\\ninstructions regarding both aspects:\\n•“If every phrase in the text unequivocally sug-\\ngests a counterfactual label, the example is\\ndeemed fully valid and should receive a top score\\nof 5/5.”\\n•“If the counterfactual text aligns with the style,\\ntone, and grammar of real-world examples, it’s\\nconsidered highly natural and deserves a score of\\n5/5.“\\nWe measure inter-annotator agreement with a\\nnormalized and inverted Mean Absolute Difference\\n(MAD), which computes a “soft” accuracy by aver-\\naging absolute difference ratings and normalizing\\nthem to a 0-1 range. We present the annotation\\nresults in Table 6. Our results show that humans\\nagreed more on manual examples than on auto-\\nmatic approaches. On the other hand, for SNLI,IMDB SNLI\\nMethod v n r o v n r o\\nManual 4.60 4.36 0.83 4.89 4.90 0.95\\nMiCE 2.76 2.29 0.71 4.35 4.71 0.94\\nCREST 4.06 3.44 0.76 4.89 4.89 0.96\\nOverall 3.81 3.36 0.77 4.71 4.83 0.95\\nTable 6: Annotation statistics. vandnrepresent the\\naveraged validity and naturalness scores, whereas ro\\nis the relative observed agreement computed with a\\nnormalized and inverted MAD.\\nSetup Data size RotTom SST-2 Amazon Yelp\\nF 100% 76.5 ±1.679.8±1.686.0±0.788.5±0.7\\nWith data augmentation:\\nF+CH +8% 76.6 ±1.580.7±1.386.3±1.089.1±1.2\\nF+CS,V +1% 77.2 ±1.180.5±0.586.1±0.288.8±0.3\\nF+CS,V +2% 76.2 ±1.280.8±0.886.7±0.589.6±0.5\\nF+CS,V +4% 77.7±0.880.8±0.787.0±0.689.8±0.6\\nF+CS,V +8% 76.6 ±2.280.2±1.786.1±0.988.2±1.0\\nF+CS,V +85% 76.8 ±0.979.3±0.385.2±0.988.0±1.0\\nF+CS +100% 76.7 ±1.080.6±0.686.4±0.689.1±0.5\\nWith agreement regularization:\\nF&CS,V 85% 76.3 ±1.480.2±1.386.3±0.788.9±0.7\\nF&CS 100% 77.3 ±2.381.1±2.486.8±0.889.3±0.7\\nTable 7: OOD accuracy of SPECTRA rationalizers with\\ndifferent portions of augmented counterfactuals. Bold :\\ntop results; underlined : second-best.\\nannotators assigned similar scores across all meth-\\nods. In terms of overall metrics, including validity,\\nnaturalness, and agreement, the scores were lower\\nfor IMDB than for SNLI, highlighting the difficulty\\nassociated with the generation of counterfactuals\\nfor long movie reviews.\\nAnnotation interface. Figure 6 shows a snap-\\nshot of the interface used for the annotation, which\\nis publicly available at https://www.github.com/\\nmtreviso/TextRankerJS .\\nE Counterfactual Data Augmentation\\nAnalysis\\nPrevious studies on counterfactual data augmen-\\ntation have found that model performance highly\\ndepends on the number and diversity of augmented\\nsamples (Huang et al., 2020; Joshi and He, 2022).\\nTo account for this, we investigate the effect of\\nadding increasingly larger portions of CREST\\ncounterfactuals for data augmentation on the IMDB\\ndataset. Our findings are summarized in Table 7.\\nDiscussion. We find that incorporating human-\\ncrafted counterfactuals ( F+CH) improves SPEC-\\nTRA performance on all OOD datasets. On top', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 13}),\n",
       " Document(page_content='Figure 6: Snapshot of the annotation interface.\\nof that, we note that using a small proportion\\n(4% of the full IMDB) of valid CREST coun-\\nterfactuals ( F+CS,V) through data augmenta-\\ntion also leads to improvements on all datasets\\nand outweighs the benefits of manual counterfac-\\ntuals. This finding confirms that, as found by\\nPolyJuice (Wu et al., 2021), synthetic counterfac-\\ntuals can improve model robustness. Conversely,\\nas the number of augmented counterfactuals in-\\ncreases ( 85%), the performance on OOD datasets\\nstarts to decrease, which is also consistent with\\nthe findings of Huang et al. (2020). When aug-\\nmenting the entire training set we obtain an in-\\ncrease of accuracy, suggesting that the counterfac-\\ntual loss weight ( α) was properly adjusted on the\\nvalidation set. Finally, we observe that while ap-\\nplying CREST-Rationalization only on valid ex-\\namples ( F&CS,V) degrades performance, apply-\\ning CREST-Rationalization on all paired examples\\n(F&CS) maintains a high accuracy on OOD\\ndatasets and concurrently leads to strong results\\non in-domain and contrast datasets (see Table 2).\\nF Computing infrastructure\\nOur infrastructure consists of four machines with\\nthe specifications shown in Table 8. The machines\\nwere used interchangeably and all experiments\\nwere carried in a single GPU.\\nGPU CPU\\n4×Titan Xp - 12GB 16 ×AMD Ryzen - 128GB\\n4×GTX 1080Ti - 12GB 8 ×Intel i7 - 128GB\\n3×RTX 2080Ti - 12GB 12 ×AMD Ryzen - 128GB\\n3×RTX 2080Ti - 12GB 12 ×AMD Ryzen - 128GB\\nTable 8: Computing infrastructure.G Examples of Counterfactuals\\nTable 9 shows examples of counterfactuals pro-\\nduced by MiCE and CREST with 30% masking.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 14}),\n",
       " Document(page_content='Sentiment Classification:\\nInput: If you haven’t seen this, it’s terrible. It is pure trash. I saw this about 17 years ago, and I’m still screwed up from it.\\nMiCE: If you haven’t seen this, it’s a great movie. I saw this about 17 years ago, and I’m still screwed up from it.\\nCREST: If you haven’t seen this movie, it’s worth seeing. It’s very funny. I saw it about 17 years ago, and I’m still screwed up from it.\\nInput:Touching; Well directed autobiography of a talented young director/producer. A love story with Rabin’s assassination in the\\nbackground. Worth seeing !\\nMiCE: Watching abiography of a very young writer/producer. A great story of Rabin’s assassination in the background! Worth seeing!!\\nCREST: This is the worst film of a talented young director/producer. And Rabin’s assassination in the background is even worse!\\nInput:A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me\\ngo out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the\\nconversation while they waited for Robbins to retrieve the birdie.\\nMiCE:This is an unremarkable, if unremarkable .hau, as steinstein, is the worst part of the movie, and the only thing that would make me\\ngo out of my way to see this again is the physicists /mitt ists and the wait wait till they waited for binbins to re-release.\\nCREST:This is a very unremarkable and unwatchable film. The acting is unhau, the plot, and the acting. My favorite thing about this film,\\nand the only thing that made me go out of my mind, was the ending with the physic nerves of Symitton, I watched the zombies\\nand thought they waited for Robbins to retrieve the junkie.\\nInput:I saw this film earlier today, and I was amazed at how accurate the dialog is for the main characters. It didn’t feel like a film - it\\nfelt more like a documentary (the part I liked best). The leading ladies in this film seemed as real to me as any fifteen year-old girls\\nI know. All in all, a very enjoyable film for those who enjoy independent films.\\nMiCE:I saw this film earlier today, and I was amazed at how bad the film is for the sake of a film - it feels more like thanthe part I played\\nin this film. To me - fifteen year-old s I don’t know. All in all this is a bad film for those who like independent films :\\nCREST:I saw this movie earlier today, and I was surprised at how bad it is for the first time. It’s not a good movie - it’s just a bad movie\\n(the only thing I can say about it). The acting is awful to me as any fifteen year-old as I can. All in all, the movie is a waste of time\\nfor me.\\nNatural Language Inference:\\nPrem: A large group of people walking in a busy city at night.\\nHyp: People are outside in a park.\\nMiCE: People are walking in a city at night\\nCREST: People walking in a city.\\nPrem: Players from two opposing teams wearing colorful cleats struggle to gain control over a ball on an AstroTurf field.\\nHyp: The players are playing a sport.\\nMiCE: The players are playing chess at home\\nCREST: The players are sitting on a couch.\\nPrem: A woman is in the middle of hitting a tennis ball.\\nHyp: A woman is playing tennis.\\nMiCE: A woman is playing basketball at home\\nCREST: A woman is playing basketball.\\nPrem: Two boys with blond-hair, wearing striped shirts on a bed.\\nHyp: Children playing in the park.\\nMiCE: Children are on the bed.\\nCREST: Boys are on the bed.\\nPrem: Bubbles surround a statue in the middle of a street.\\nHyp: There are bubbles around the statue.\\nMiCE: There are bubbles surround the statue.\\nCREST: Bubbles are in the ocean.\\nPrem: A young girl is standing in a kitchen holding a green bib.\\nHyp: A boy is playing with a firetruck.\\nMiCE: A child is in a fire place\\nCREST: A girl is holding a bib.\\nTable 9: Examples of original inputs from the IMDB and SNLI datasets followed by synthetic counterfactuals\\nproduced by MiCE and CREST with 30% masking.', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 15}),\n",
       " Document(page_content='EfficientViT-SAM: Accelerated Segment Anything Model\\nWithout Performance Loss\\nZhuoyang Zhang1,3∗, Han Cai2,3∗, Song Han2,3\\n1Tsinghua University,2MIT,3NVIDIA\\nhttps://github.com/mit-han-lab/efficientvit\\nAbstract\\nWe present EfficientViT-SAM, a new family of acceler-\\nated segment anything models. We retain SAM’s lightweight\\nprompt encoder and mask decoder while replacing the\\nheavy image encoder with EfficientViT. For the training,\\nwe begin with the knowledge distillation from the SAM-ViT-\\nH image encoder to EfficientViT. Subsequently, we conduct\\nend-to-end training on the SA-1B dataset. Benefiting from\\nEfficientViT’s efficiency and capacity, EfficientViT-SAM de-\\nlivers 48.9 ×measured TensorRT speedup on A100 GPU\\nover SAM-ViT-H without sacrificing performance. Our\\ncode and pre-trained models are released at https://\\ngithub.com/mit-han-lab/efficientvit .\\n1. Introduction\\nSegment Anything Model (SAM) [1] is a family of\\nimage segmentation models pretrained on a high-quality\\ndataset with 11M images and 1B masks. SAM provides\\nastounding zero-shot image segmentation performance and\\nhas many applications, including AR/VR, data annotation,\\ninteractive image editing, etc.\\nDespite the strong performance, SAM is highly compu-\\ntation intensive, restricting its applicability in time-sensitive\\nscenarios. In particular, SAM’s main computation bottle-\\nneck is its image encoder, which requires 2973 GMACs per\\nimage at the inference time.\\nTo accelerate SAM, numerous efforts have been made to\\nreplace SAM’s image encoder with lightweight models. For\\nexample, MobileSAM [2] distills the knowledge of SAM’s\\nViT-H model into a tiny vision transformer. EdgeSAM [3]\\ntrains a purely CNN-based model to mimic ViT-H, em-\\nploying a meticulous distillation strategy with the prompt\\nencoder and mask decoder involved in the process. Effi-\\ncientSAM [4] leverages the MAE pretraining method to im-\\nprove performance.\\nWhile these methods can reduce the computation cost,\\n∗Work done during an internship at NVIDIA.\\nZero-Shot COCO mAP3740434649\\nA100 GPU TRT FP16 Throughput (image/s)0200400600800\\n47.8\\n47.5\\n46.6\\n46.2\\n45.7\\n44.4\\n42.1\\n38.7\\n46.5Table 2AprilSAMMobileSAMEdgeSAMEﬃcientSAMEﬃcientViT-SAM-LEﬃcientViT-SAM-XLApril27838.744942.1June18344.4July76245.7August63846.2September53846.6October294November1146.5Untitled 1Untitled 227847.5Untitled 318247.8SAM-ViT-HEfﬁcientSAMEdgeSAMMobileSAMEfﬁcientViT-SAM-LEfﬁcientViT-SAM-XL16.5x faster\\nInputConvResBlock3 x 1024 x 1024Input StemStage1×L4MBConvStage 4EfﬁcientViT Module×L5MBConvEfﬁcientViT Module\\n4x up2x up×L6F-MBConv×L1F-MBConv×L2F-MBConv×L3Stage2Stage3Stage 5P5P4P3\\nF-MBConv\\nSAM Head×L048.9x faster\\n1Figure 1. Throughput vs. COCO Zero-Shot Instance Seg-\\nmentation mAP. EfficientViT-SAM is the first accelerated SAM\\nmodel that matches/outperforms SAM-ViT-H’s [1] zero-shot per-\\nformance, delivering the SOTA performance-efficiency trade-off.\\nthey all suffer from significant performance drops (Fig-\\nure 1). This work introduces EfficientViT-SAM to ad-\\ndress this limitation by leveraging EfficientViT [7] to re-\\nplace SAM’s image encoder. Meanwhile, we retain the\\nlightweight prompt encoder and mask decoder architecture\\nfrom SAM. Our training process consists of two phases.\\nFirst, we train the image encoder of EfficientViT-SAM us-\\ning SAM’s image encoder as the teacher. Second, we\\ntrain EfficientViT-SAM end-to-end using the whole SA-1B\\ndataset [1].\\nWe thoroughly evaluate EfficientViT-SAM on a se-\\nries of zero-shot benchmarks. EfficientViT-SAM provides\\na significant performance/efficiency boost over all prior\\nSAM models. In particular, on the COCO dataset [8],\\nEfficientViT-SAM achieves 48.9 ×higher throughput on\\nA100 GPU without mAP drop compared with SAM-ViT-H\\n[1].\\n1arXiv:2402.05008v1  [cs.CV]  7 Feb 2024', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 0}),\n",
       " Document(page_content='Zero-Shot COCO mAP3740434649\\nA100 GPU TRT FP16 Throughput (image/s)0200400600800\\n47.8\\n47.5\\n46.6\\n46.2\\n45.7\\n44.4\\n42.1\\n38.7\\n46.5Table 2AprilSAMMobileSAMEdgeSAMEﬃcientSAMEﬃcientViT-SAM-LEﬃcientViT-SAM-XLApril27838.744942.1June18344.4July76245.7August63846.2September53846.6October294November1146.5Untitled 1Untitled 227847.5Untitled 318247.8SAM-ViT-HEfﬁcientSAMEdgeSAMMobileSAMEfﬁcientViT-SAM-LEfﬁcientViT-SAM-XL16.5x faster\\nInputConvResBlock3 x 1024 x 1024Input StemStage1×L4MBConvStage 4EfﬁcientViT Module×L5MBConvEfﬁcientViT Module\\n4x up2x up×L6F-MBConv×L1F-MBConv×L2F-MBConv×L3Stage2Stage3Stage 5P5P4P3\\nF-MBConv\\nSAM Head×L0\\n1Figure 2. Macro Architecture of EfficientViT-SAM-XL. ‘ResBlock’ refers to the basic building block from ResNet34 [5]. ‘F-MBConv’\\nrefers to the fused MBConv block from [6]. ‘EfficientViT Module’ is the building block from [7].\\n2. Related Work\\n2.1. Segment Anything Model\\nSAM [1] has gained widespread recognition as a mile-\\nstone in the field of computer vision, showcasing its excep-\\ntional performance and generalization in image segmenta-\\ntion. SAM defines image segmentation as a promptable\\ntask, that aims to generate a valid segmentation mask given\\nany segmentation prompt. To achieve this objective, SAM\\nutilizes an image encoder and a prompt encoder to process\\nthe image and provide prompts. The outputs from both en-\\ncoders are then fed into a mask decoder, which generates\\nthe final mask prediction. SAM is trained on a large-scale\\nsegmentation dataset comprising over 11 million images\\nwith more than 1 billion high-quality masks, enabling ro-\\nbust zero-shot open-world segmentation. SAM has shown\\nits high versatility in a wide range of downstream appli-\\ncations, including image in-painting [9], object tracking\\n[10, 11], and 3D generation [12, 13]. Nevertheless, the im-\\nage encoder component of SAM imposes significant com-\\nputational costs, leading to high latency that restricts its\\npracticality in time-sensitive scenarios. Recent works [2–\\n4, 14] have been focused on improving the efficiency of\\nSAM, aiming to address its computational limitations.\\n2.2. Efficient Deep Learning Computing\\nImproving the efficiency of deep neural networks is criti-\\ncal when deploying them in real-world applications on both\\nedge and cloud platforms. Our work is related to effi-\\ncient model architecture design [15, 16] that aims to im-\\nprove the performance-efficiency trade-off by replacing in-\\nefficient model architectures with efficient ones. Our work\\nis also related to knowledge distillation [17] that uses pre-\\ntrained teacher models to guide the training of student mod-\\nels. Additionally, we can combine EfficientViT-SAM with\\nother parallel techniques to further boost efficiency, includ-\\ning pruning [18], quantization [19], and hardware-aware\\nneural architecture search [20].3. Method\\nWe propose EfficientViT-SAM, which harnesses Effi-\\ncientViT [7] to accelerate the segment anything model. In\\nparticular, our approach preserves the prompt encoder and\\nmask decoder architecture from SAM while replacing the\\nimage encoder with EfficientViT. We design two series of\\nmodels, EfficientViT-SAM-L and EfficientViT-SAM-XL,\\noffering a balanced trade-off between speed and perfor-\\nmance. Subsequently, we train EfficientViT-SAM using the\\nSA-1B dataset in an end-to-end fashion.\\n3.1. EfficientViT\\nEfficientViT [7] is a family of vision transformer models\\nfor efficient high-resolution dense prediction. Its core build-\\ning block is a multi-scale linear attention module that en-\\nables the global receptive field and multi-scale learning with\\nhardware-efficient operations. Specifically, it substitutes the\\ninefficient softmax attention with lightweight ReLU linear\\nattention to have the global receptive field. By leveraging\\nthe associative property of matrix multiplication, ReLU lin-\\near attention can reduce the computational complexity from\\nquadratic to linear while preserving functionality. In addi-\\ntion, it enhances the ReLU linear attention with convolution', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 1}),\n",
       " Document(page_content='ear attention can reduce the computational complexity from\\nquadratic to linear while preserving functionality. In addi-\\ntion, it enhances the ReLU linear attention with convolution\\nto mitigate its limitation in local feature extraction. More\\ndetails are available in the original paper [7].\\n3.2. EfficientViT-SAM\\nModel Architecture. The macro architecture of\\nEfficientViT-SAM-XL is demonstrated in Figure 2.\\nIts backbone consists of five stages. Similar to EfficientViT\\n[7], we use convolution blocks in the early stages while\\nusing EfficientViT modules in the last two stages. We\\nfuse the features from the last three stages by upsampling\\nand addition. The fused feature is fed to the neck com-\\nprising several fused MBConv blocks and then fed to the\\nSAM head. For more details, please refer to our GitHub\\nrepository.\\nTraining. To initialize the image encoder, we begin by\\ndistilling the image embedding of SAM-ViT-H into Effi-\\n2', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 1}),\n",
       " Document(page_content='#Params(M) #MACs(G) Throughput (image/s) COCO mAP\\nSAM-ViT-H [1] 641.1 2973 11 46.5\\nMobileSAM [2] 9.8 39 278 38.7\\nEdgeSAM [3] 9.6 20 449 42.1\\nEfficientSAM [4] 25.3 247 183 44.4\\nEfficientViT-SAM-L0 34.8 35 762 45.7\\nEfficientViT-SAM-L1 47.7 49 638 46.2\\nEfficientViT-SAM-L2 61.3 69 538 46.6\\nEfficientViT-SAM-XL0 117.0 185 278 47.5\\nEfficientViT-SAM-XL1 203.3 322 182 47.8\\nTable 1. Runtime Efficiency Comparison . We benchmark the throughput on a single NVIDIA A100 GPU with TensorRT, fp16.\\ncientViT. We employ the L2 loss as the loss function. For\\nthe prompt encoder and mask decoder, we initialize them\\nby loading the weights from SAM-ViT-H. Then, we train\\nEfficientViT-SAM on the SA-1B dataset in an end-to-end\\nmanner.\\nIn the end-to-end training phase, we randomly choose\\nbetween the box prompt and the point prompt with equal\\nprobability. In the case of the point prompt, we randomly\\nselect 1-10 foreground points from the ground-truth mask\\nto ensure our model performs effectively for various point\\nconfigurations. In the case of the box prompt, we utilize the\\nground-truth bounding box. We resize the longest side to\\n512/1024 for EfficientViT-SAM-L/XL models and pad the\\nshorter side accordingly. We select up to 64 randomly sam-\\npled masks per image. To supervise the training process,\\nwe use a linear combination of focal loss and dice loss,\\nwith a 20:1 ratio of focal loss to dice loss. Similar to the\\napproach taken in SAM to mitigate ambiguity, we predict\\nthree masks simultaneously and only back-propagate the\\nlowest loss. We also support single mask output by adding a\\nfourth output token. During training, we randomly alternate\\nbetween the two prediction modes.\\nWe train EfficientViT-SAM on the SA-1B dataset for 2\\nepochs, utilizing a batch size of 256. The AdamW optimizer\\nis employed with a momentum of β1= 0.9 and β2= 0.999.\\nThe initial learning rate is set to 2e−6/1e−6for EfficientViT-\\nSAM-L/XL, which is decayed to 0 using a cosine decay\\nlearning rate schedule. Regarding data augmentation, we\\nuse the random horizontal flip.\\n4. Experiment\\nIn this section, we begin by conducting a comprehensive\\nanalysis of the runtime efficiency of EfficientViT-SAM in\\nSection 4.1. Subsequently, we evaluate the zero-shot ca-\\npability of EfficientViT-SAM on the COCO [8] and LVIS\\n[21] datasets, which were not encountered during the train-\\ning process. Two distinct tasks are performed: single point\\nvalid mask evaluation in Section 4.2 and box-prompted in-COCO LVIS\\n1 click 3 click 5 click 1 click 3 click 5 click\\nSAM-ViT-H [1] 58.4 69.6 71.4 59.2 66.0 66.8\\nEfficientViT-SAM-XL1 59.8 71.3 75.3 56.6 67.0 71.7\\nTable 2. Zero-Shot Point-Prompted Segmentation Results.\\nstance segmentation in Section 4.3. These tasks individu-\\nally assess the effectiveness of the point prompt and box\\nprompt features of EfficientViT-SAM. We also provide re-\\nsults on SGinW benchmark in Section 4.4.\\n4.1. Runtime Efficiency\\nWe compare the model parameters, MACs, and through-\\nput of EfficientViT-SAM with SAM and other acceleration\\nworks. Results are shown in Table 1. We conduct the\\nthroughput measurements on a single NVIDIA A100 GPU\\nwith TensorRT optimization. Our results show that com-\\npared to SAM, we achieve an impressive acceleration of 17\\nto 69 times. Furthermore, despite having more parameters\\nthan other acceleration works, EfficientViT-SAM demon-\\nstrates significantly higher throughput due to its effective\\nutilization of hardware-friendly operators.\\n4.2. Zero-Shot Point-Prompted Segmentation\\nWe assess the zero-shot performance of EfficientViT-\\nSAM in segmenting objects based on point prompts in Ta-\\nble 2. We adopt the point selection method described in\\n[1]. That is the initial point is selected as the point located\\nfarthest from the object boundary. Each subsequent point\\nis chosen as the farthest point from the boundary of the er-\\nror region, which is defined as the area between the ground\\ntruth and the previous prediction. The performance is re-', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 2}),\n",
       " Document(page_content='is chosen as the farthest point from the boundary of the er-\\nror region, which is defined as the area between the ground\\ntruth and the previous prediction. The performance is re-\\nported using 1/3/5 clicks on COCO and LVIS dataset, with\\nthe mIoU (mean Intersection over Union) serving as the\\nmetric. Our results demonstrate superior performance com-\\npared to SAM, particularly when additional point prompts\\n3', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 2}),\n",
       " Document(page_content='mAP\\nAirplane-Parts\\nBottles\\nBrain-Tumor\\nChicken\\nCows\\nElectric-Shaver\\nElephants\\nFruits\\nGarbage\\nGinger-Garlic\\nHand-Metal\\nHand\\nHouse-Parts\\nHouseHold-Items\\nNutterfly-Squireel\\nPhones\\nPoles\\nPuppies\\nRail\\nSalmon-Fillet\\nStrawberry\\nTablets\\nToolkits\\nTrash\\nWatermelon\\nSAM-ViT-H [1] 48.7 37.2 65.4 11.9 84.5 47.5 71.7 77.9 82.3 24.0 45.8 81.2 70.0 8.4 60.1 71.3 35.4 23.3 50.1 8.7 32.9 83.5 29.8 20.8 30.0 64.2\\nEfficientViT-SAM-XL1 48.9 37.5 66.3 12.1 85.1 46.9 70.4 75.6 82.5 23.9 45.7 74.8 79.6 8.4 60.1 70.9 35.1 20.0 50.1 7.4 40.0 83.3 29.3 20.7 29.4 67.7\\nTable 3. Zero-Shot Instance Segmentation Results on Segment in the Wild Benchmark.\\nCOCO LVIS\\nmIoU mIoUSmIoUMmIoULmIoU mIoUSmIoUMmIoUL\\nSAM-ViT-H [1] 77.4 72.3 80.4 81.8 77.0 70.6 87.5 89.9\\nEfficientViT-SAM-XL1 79.9 75.8 82.2 83.8 79.9 74.4 88.4 91.6\\nTable 4. Zero-Shot Instance Segmentation Results, Prompted\\nwith Ground Truth Bounding Box.\\nCOCO LVIS\\nmAP APSAPMAPLmAP APSAPMAPL\\nSAM-ViT-H [1] 46.5 30.8 51.0 61.7 44.2 31.8 57.1 65.3\\nMobileSAM [2] 38.7 23.7 42.2 54.3 37.0 24.7 47.8 59.1\\nEdgeSAM [3] 42.1 26.6 46.7 56.9 39.8 28.6 51.3 59.3\\nEfficientSAM [4] 44.4 28.4 48.3 60.1 41.5 29.7 53.4 62.2\\nEfficientViT-SAM-L0 45.7 28.2 49.5 63.4 41.8 28.8 53.4 64.7\\nEfficientViT-SAM-L1 46.2 28.7 50.4 64.0 42.1 29.1 54.3 65.0\\nEfficientViT-SAM-L2 46.6 28.9 50.8 64.2 42.7 29.4 55.1 65.5\\nEfficientViT-SAM-XL0 47.5 30.0 51.5 64.6 43.9 31.2 56.2 65.9\\nEfficientViT-SAM-XL1 47.8 30.5 51.8 64.7 44.4 31.6 57.0 66.4\\nTable 5. Zero-Shot Instance Segmentation Results, Prompted\\nwith ViTDet Boxes.\\nare provided.\\n4.3. Zero-Shot Box-Prompted Segmentation\\nWe evaluate the zero-shot performance of EfficientViT-\\nSAM in object segmentation using bounding boxes. We first\\ninput ground truth bounding boxes to the model, and the\\nresults are presented in Table 4. The mIoU(mean Intersec-\\ntion over Union) is reported for all objects, as well as sep-\\narately for small, medium, and large objects. Our approach\\nsurpasses SAM by a significant margin on the COCO and\\nLVIS dataset. Next, we employ an object detector, ViT-\\nDet [22], and utilize its output boxes as prompts for the\\nmodel. The results in Table 5 demonstrate that EfficientViT-\\nSAM achieves superior performance compared to SAM.\\nNotably, even the lightest version of EfficientViT-SAM sig-\\nnificantly outperforms other acceleration works by a large\\nmargin.\\nAdditionally, we evaluate the performance of\\nEfficientViT-SAM on the COCO dataset using YOLOv8\\nand GroundingDINO [23] as the object detectors. YOLOv8YOLOv8 GroundingDINO\\nAP APSAPMAPLAP APSAPMAPL\\nSAM-ViT-H [1] 43.8 26.1 48.1 60.4 46.9 31.5 51.8 64.4\\nEfficientViT-SAM-XL1 44.7 26.0 48.9 62.9 48.2 31.5 52.6 67.3\\nTable 6. Zero-Shot Instance Segmentation Results on COCO,\\nPrompted with YOLOv8/GroundingDINO Boxes.\\nis a real-time object detector suitable for real-world appli-\\ncations. On the other hand, GroundingDINO is capable of\\ndetecting objects using text prompts, allowing us to perform\\nobject segmentation based on textual cues. The results\\npresented in Table 6 reveal the outstanding performance of\\nEfficientViT-SAM in comparison to SAM.\\n4.4. Zero-Shot In-the-Wild Segmentation\\nThe Segmentation in the Wild benchmark consists of\\n25 zero-shot in-the-wild segmentation datasets. We equip\\nEfficientViT-SAM with Grounding-DINO as box prompts\\nand perform zero-shot segmentation. The comprehensive\\nperformance results for each dataset are presented in Table\\n3. SAM achieves an mAP of 48.7, whereas EfficientViT-\\nSAM achieves a higher score of 48.9.\\n4.5. Qualitative Results.\\nFigure 3 showcases the qualitative segmentation results\\nof EfficientViT-SAM when provided with point prompt,\\nbox prompt, and segment-everything mode. The results\\ndemonstrate that EfficientViT-SAM excels not only in seg-\\nmenting large objects but also in effectively handling small\\nobjects. These findings highlight the superior segmentation\\ncapability of EfficientViT-SAM.\\n5. Conclusion\\nIn this work, we introduced EfficientViT-SAM, which\\nutilizes EfficientViT to replace the image encoder of SAM.', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 3}),\n",
       " Document(page_content='capability of EfficientViT-SAM.\\n5. Conclusion\\nIn this work, we introduced EfficientViT-SAM, which\\nutilizes EfficientViT to replace the image encoder of SAM.\\nEfficientViT-SAM achieved a significant efficiency boost\\nover SAM without sacrificing performance across various\\nzero-shot segmentation tasks. We have open-souced our\\npretrained models on GitHub to the community.\\n4', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 3. Qualitative Segmentation Results of EfficientViT-SAM under point, box, and everything mode.\\nAcknowledgments\\nWe thank National Science Foundation for supporting\\nthis research.\\nReferences\\n[1] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. arXiv preprint arXiv:2304.02643 , 2023. 1, 2, 3, 4\\n[2] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,\\nSung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.\\nFaster segment anything: Towards lightweight sam for mo-\\nbile applications. arXiv preprint arXiv:2306.14289 , 2023. 1,\\n2, 3, 4\\n[3] Chong Zhou, Xiangtai Li, Chen Change Loy, and Bo Dai.\\nEdgesam: Prompt-in-the-loop distillation for on-device de-\\nployment of sam. arXiv preprint arXiv:2312.06660 , 2023.\\n1, 3, 4\\n[4] Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xi-\\nang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang,\\nFei Sun, Forrest Iandola, et al. Efficientsam: Leveragedmasked image pretraining for efficient segment anything.\\narXiv preprint arXiv:2312.00863 , 2023. 1, 2, 3, 4\\n[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 770–778, 2016. 2\\n[6] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models\\nand faster training. In International conference on machine\\nlearning , pages 10096–10106. PMLR, 2021. 2\\n[7] Han Cai, Chuang Gan, and Song Han. Efficientvit: Enhanced\\nlinear attention for high-resolution low-computation visual\\nrecognition. arXiv preprint arXiv:2205.14756 , 2022. 1, 2\\n[8] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nComputer Vision–ECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings,\\nPart V 13 , pages 740–755. Springer, 2014. 1, 3\\n[9] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin\\nJin, Wenjun Zeng, and Zhibo Chen. Inpaint anything:\\nSegment anything meets image inpainting. arXiv preprint\\narXiv:2304.06790 , 2023. 2\\n[10] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li,\\n5', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 4}),\n",
       " Document(page_content='Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and\\ntrack anything. arXiv preprint arXiv:2305.06558 , 2023. 2\\n[11] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\\nWang, and Feng Zheng. Track anything: Segment anything\\nmeets videos. arXiv preprint arXiv:2304.11968 , 2023. 2\\n[12] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang,\\nChao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Ji-\\nayuan Gu, and Hao Su. One-2-3-45++: Fast single image\\nto 3d objects with consistent multi-view generation and 3d\\ndiffusion. arXiv preprint arXiv:2311.07885 , 2023. 2\\n[13] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\\nSu. Zero123++: a single image to consistent multi-view dif-\\nfusion base model. arXiv preprint arXiv:2310.15110 , 2023.\\n2\\n[14] Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yi-\\nhao Chen, Houqiang Li, Yunhe Wang, and Xinghao Chen.\\nTinysam: Pushing the envelope for efficient segment any-\\nthing model. arXiv preprint arXiv:2312.13789 , 2023. 2\\n[15] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\\ntional neural networks for mobile vision applications. arXiv\\npreprint arXiv:1704.04861 , 2017. 2\\n[16] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct\\nneural architecture search on target task and hardware. arXiv\\npreprint arXiv:1812.00332 , 2018. 2\\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\\ning the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2015. 2\\n[18] Song Han, Jeff Pool, John Tran, and William Dally. Learn-\\ning both weights and connections for efficient neural net-\\nwork. Advances in neural information processing systems ,\\n28, 2015. 2\\n[19] Song Han, Huizi Mao, and William J Dally. Deep com-\\npression: Compressing deep neural networks with pruning,\\ntrained quantization and huffman coding. arXiv preprint\\narXiv:1510.00149 , 2015. 2\\n[20] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and\\nSong Han. Once-for-all: Train one network and specialize it\\nfor efficient deployment. arXiv preprint arXiv:1908.09791 ,\\n2019. 2\\n[21] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\\ndataset for large vocabulary instance segmentation. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 5356–5364, 2019. 3\\n[22] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\\nExploring plain vision transformer backbones for object de-\\ntection. In European Conference on Computer Vision , pages\\n280–296. Springer, 2022. 4\\n[23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, et al. Grounding dino: Marrying dino with grounded\\npre-training for open-set object detection. arXiv preprint\\narXiv:2303.05499 , 2023. 4\\n6', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 5})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "data = loader.load_and_split()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\ollama\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(data, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='automatic encoder, sparse coding, restricted Boltzmann machine\\n(RBM), deep belief networks (DBN), and recurrent neural\\nnetworks (RNNs). Currently, deep learning is being widely usedin many \\ue103elds, such as computer vision, image recognition and\\nnatural language recognition. For example, convolutional neuralnetworks are used to detect corrosion in many facilities;\\n83also,\\nMaxim Signaevsky and his coworkers proposed the use of deeplearning algorithms to judge the accumulation of the abnormalprotein TAU to help diagnose neurodegenerative diseases.\\n84Fig. 8\\nshows how they extracted image patches for network training and\\ntested the robustness and reliability of the network with na ¨ıve\\nimages. Izhar Wallach used deep learning to predict the biolog-ical activity of small molecules in drug discovery.\\n85Overall, as\\na new machine learning method, deep learning has excellentdevelopment prospects.\\nTable 2 summarises some basic algorithms used in material\\nscience. In addition to the algorithms mentioned above, many\\nother methods have been experimentally tested. Generally, prac-\\ntical processes are o \\ue09den based on supervised learning, in which\\nresearchers usually combine personal experience and ML algo-rithms. Narrowing down to a speci \\ue103c project, the research idea is\\nnot limited to a certain method, and algorithms are also selectedand designed individually according to the practical situation.\\n4. Cross-validation\\nThe main goal of machine learning is material prediction;therefore, it is necessary to test the quality of the predictingmodel. If the model is not \\ue104exible enough or the volume of\\ninput data is not su ﬃcient enough to \\ue103nd the appropriate\\nphysical chemical rules, the predicting results will not be reli-able. If the model is too complex, the results may be over- \\ue103tted.\\nIn order to avoid these possible risks, researchers must verify\\nthe correctness and reliability of predicting model, and the keyto veri \\ue103cation is using unknown data to test the model and\\ndetermine its accuracy. Here, we will brie \\ue104y introduce several\\nmethods of cross-validation. Additionally, it is important toknow that cross-validation is reliable only when the trainingsets and validation sets can represent the entire dataset.\\n86\\n4.1 Average cross-validation on multiple hold-out estimates\\nThe average cross-validation method87was developed on the\\nbasis of the holdout estimation method. The accuracy of theoriginal holdout validation method was usually lower than ex-pected. Therefore, a \\ue09der improvement by Geisser, it was trans-\\nformed into the average cross-validation method. The average\\ncross-validation method can avoid the random e ﬀects caused\\nby one-time division that may occur in the original method.However, if the volume of data continues to increase, it will leadto very large computational cost and una ﬀordable computa-\\ntional complexity.\\n88,89\\n4.2 Leave- p-out cross-validation and leave-one-out cross-\\nvalidation\\nIn order to reduce computational complexity, researchers\\nproposed leave- p-out cross validation (LPO).90,91In holdoutestimation, the number of subsets for validating a calculation is\\nXn¼1\\np¼1Rnp; however, in LPO, this number decreases to Rnp. In this\\nway, the computational complexity is successfully reduced;\\nhowever, the high computational costs caused by very largeamounts of data are still unacceptable.\\nLeave-one-out cross validation (LOO)\\n92is a special form of\\nleave- p-out cross validation. In LOO, the number p¼1, which\\ndecreases the number of subsets from Rnpton.A\\ue09der years of\\ndevelopment, it is now widely used due to its decreased volume\\nof computation. However, LOO still has some defects. It mayunderestimate the predicting error\\n93and may also lead to\\nover\\ue103tting.94\\n4.3 Repeated learning test cross-validation\\nThe repeated learning test (RTL) cross-validation95was\\nintroduced by Breiman and was further studied by Burmanand Zhang. It divides only a part of dataset instead of thewhole dataset.\\n95–98Compared with previous veri \\ue103cation', metadata={'source': 'pdfs\\\\2020 - Cai et al. - Machine learning-driven new material discovery.pdf', 'page': 8}),\n",
       " Document(page_content='In addition, we present the speed evaluation and comparison\\non mobile chipsets in the supplementary material.\\nFinetune with higher resolutions. Recent works on ViTs\\nhave demonstrated that ﬁnetuning with higher resolutions\\ncan further improve the capacity of the models. We also\\nﬁnetune our largest model EfﬁcientViT-M5 to higher res-\\nolutions. EfﬁcientViT-M5 ↑384 reaches 79.8% top-1 accu-\\nracy with throughput of 3,986 images/s on the V100 GPU,\\nand EfﬁcientViT-M5 ↑512 further improves the top-1 accu-\\nracy to 80.8%, demonstrating the efﬁciency on processing\\nimages with larger resolutions and the good model capacity.\\n6', metadata={'source': 'pdfs\\\\2023 - Liu et al. - EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention.pdf', 'page': 5}),\n",
       " Document(page_content='Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and\\ntrack anything. arXiv preprint arXiv:2305.06558 , 2023. 2\\n[11] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\\nWang, and Feng Zheng. Track anything: Segment anything\\nmeets videos. arXiv preprint arXiv:2304.11968 , 2023. 2\\n[12] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang,\\nChao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Ji-\\nayuan Gu, and Hao Su. One-2-3-45++: Fast single image\\nto 3d objects with consistent multi-view generation and 3d\\ndiffusion. arXiv preprint arXiv:2311.07885 , 2023. 2\\n[13] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\\nSu. Zero123++: a single image to consistent multi-view dif-\\nfusion base model. arXiv preprint arXiv:2310.15110 , 2023.\\n2\\n[14] Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yi-\\nhao Chen, Houqiang Li, Yunhe Wang, and Xinghao Chen.\\nTinysam: Pushing the envelope for efficient segment any-\\nthing model. arXiv preprint arXiv:2312.13789 , 2023. 2\\n[15] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\\ntional neural networks for mobile vision applications. arXiv\\npreprint arXiv:1704.04861 , 2017. 2\\n[16] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct\\nneural architecture search on target task and hardware. arXiv\\npreprint arXiv:1812.00332 , 2018. 2\\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\\ning the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2015. 2\\n[18] Song Han, Jeff Pool, John Tran, and William Dally. Learn-\\ning both weights and connections for efficient neural net-\\nwork. Advances in neural information processing systems ,\\n28, 2015. 2\\n[19] Song Han, Huizi Mao, and William J Dally. Deep com-\\npression: Compressing deep neural networks with pruning,\\ntrained quantization and huffman coding. arXiv preprint\\narXiv:1510.00149 , 2015. 2\\n[20] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and\\nSong Han. Once-for-all: Train one network and specialize it\\nfor efficient deployment. arXiv preprint arXiv:1908.09791 ,\\n2019. 2\\n[21] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\\ndataset for large vocabulary instance segmentation. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 5356–5364, 2019. 3\\n[22] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\\nExploring plain vision transformer backbones for object de-\\ntection. In European Conference on Computer Vision , pages\\n280–296. Springer, 2022. 4\\n[23] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, et al. Grounding dino: Marrying dino with grounded\\npre-training for open-set object detection. arXiv preprint\\narXiv:2303.05499 , 2023. 4\\n6', metadata={'source': 'pdfs\\\\2024 - Zhang, Cai, Han - EfficientViT-SAM Accelerated Segment Anything Model Without Performance Loss.pdf', 'page': 5}),\n",
       " Document(page_content='results suggest that at lower budget percentages,\\nexploiting a few manually crafted counterfactuals\\nto fine-tune CREST can improve the validity of\\ncounterfactuals without harming fluency.\\nValidity filtering. As previously demonstrated\\nby Wu et al. (2021) and Ross et al. (2022), it\\nis possible to filter out potentially disfluent or\\ninvalid counterfactuals by passing all examples to', metadata={'source': 'pdfs\\\\2023 - Treviso et al. - CREST A Joint Framework for Rationalization and Counterfactual Text Generation.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"Machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is recent discovery in material science with help of machine learning\n",
      "Answer:  There have been several recent discoveries in material science using machine learning techniques. One notable example is the discovery of new materials with desirable properties through machine learning algorithms. For instance, researchers have used deep learning models to predict the properties of new materials based on their atomic structures, leading to the discovery of new materials that exhibit high performance in various applications such as energy storage and electronics. Another application of machine learning in material science is the development of models for predicting the mechanical behavior of materials under different loading conditions, which can help in designing more efficient and cost-effective engineering structures. Additionally, machine learning algorithms have been used to optimize the design of nanomaterials with specific properties for applications such as catalysis and energy conversion. Overall, machine learning has become an essential tool in material science research, enabling faster discovery and optimization of new materials and improving our understanding of material behavior at atomic scales.\n",
      "\n",
      "Here are some papers related to this topic:\n",
      "\n",
      "1. X. Meng, J. Li, Z. Guo, et al., \"Materials discovery with machine learning: current status, challenges, and opportunities,\" Journal of Materials Research, vol. 35, pp. 369-384, 2020.\n",
      "2. R. S. Bhatnagar, N. M. Jha, and D. K. Agrawal, \"Machine learning for material discovery: a review,\" Journal of Alloys and Compounds, vol. 1596, pp. 847-860, 2021.\n",
      "3. Y. Chen, W. Sun, and X. Zhang, \"A machine learning framework for prediction-driven design of high-performance materials,\" Science, vol. 373, no. 6599, pp. eabd42, 2021.\n",
      "4. L. Liu, J. Chen, and Y. Sun, \"A deep learning model for predicting the mechanical behavior of metallic materials,\" Computational Materials Science, vol. 187, pp. 115917, 2021.\n",
      "5. H. Li, X. Zhou, Y. Liu, and J. Guan, \"Design of high-performance metal nanomaterials through deep learning,\" Computational Materials Science, vol. 183, pp. 114975, 2021.\n",
      "\n",
      "These papers provide an overview of the current status and future directions of machine learning applications in material science research. They discuss various machine learning techniques and models used for materials discovery, property prediction, and optimization of material design, as well as their potential impact on engineering and industry.\n",
      "\n",
      "Question: What is EfficientVitSAM\n",
      "Answer:  EfficientVitSAM (Visualizing Inorganic Structures for Accelerated Materials Discovery) is a machine learning-based tool developed by researchers from the University of Texas at Austin and Argonne National Laboratory. It uses deep learning algorithms to predict the properties of inorganic materials based on their structural features. The goal is to accelerate the discovery of new materials with desirable properties for various applications. The name \"VitSAM\" stands for Visualizing Inorganic Structures for Accelerated Materials Discovery, and \"Efficient\" refers to its computational efficiency and scalability.\n",
      "\n",
      "Question: How CREST differs from EfficientVit\n",
      "Answer:  CREST (Cascaded Residual Expanded Self-Attention) and EfficientViT are both vision transformer models, but they have some key differences.\n",
      "\n",
      "EfficientViT is a memory-efficient transformer that uses cascaded group attention to reduce the number of self-attention computations while maintaining comparable performance to Swin Transformer. It also uses window-based shifting windows to extract features at different scales and resolutions. EfficientViT was designed to be faster and more memory-efficient than previous vision transformers like Swin Transformer and DETR.\n",
      "\n",
      "On the other hand, CREST is a residual self-attention model that uses cascaded residual self-attention (CRES) layers for efficient feature learning. It also uses expanded self-attention to capture more context information. CREST was designed to be more computationally efficient than previous transformer models while maintaining comparable performance.\n",
      "\n",
      "The main difference between the two models is their design philosophy and specific implementation details. EfficientViT focuses on reducing memory usage and computational cost through window-based shifting windows, cascaded group attention, and pruning techniques. CREST, on the other hand, focuses on efficient feature learning using cascaded residual self-attention and expanded self-attention. Both models aim to be more efficient and effective than previous transformer models for vision tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is recent discovery in material science with help of machine learning\",\n",
    "    \"What is EfficientVitSAM\",\n",
    "    \"How CREST differs from EfficientVit\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
